{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def list_files(directory):\n",
    "    files = []\n",
    "    for entry in os.listdir(directory):\n",
    "        if os.path.isfile(os.path.join(directory, entry)):\n",
    "            files.append(entry)\n",
    "    return files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = \"results_dgl_regression_scaffold_val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['model_gnn_conv', 'model_gnn_conv_1', 'model_gnn_conv_10', 'model_gnn_conv_11', 'model_gnn_conv_12', 'model_gnn_conv_13', 'model_gnn_conv_14', 'model_gnn_conv_15', 'model_gnn_conv_16', 'model_gnn_conv_17', 'model_gnn_conv_18', 'model_gnn_conv_19', 'model_gnn_conv_2', 'model_gnn_conv_20', 'model_gnn_conv_21', 'model_gnn_conv_22', 'model_gnn_conv_23', 'model_gnn_conv_24', 'model_gnn_conv_25', 'model_gnn_conv_26', 'model_gnn_conv_27', 'model_gnn_conv_28', 'model_gnn_conv_29', 'model_gnn_conv_3', 'model_gnn_conv_30', 'model_gnn_conv_31', 'model_gnn_conv_32', 'model_gnn_conv_33', 'model_gnn_conv_34', 'model_gnn_conv_35', 'model_gnn_conv_36', 'model_gnn_conv_37', 'model_gnn_conv_38', 'model_gnn_conv_39', 'model_gnn_conv_4', 'model_gnn_conv_40', 'model_gnn_conv_41', 'model_gnn_conv_42', 'model_gnn_conv_43', 'model_gnn_conv_44', 'model_gnn_conv_45', 'model_gnn_conv_46', 'model_gnn_conv_47', 'model_gnn_conv_48', 'model_gnn_conv_49', 'model_gnn_conv_5', 'model_gnn_conv_50', 'model_gnn_conv_51', 'model_gnn_conv_52', 'model_gnn_conv_53', 'model_gnn_conv_54', 'model_gnn_conv_55', 'model_gnn_conv_56', 'model_gnn_conv_57', 'model_gnn_conv_58', 'model_gnn_conv_59', 'model_gnn_conv_6', 'model_gnn_conv_60', 'model_gnn_conv_61', 'model_gnn_conv_62', 'model_gnn_conv_63', 'model_gnn_conv_7', 'model_gnn_conv_8', 'model_gnn_conv_9']\n"
     ]
    }
   ],
   "source": [
    "files = list_files(path1)\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1_load = []\n",
    "import dill as pickle\n",
    "\n",
    "for file in files:\n",
    "    file_path = os.path.join(path1, file)\n",
    "    with open(file_path, 'rb') as filel:\n",
    "        loaded_data = pickle.load(filel)\n",
    "    file1_load.append(loaded_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'pred_df':     y_real    y_pred\n",
       "  0   -3.672 -2.948319\n",
       "  1   -3.050 -2.109394\n",
       "  2   -3.620 -2.985116\n",
       "  3   -3.658 -3.416189\n",
       "  4   -2.120 -1.447404\n",
       "  ..     ...       ...\n",
       "  44  -4.743 -4.917334\n",
       "  45  -8.600 -7.652426\n",
       "  46  -3.880 -4.057945\n",
       "  47  -3.401 -2.210320\n",
       "  48  -3.460 -3.153419\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 54.7974739074707,\n",
       "  'mean_mse': 1.0798689126968384,\n",
       "  'mean_l1': 0.8108006715774536,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [64],\n",
       "  'predictor_hidden_feats': 128,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=64, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=64, bias=True)\n",
       "          (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -5.190 -3.480190\n",
       "  1    0.522  0.605433\n",
       "  2   -2.281 -2.527041\n",
       "  3   -6.800 -6.747523\n",
       "  4   -2.320 -2.152536\n",
       "  ..     ...       ...\n",
       "  44  -2.349 -1.725280\n",
       "  45  -4.950 -4.642383\n",
       "  46  -2.461 -2.663934\n",
       "  47  -1.640 -2.538108\n",
       "  48  -3.690 -4.143588\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 19.852107763290405,\n",
       "  'mean_mse': 1.072730541229248,\n",
       "  'mean_l1': 0.7806079387664795,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [64],\n",
       "  'predictor_hidden_feats': 256,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=64, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=64, bias=True)\n",
       "          (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=128, out_features=256, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -5.410 -3.768357\n",
       "  1   -6.780 -6.412265\n",
       "  2   -9.018 -7.280151\n",
       "  3   -2.160 -1.175003\n",
       "  4   -6.020 -4.313154\n",
       "  ..     ...       ...\n",
       "  44  -3.880 -4.190438\n",
       "  45  -1.250 -2.035389\n",
       "  46  -9.332 -8.556228\n",
       "  47  -3.401 -0.906436\n",
       "  48  -8.000 -7.166644\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 38.62829566001892,\n",
       "  'mean_mse': 2.417750120162964,\n",
       "  'mean_l1': 1.0598916113376617,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [64, 64, 64],\n",
       "  'predictor_hidden_feats': 512,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=64, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=64, bias=True)\n",
       "          (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-2): 2 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=64, out=64, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -5.270 -4.864627\n",
       "  1   -3.360 -2.103081\n",
       "  2   -6.237 -2.920094\n",
       "  3   -3.658 -4.105947\n",
       "  4   -3.050 -3.708295\n",
       "  ..     ...       ...\n",
       "  44  -5.190 -3.834626\n",
       "  45  -5.696 -4.621793\n",
       "  46  -3.094 -2.703162\n",
       "  47  -3.168 -2.479919\n",
       "  48  -1.250 -1.363208\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 24.030266046524048,\n",
       "  'mean_mse': 4.786823034286499,\n",
       "  'mean_l1': 1.386451780796051,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [64, 64, 64],\n",
       "  'predictor_hidden_feats': 1024,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=64, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=64, bias=True)\n",
       "          (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-2): 2 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=64, out=64, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=128, out_features=1024, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=1024, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -2.350 -2.321740\n",
       "  1   -2.920 -2.873185\n",
       "  2   -3.620 -3.605720\n",
       "  3   -4.173 -4.973496\n",
       "  4   -4.450 -2.118742\n",
       "  ..     ...       ...\n",
       "  44  -5.000 -4.543682\n",
       "  45  -3.583 -1.911636\n",
       "  46  -7.280 -4.293761\n",
       "  47  -3.360 -2.722179\n",
       "  48  -6.860 -6.742632\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 73.104012966156,\n",
       "  'mean_mse': 4.752409338951111,\n",
       "  'mean_l1': 1.0658863186836243,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [64, 64, 64, 64],\n",
       "  'predictor_hidden_feats': 128,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=64, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=64, bias=True)\n",
       "          (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-3): 3 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=64, out=64, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -3.880 -4.317972\n",
       "  1   -0.400 -0.698211\n",
       "  2   -4.310 -4.428699\n",
       "  3   -3.638 -6.651589\n",
       "  4   -5.000 -4.587252\n",
       "  ..     ...       ...\n",
       "  44  -4.632 -3.899760\n",
       "  45  -1.850 -3.007399\n",
       "  46  -3.401 -0.543790\n",
       "  47  -5.400 -5.536786\n",
       "  48  -5.915 -4.224378\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 32.659266233444214,\n",
       "  'mean_mse': 6.754857301712036,\n",
       "  'mean_l1': 1.4167295694351196,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [64, 64, 64, 64],\n",
       "  'predictor_hidden_feats': 256,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=64, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=64, bias=True)\n",
       "          (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-3): 3 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=64, out=64, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=128, out_features=256, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real     y_pred\n",
       "  0   -2.700  -3.090664\n",
       "  1   -8.000  -7.016448\n",
       "  2   -9.018  -6.906355\n",
       "  3   -4.310  -3.967221\n",
       "  4   -6.680  -6.009530\n",
       "  ..     ...        ...\n",
       "  44  -1.960  -2.294645\n",
       "  45  -2.943  -1.818125\n",
       "  46  -4.445 -12.761958\n",
       "  47  -5.190  -4.060229\n",
       "  48  -3.930  -1.340317\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 47.951942920684814,\n",
       "  'mean_mse': 4.9435083866119385,\n",
       "  'mean_l1': 1.206407368183136,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [64, 64, 64, 64],\n",
       "  'predictor_hidden_feats': 512,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=64, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=64, bias=True)\n",
       "          (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-3): 3 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=64, out=64, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real     y_pred\n",
       "  0   -4.445 -13.596061\n",
       "  1   -4.370  -3.644025\n",
       "  2   -0.742  -3.943400\n",
       "  3   -3.538  -4.077275\n",
       "  4   -4.450  -2.193866\n",
       "  ..     ...        ...\n",
       "  44  -4.402  -5.204989\n",
       "  45  -4.047  -5.874790\n",
       "  46  -6.780  -6.617874\n",
       "  47  -0.600  -0.856759\n",
       "  48  -4.328  -4.172378\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 60.864784955978394,\n",
       "  'mean_mse': 4.741706371307373,\n",
       "  'mean_l1': 1.2968700528144836,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [64, 64, 64, 64],\n",
       "  'predictor_hidden_feats': 1024,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=64, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=64, bias=True)\n",
       "          (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-3): 3 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=64, out=64, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=128, out_features=1024, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=1024, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -5.400 -6.344931\n",
       "  1   -7.800 -8.386624\n",
       "  2   -5.190 -4.305386\n",
       "  3   -6.680 -6.897020\n",
       "  4   -4.634 -4.255890\n",
       "  ..     ...       ...\n",
       "  44  -2.100 -2.090929\n",
       "  45   0.522 -1.048780\n",
       "  46  -3.672 -2.903706\n",
       "  47  -2.266 -1.621999\n",
       "  48  -3.401 -1.980863\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 22.138380765914917,\n",
       "  'mean_mse': 0.9423701763153076,\n",
       "  'mean_l1': 0.7663335800170898,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [128],\n",
       "  'predictor_hidden_feats': 128,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=128, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=128, bias=True)\n",
       "          (bn_layer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -2.100 -2.180013\n",
       "  1   -3.460 -4.211036\n",
       "  2   -8.000 -8.000581\n",
       "  3   -4.376 -3.719080\n",
       "  4   -1.960 -2.769294\n",
       "  ..     ...       ...\n",
       "  44  -2.338 -2.207799\n",
       "  45  -1.990 -2.025401\n",
       "  46  -3.246 -3.874339\n",
       "  47  -6.237 -5.791589\n",
       "  48  -2.920 -2.581833\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 17.469671964645386,\n",
       "  'mean_mse': 0.9739673733711243,\n",
       "  'mean_l1': 0.7917884290218353,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [128],\n",
       "  'predictor_hidden_feats': 256,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=128, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=128, bias=True)\n",
       "          (bn_layer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -0.742 -2.079108\n",
       "  1   -2.060 -1.961471\n",
       "  2   -2.100 -1.908367\n",
       "  3   -3.060 -2.302455\n",
       "  4   -4.950 -4.102754\n",
       "  ..     ...       ...\n",
       "  44  -4.120 -3.868007\n",
       "  45  -6.237 -6.330848\n",
       "  46   0.300 -0.408855\n",
       "  47  -3.094 -2.785977\n",
       "  48  -8.000 -7.085578\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 24.659360647201538,\n",
       "  'mean_mse': 1.0058996379375458,\n",
       "  'mean_l1': 0.7824300229549408,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [128],\n",
       "  'predictor_hidden_feats': 512,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=128, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=128, bias=True)\n",
       "          (bn_layer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -1.800 -2.561685\n",
       "  1   -4.743 -4.852026\n",
       "  2   -6.237 -7.924484\n",
       "  3   -2.338 -3.233155\n",
       "  4   -5.270 -6.083638\n",
       "  ..     ...       ...\n",
       "  44  -3.290 -3.120910\n",
       "  45  -3.880 -3.814723\n",
       "  46  -3.590 -4.328417\n",
       "  47  -4.370 -3.983471\n",
       "  48   1.100  0.806317\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 16.34622836112976,\n",
       "  'mean_mse': 1.1887688040733337,\n",
       "  'mean_l1': 0.8242393732070923,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [128],\n",
       "  'predictor_hidden_feats': 1024,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=128, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=128, bias=True)\n",
       "          (bn_layer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=1024, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -3.050 -2.673315\n",
       "  1   -1.250 -1.455978\n",
       "  2   -3.460 -3.238207\n",
       "  3   -3.120 -3.598098\n",
       "  4   -4.380 -4.769804\n",
       "  ..     ...       ...\n",
       "  44  -6.800 -6.704806\n",
       "  45  -4.883 -2.547541\n",
       "  46  -3.220 -3.866193\n",
       "  47  -1.716 -3.449477\n",
       "  48  -4.160 -3.904684\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 36.87888050079346,\n",
       "  'mean_mse': 0.9716865420341492,\n",
       "  'mean_l1': 0.7470130920410156,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [64],\n",
       "  'predictor_hidden_feats': 512,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=64, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=64, bias=True)\n",
       "          (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0    1.100  1.011748\n",
       "  1   -3.180 -3.605410\n",
       "  2   -3.460 -5.372571\n",
       "  3   -2.349 -2.406439\n",
       "  4   -5.190 -4.780957\n",
       "  ..     ...       ...\n",
       "  44  -4.805 -3.516536\n",
       "  45  -3.120 -4.463319\n",
       "  46  -1.960 -2.660139\n",
       "  47  -5.915 -5.311238\n",
       "  48  -1.716 -3.762199\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 43.203208208084106,\n",
       "  'mean_mse': 1.1101739406585693,\n",
       "  'mean_l1': 0.8126453459262848,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [128, 128],\n",
       "  'predictor_hidden_feats': 128,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=128, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=128, bias=True)\n",
       "          (bn_layer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=128, out=128, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (bn_layer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -4.805 -3.763231\n",
       "  1   -3.610 -3.401190\n",
       "  2   -4.370 -4.333653\n",
       "  3   -3.401 -2.058226\n",
       "  4   -6.237 -6.236655\n",
       "  ..     ...       ...\n",
       "  44  -3.246 -5.551305\n",
       "  45  -4.328 -3.763382\n",
       "  46  -2.350 -3.467562\n",
       "  47  -3.050 -2.869102\n",
       "  48  -5.190 -3.641633\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 54.7391459941864,\n",
       "  'mean_mse': 0.9407547116279602,\n",
       "  'mean_l1': 0.7557973861694336,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [128, 128],\n",
       "  'predictor_hidden_feats': 256,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=128, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=128, bias=True)\n",
       "          (bn_layer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=128, out=128, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (bn_layer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -1.990 -2.349467\n",
       "  1   -2.943 -2.279367\n",
       "  2   -6.860 -5.350286\n",
       "  3   -1.250 -1.460343\n",
       "  4   -4.328 -3.666395\n",
       "  ..     ...       ...\n",
       "  44   0.790 -0.002394\n",
       "  45  -3.460 -2.343819\n",
       "  46  -2.700 -2.581185\n",
       "  47  -4.173 -5.303072\n",
       "  48  -3.658 -3.894140\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 29.760167121887207,\n",
       "  'mean_mse': 1.2943545281887054,\n",
       "  'mean_l1': 0.79734006524086,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [128, 128],\n",
       "  'predictor_hidden_feats': 512,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=128, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=128, bias=True)\n",
       "          (bn_layer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=128, out=128, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (bn_layer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -3.180 -2.916594\n",
       "  1   -5.190 -4.303984\n",
       "  2   -1.960 -2.503892\n",
       "  3   -3.043 -3.419991\n",
       "  4   -4.950 -4.207927\n",
       "  ..     ...       ...\n",
       "  44  -8.000 -8.586058\n",
       "  45  -2.676 -2.701174\n",
       "  46  -1.640 -2.741140\n",
       "  47  -2.350 -2.509290\n",
       "  48  -4.173 -4.554442\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 53.39509916305542,\n",
       "  'mean_mse': 2.1235026717185974,\n",
       "  'mean_l1': 0.902055561542511,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [128, 128],\n",
       "  'predictor_hidden_feats': 1024,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=128, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=128, bias=True)\n",
       "          (bn_layer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=128, out=128, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (bn_layer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=1024, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -3.672 -3.383126\n",
       "  1   -4.925 -4.207204\n",
       "  2   -1.040 -1.574038\n",
       "  3   -6.680 -6.341588\n",
       "  4   -6.237 -7.060769\n",
       "  ..     ...       ...\n",
       "  44  -3.094 -3.124066\n",
       "  45  -4.743 -3.958280\n",
       "  46  -3.690 -4.105329\n",
       "  47  -1.899 -1.274251\n",
       "  48   0.522  0.184665\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 40.381035566329956,\n",
       "  'mean_mse': 1.327627420425415,\n",
       "  'mean_l1': 0.8684000074863434,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [128, 128, 128],\n",
       "  'predictor_hidden_feats': 128,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=128, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=128, bias=True)\n",
       "          (bn_layer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-2): 2 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=128, out=128, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (bn_layer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -0.742 -4.206795\n",
       "  1   -3.401 -1.777406\n",
       "  2   -3.538 -3.868371\n",
       "  3   -3.060 -2.346437\n",
       "  4   -3.171 -3.062758\n",
       "  ..     ...       ...\n",
       "  44  -4.120 -3.791021\n",
       "  45  -1.899 -1.313776\n",
       "  46  -8.600 -6.377983\n",
       "  47  -4.376 -3.674438\n",
       "  48  -4.380 -5.652614\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 23.510163068771362,\n",
       "  'mean_mse': 5.108415842056274,\n",
       "  'mean_l1': 1.2218800783157349,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [128, 128, 128],\n",
       "  'predictor_hidden_feats': 256,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=128, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=128, bias=True)\n",
       "          (bn_layer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-2): 2 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=128, out=128, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (bn_layer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -2.060 -2.357026\n",
       "  1   -4.925 -3.818773\n",
       "  2   -3.538 -5.318452\n",
       "  3   -3.180 -3.166680\n",
       "  4   -4.634 -3.438765\n",
       "  ..     ...       ...\n",
       "  44  -5.696 -4.938624\n",
       "  45  -2.920 -2.574062\n",
       "  46  -1.640 -2.962912\n",
       "  47  -4.743 -4.729537\n",
       "  48  -5.000 -4.806971\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 46.45222187042236,\n",
       "  'mean_mse': 1.9752842783927917,\n",
       "  'mean_l1': 0.963010311126709,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [128, 128, 128],\n",
       "  'predictor_hidden_feats': 512,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=128, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=128, bias=True)\n",
       "          (bn_layer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-2): 2 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=128, out=128, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (bn_layer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real     y_pred\n",
       "  0   -2.350  -2.881980\n",
       "  1   -4.950  -3.733336\n",
       "  2   -2.676  -2.678232\n",
       "  3   -4.445 -15.043383\n",
       "  4   -4.634  -3.400967\n",
       "  ..     ...        ...\n",
       "  44  -2.349  -1.902052\n",
       "  45  -4.743  -4.174140\n",
       "  46  -7.800  -7.854146\n",
       "  47   0.790   3.412668\n",
       "  48  -3.451  -3.384820\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 33.02331304550171,\n",
       "  'mean_mse': 4.011960744857788,\n",
       "  'mean_l1': 1.0836032927036285,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [128, 128, 128],\n",
       "  'predictor_hidden_feats': 1024,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=128, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=128, bias=True)\n",
       "          (bn_layer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-2): 2 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=128, out=128, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (bn_layer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=1024, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -3.460 -3.610588\n",
       "  1   -5.270 -3.327125\n",
       "  2   -3.930 -2.764139\n",
       "  3   -4.632 -4.045396\n",
       "  4   -2.281 -3.018318\n",
       "  ..     ...       ...\n",
       "  44  -1.250 -1.509159\n",
       "  45  -2.349 -2.716938\n",
       "  46  -8.600 -6.106876\n",
       "  47  -1.716 -4.494265\n",
       "  48  -9.018 -8.079215\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 57.300262689590454,\n",
       "  'mean_mse': 7.329993963241577,\n",
       "  'mean_l1': 1.2011379599571228,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [128, 128, 128, 128],\n",
       "  'predictor_hidden_feats': 128,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=128, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=128, bias=True)\n",
       "          (bn_layer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-3): 3 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=128, out=128, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (bn_layer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -2.281 -2.646846\n",
       "  1   -3.094 -2.944636\n",
       "  2   -5.270 -4.067283\n",
       "  3   -7.280 -4.311846\n",
       "  4   -4.120 -3.816114\n",
       "  ..     ...       ...\n",
       "  44  -3.401 -1.878170\n",
       "  45  -3.930 -3.353840\n",
       "  46   0.790 -1.362129\n",
       "  47  -3.460 -2.577409\n",
       "  48  -3.880 -3.740376\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 64.8173155784607,\n",
       "  'mean_mse': 2.184838056564331,\n",
       "  'mean_l1': 0.9211110770702362,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [128, 128, 128, 128],\n",
       "  'predictor_hidden_feats': 256,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=128, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=128, bias=True)\n",
       "          (bn_layer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-3): 3 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=128, out=128, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (bn_layer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0    0.522 -0.433759\n",
       "  1   -5.000 -4.903979\n",
       "  2    0.790 -0.550153\n",
       "  3   -4.634 -3.123168\n",
       "  4   -3.620 -2.340785\n",
       "  ..     ...       ...\n",
       "  44  -3.610 -3.126033\n",
       "  45  -3.220 -3.929760\n",
       "  46  -0.400 -0.932794\n",
       "  47  -1.899 -1.328201\n",
       "  48  -8.000 -8.180911\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 25.236124277114868,\n",
       "  'mean_mse': 1.17404043674469,\n",
       "  'mean_l1': 0.8360958695411682,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [64],\n",
       "  'predictor_hidden_feats': 1024,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=64, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=64, bias=True)\n",
       "          (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=128, out_features=1024, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=1024, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -6.237 -5.905336\n",
       "  1   -6.780 -5.725532\n",
       "  2   -3.021 -4.066250\n",
       "  3   -3.451 -3.836694\n",
       "  4    1.100  0.225392\n",
       "  ..     ...       ...\n",
       "  44  -5.400 -5.681688\n",
       "  45  -4.310 -4.713808\n",
       "  46  -1.899 -3.162103\n",
       "  47  -1.960 -2.729346\n",
       "  48  -4.632 -4.388480\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 40.427056550979614,\n",
       "  'mean_mse': 4.346827983856201,\n",
       "  'mean_l1': 1.1420197486877441,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [128, 128, 128, 128],\n",
       "  'predictor_hidden_feats': 512,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=128, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=128, bias=True)\n",
       "          (bn_layer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-3): 3 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=128, out=128, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (bn_layer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -3.638 -7.262432\n",
       "  1   -3.590 -4.344755\n",
       "  2   -8.040 -6.535925\n",
       "  3   -4.376 -4.571929\n",
       "  4   -1.800 -3.396985\n",
       "  ..     ...       ...\n",
       "  44  -5.220 -5.476831\n",
       "  45  -2.920 -3.656505\n",
       "  46  -7.280 -5.267988\n",
       "  47  -3.171 -3.628011\n",
       "  48  -6.780 -7.459464\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 69.94724464416504,\n",
       "  'mean_mse': 4.839312314987183,\n",
       "  'mean_l1': 1.1272207498550415,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [128, 128, 128, 128],\n",
       "  'predictor_hidden_feats': 1024,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=128, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=128, bias=True)\n",
       "          (bn_layer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-3): 3 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=128, out=128, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (bn_layer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=1024, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -1.960 -2.747690\n",
       "  1   -2.780 -3.775218\n",
       "  2   -5.350 -5.445259\n",
       "  3   -5.270 -5.348742\n",
       "  4   -3.538 -4.784298\n",
       "  ..     ...       ...\n",
       "  44  -4.370 -3.415051\n",
       "  45  -3.620 -2.573238\n",
       "  46  -3.220 -3.698995\n",
       "  47  -1.040 -1.231113\n",
       "  48  -2.349 -1.866065\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 18.396114826202393,\n",
       "  'mean_mse': 1.0090899169445038,\n",
       "  'mean_l1': 0.7391971349716187,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [256],\n",
       "  'predictor_hidden_feats': 128,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=256, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=256, bias=True)\n",
       "          (bn_layer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -3.401 -1.888406\n",
       "  1   -1.899 -2.090201\n",
       "  2   -5.000 -5.059719\n",
       "  3   -3.672 -2.688504\n",
       "  4   -4.632 -3.510843\n",
       "  ..     ...       ...\n",
       "  44  -4.380 -5.020244\n",
       "  45  -3.620 -2.238293\n",
       "  46  -3.290 -2.934638\n",
       "  47  -3.638 -4.875337\n",
       "  48  -5.696 -4.200318\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 32.34506916999817,\n",
       "  'mean_mse': 0.9676992297172546,\n",
       "  'mean_l1': 0.7407112121582031,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [256],\n",
       "  'predictor_hidden_feats': 256,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=256, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=256, bias=True)\n",
       "          (bn_layer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -1.740 -3.030745\n",
       "  1   -2.266 -1.534045\n",
       "  2    0.522  0.259862\n",
       "  3   -6.780 -5.563404\n",
       "  4   -4.328 -4.299225\n",
       "  ..     ...       ...\n",
       "  44  -3.638 -5.649616\n",
       "  45  -3.583 -1.944453\n",
       "  46  -2.349 -1.464300\n",
       "  47  -6.726 -8.064503\n",
       "  48  -1.850 -2.325568\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 21.695222854614258,\n",
       "  'mean_mse': 1.2500008940696716,\n",
       "  'mean_l1': 0.8267291486263275,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [256],\n",
       "  'predictor_hidden_feats': 512,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=256, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=256, bias=True)\n",
       "          (bn_layer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -1.716 -3.624280\n",
       "  1   -2.320 -1.651331\n",
       "  2   -2.920 -2.704041\n",
       "  3   -4.883 -3.257137\n",
       "  4   -3.451 -2.422078\n",
       "  ..     ...       ...\n",
       "  44  -8.040 -6.329603\n",
       "  45  -3.290 -2.908089\n",
       "  46  -2.100 -1.867409\n",
       "  47  -2.982 -2.223389\n",
       "  48  -5.915 -5.030893\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 19.781437397003174,\n",
       "  'mean_mse': 1.5286960005760193,\n",
       "  'mean_l1': 0.8154968321323395,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [256],\n",
       "  'predictor_hidden_feats': 1024,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=256, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=256, bias=True)\n",
       "          (bn_layer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=1024, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -1.800 -2.491288\n",
       "  1   -1.740 -2.474877\n",
       "  2   -3.401 -2.104551\n",
       "  3   -4.800 -4.549267\n",
       "  4   -3.120 -4.022310\n",
       "  ..     ...       ...\n",
       "  44  -6.860 -5.949916\n",
       "  45  -3.290 -3.362545\n",
       "  46  -3.638 -4.521399\n",
       "  47  -3.220 -2.749080\n",
       "  48  -0.600 -1.470178\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 52.104366302490234,\n",
       "  'mean_mse': 0.8884322941303253,\n",
       "  'mean_l1': 0.7540042102336884,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [256, 256],\n",
       "  'predictor_hidden_feats': 128,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=256, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=256, bias=True)\n",
       "          (bn_layer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=256, out=256, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (bn_layer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -4.632 -4.164397\n",
       "  1   -1.899 -2.224290\n",
       "  2   -2.120 -1.840871\n",
       "  3   -1.990 -2.601139\n",
       "  4   -4.370 -3.843907\n",
       "  ..     ...       ...\n",
       "  44  -3.094 -3.230051\n",
       "  45  -4.376 -4.707936\n",
       "  46  -3.451 -3.425795\n",
       "  47  -6.020 -4.087382\n",
       "  48  -0.600 -1.159108\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 50.06304931640625,\n",
       "  'mean_mse': 1.4601299166679382,\n",
       "  'mean_l1': 0.814393013715744,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [256, 256],\n",
       "  'predictor_hidden_feats': 256,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=256, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=256, bias=True)\n",
       "          (bn_layer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=256, out=256, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (bn_layer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -4.950 -4.287918\n",
       "  1   -4.632 -4.317275\n",
       "  2   -4.047 -5.083232\n",
       "  3   -2.676 -3.151661\n",
       "  4   -3.658 -4.079775\n",
       "  ..     ...       ...\n",
       "  44  -4.120 -4.472229\n",
       "  45  -3.360 -3.497637\n",
       "  46  -2.338 -2.659504\n",
       "  47  -2.780 -3.591708\n",
       "  48   0.790 -1.483314\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 32.24044728279114,\n",
       "  'mean_mse': 1.422254204750061,\n",
       "  'mean_l1': 0.8211053609848022,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [256, 256],\n",
       "  'predictor_hidden_feats': 512,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=256, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=256, bias=True)\n",
       "          (bn_layer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=256, out=256, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (bn_layer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real     y_pred\n",
       "  0   -2.700  -2.866607\n",
       "  1   -4.634  -4.422109\n",
       "  2   -9.018  -7.947210\n",
       "  3   -6.800 -10.359536\n",
       "  4   -2.920  -3.488025\n",
       "  ..     ...        ...\n",
       "  44  -4.402  -3.632184\n",
       "  45  -0.600  -1.176667\n",
       "  46   1.100   0.868550\n",
       "  47  -2.100  -1.578532\n",
       "  48  -2.350  -2.225929\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 44.8725311756134,\n",
       "  'mean_mse': 1.2429488599300385,\n",
       "  'mean_l1': 0.8269948363304138,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [256, 256],\n",
       "  'predictor_hidden_feats': 1024,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=256, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=256, bias=True)\n",
       "          (bn_layer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=256, out=256, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (bn_layer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=1024, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -2.461 -2.621537\n",
       "  1   -3.460 -4.731940\n",
       "  2    1.100  0.553795\n",
       "  3   -2.780 -4.690739\n",
       "  4   -4.800 -4.222029\n",
       "  ..     ...       ...\n",
       "  44  -4.376 -4.663107\n",
       "  45   0.522  0.101897\n",
       "  46  -4.370 -3.575211\n",
       "  47  -7.800 -8.296951\n",
       "  48  -3.120 -4.216475\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 30.183627605438232,\n",
       "  'mean_mse': 1.635543704032898,\n",
       "  'mean_l1': 0.8993659913539886,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [64, 64],\n",
       "  'predictor_hidden_feats': 128,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=64, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=64, bias=True)\n",
       "          (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=64, out=64, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -0.742 -2.772497\n",
       "  1   -6.726 -6.844060\n",
       "  2   -6.680 -6.260504\n",
       "  3   -4.883 -3.437920\n",
       "  4   -2.120 -1.308324\n",
       "  ..     ...       ...\n",
       "  44  -5.220 -5.281751\n",
       "  45  -3.180 -3.247480\n",
       "  46  -4.328 -2.739048\n",
       "  47  -8.000 -7.559362\n",
       "  48  -3.290 -3.300359\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 54.77510476112366,\n",
       "  'mean_mse': 1.8721410036087036,\n",
       "  'mean_l1': 0.8871234655380249,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [256, 256, 256],\n",
       "  'predictor_hidden_feats': 128,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=256, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=256, bias=True)\n",
       "          (bn_layer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-2): 2 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=256, out=256, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (bn_layer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -4.120 -4.263233\n",
       "  1   -6.780 -4.720968\n",
       "  2   -3.050 -2.491313\n",
       "  3   -4.950 -4.050185\n",
       "  4   -3.672 -3.797057\n",
       "  ..     ...       ...\n",
       "  44  -3.060 -1.725659\n",
       "  45  -1.990 -2.758270\n",
       "  46  -3.538 -4.742413\n",
       "  47  -5.400 -6.155577\n",
       "  48  -3.094 -3.004535\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 37.050759077072144,\n",
       "  'mean_mse': 1.831526517868042,\n",
       "  'mean_l1': 0.908020406961441,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [256, 256, 256],\n",
       "  'predictor_hidden_feats': 256,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=256, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=256, bias=True)\n",
       "          (bn_layer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-2): 2 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=256, out=256, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (bn_layer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real     y_pred\n",
       "  0   -6.680  -6.296421\n",
       "  1   -6.860  -6.138568\n",
       "  2   -3.590  -4.060721\n",
       "  3   -4.632  -3.889902\n",
       "  4    0.522   0.246526\n",
       "  ..     ...        ...\n",
       "  44  -3.538  -4.939131\n",
       "  45  -1.716  -3.213799\n",
       "  46  -3.931 -12.041817\n",
       "  47  -7.800  -8.380328\n",
       "  48  -5.270  -3.836800\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 64.88649988174438,\n",
       "  'mean_mse': 2.2737326622009277,\n",
       "  'mean_l1': 0.9495567083358765,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [256, 256, 256],\n",
       "  'predictor_hidden_feats': 512,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=256, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=256, bias=True)\n",
       "          (bn_layer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-2): 2 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=256, out=256, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (bn_layer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -4.370 -3.543158\n",
       "  1   -3.931 -2.238003\n",
       "  2   -2.350 -2.298440\n",
       "  3   -3.460 -4.757877\n",
       "  4   -3.401 -2.019602\n",
       "  ..     ...       ...\n",
       "  44  -3.180 -3.281898\n",
       "  45  -2.266 -1.593260\n",
       "  46  -3.094 -3.007890\n",
       "  47  -0.742 -2.328849\n",
       "  48  -6.780 -6.563923\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 67.0508987903595,\n",
       "  'mean_mse': 1.1427469849586487,\n",
       "  'mean_l1': 0.8331337869167328,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [256, 256, 256],\n",
       "  'predictor_hidden_feats': 1024,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=256, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=256, bias=True)\n",
       "          (bn_layer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-2): 2 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=256, out=256, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (bn_layer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=1024, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -3.220 -3.400239\n",
       "  1   -5.190 -4.896808\n",
       "  2   -1.850 -2.658645\n",
       "  3   -3.060 -2.857524\n",
       "  4    0.300 -0.886302\n",
       "  ..     ...       ...\n",
       "  44  -2.160 -0.867956\n",
       "  45  -3.590 -3.604812\n",
       "  46  -3.171 -4.571527\n",
       "  47  -9.332 -7.774680\n",
       "  48  -2.932 -1.891243\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 33.9969642162323,\n",
       "  'mean_mse': 3.2158920764923096,\n",
       "  'mean_l1': 1.106164574623108,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [256, 256, 256, 256],\n",
       "  'predictor_hidden_feats': 128,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=256, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=256, bias=True)\n",
       "          (bn_layer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-3): 3 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=256, out=256, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (bn_layer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real     y_pred\n",
       "  0   -5.400  -5.642581\n",
       "  1   -4.950  -3.925714\n",
       "  2   -3.401  -2.570269\n",
       "  3   -5.190  -3.980259\n",
       "  4   -4.047  -3.835723\n",
       "  ..     ...        ...\n",
       "  44  -6.800 -17.461395\n",
       "  45  -5.270  -4.213109\n",
       "  46  -3.451  -3.379315\n",
       "  47  -1.740  -3.145689\n",
       "  48  -2.461  -2.086826\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 31.426615238189697,\n",
       "  'mean_mse': 7.585700511932373,\n",
       "  'mean_l1': 1.2393844723701477,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [256, 256, 256, 256],\n",
       "  'predictor_hidden_feats': 256,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=256, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=256, bias=True)\n",
       "          (bn_layer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-3): 3 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=256, out=256, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (bn_layer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -3.590 -5.314023\n",
       "  1   -3.620 -2.875650\n",
       "  2   -2.060 -1.681560\n",
       "  3   -6.680 -6.236510\n",
       "  4   -1.640 -1.915299\n",
       "  ..     ...       ...\n",
       "  44  -2.461 -2.679510\n",
       "  45  -2.349 -1.663255\n",
       "  46  -3.120 -3.682847\n",
       "  47  -3.171 -3.000613\n",
       "  48  -4.160 -4.433457\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 44.829387187957764,\n",
       "  'mean_mse': 1.7246347665786743,\n",
       "  'mean_l1': 0.907240480184555,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [256, 256, 256, 256],\n",
       "  'predictor_hidden_feats': 512,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=256, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=256, bias=True)\n",
       "          (bn_layer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-3): 3 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=256, out=256, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (bn_layer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -5.915 -4.409555\n",
       "  1   -2.120 -1.196736\n",
       "  2   -4.634 -3.323439\n",
       "  3   -4.883 -3.127293\n",
       "  4   -5.410 -4.903347\n",
       "  ..     ...       ...\n",
       "  44  -3.800 -2.957428\n",
       "  45  -3.246 -4.326587\n",
       "  46  -2.780 -4.603909\n",
       "  47  -2.060 -1.944976\n",
       "  48   0.300 -0.697726\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 59.71824836730957,\n",
       "  'mean_mse': 6.065441846847534,\n",
       "  'mean_l1': 1.1857866048812866,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [256, 256, 256, 256],\n",
       "  'predictor_hidden_feats': 1024,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=256, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=256, bias=True)\n",
       "          (bn_layer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-3): 3 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=256, out=256, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (bn_layer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=1024, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -4.799 -4.314804\n",
       "  1   -7.800 -8.225785\n",
       "  2   -1.640 -2.257568\n",
       "  3   -4.883 -3.404646\n",
       "  4   -0.400 -1.500824\n",
       "  ..     ...       ...\n",
       "  44  -4.402 -3.848578\n",
       "  45  -6.680 -6.212323\n",
       "  46  -3.658 -3.448907\n",
       "  47  -4.925 -3.886846\n",
       "  48  -4.950 -3.829596\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 18.440919876098633,\n",
       "  'mean_mse': 1.3970991969108582,\n",
       "  'mean_l1': 0.8211726248264313,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [512],\n",
       "  'predictor_hidden_feats': 128,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=512, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=512, bias=True)\n",
       "          (bn_layer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=1024, out_features=128, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -5.190 -4.441854\n",
       "  1   -3.060 -2.054903\n",
       "  2   -2.982 -2.457237\n",
       "  3   -8.040 -6.476484\n",
       "  4   -4.805 -4.650999\n",
       "  ..     ...       ...\n",
       "  44  -4.376 -4.790390\n",
       "  45  -4.950 -4.743901\n",
       "  46  -2.320 -2.264499\n",
       "  47  -8.000 -7.424648\n",
       "  48  -3.931 -5.898610\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 31.011533737182617,\n",
       "  'mean_mse': 0.8196833431720734,\n",
       "  'mean_l1': 0.694101870059967,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [512],\n",
       "  'predictor_hidden_feats': 256,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=512, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=512, bias=True)\n",
       "          (bn_layer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -4.173 -4.680129\n",
       "  1   -4.925 -4.627719\n",
       "  2   -6.780 -7.286511\n",
       "  3   -5.696 -3.152400\n",
       "  4   -4.380 -5.547694\n",
       "  ..     ...       ...\n",
       "  44  -5.190 -4.252111\n",
       "  45  -4.743 -4.265701\n",
       "  46  -9.018 -7.387994\n",
       "  47  -3.880 -3.611667\n",
       "  48  -4.047 -4.522079\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 39.79138112068176,\n",
       "  'mean_mse': 1.215984284877777,\n",
       "  'mean_l1': 0.891100138425827,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [64, 64],\n",
       "  'predictor_hidden_feats': 256,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=64, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=64, bias=True)\n",
       "          (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=64, out=64, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=128, out_features=256, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -2.120 -1.079656\n",
       "  1   -3.180 -2.919507\n",
       "  2   -5.400 -6.016766\n",
       "  3   -4.402 -3.912643\n",
       "  4   -5.190 -4.427773\n",
       "  ..     ...       ...\n",
       "  44  -4.173 -4.644856\n",
       "  45  -5.915 -3.188791\n",
       "  46  -5.220 -5.230361\n",
       "  47  -1.250 -0.831232\n",
       "  48  -3.460 -2.924665\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 19.6031813621521,\n",
       "  'mean_mse': 1.0340801775455475,\n",
       "  'mean_l1': 0.7725471258163452,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [512],\n",
       "  'predictor_hidden_feats': 512,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=512, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=512, bias=True)\n",
       "          (bn_layer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -2.070 -2.158291\n",
       "  1   -3.930 -3.700836\n",
       "  2   -5.270 -5.042744\n",
       "  3   -8.000 -7.629576\n",
       "  4   -1.800 -0.977911\n",
       "  ..     ...       ...\n",
       "  44  -3.043 -3.373606\n",
       "  45  -2.320 -2.094139\n",
       "  46  -4.120 -3.940206\n",
       "  47  -5.410 -4.592276\n",
       "  48  -4.743 -4.073710\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 48.931334495544434,\n",
       "  'mean_mse': 1.0073366463184357,\n",
       "  'mean_l1': 0.7446196973323822,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [512],\n",
       "  'predictor_hidden_feats': 1024,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=512, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=512, bias=True)\n",
       "          (bn_layer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=1024, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -2.676 -3.309111\n",
       "  1   -3.800 -4.628699\n",
       "  2   -4.634 -3.921031\n",
       "  3   -3.246 -5.893277\n",
       "  4   -2.349 -1.891644\n",
       "  ..     ...       ...\n",
       "  44  -1.960 -3.056798\n",
       "  45  -3.180 -2.754234\n",
       "  46  -3.583 -2.617447\n",
       "  47  -3.043 -3.888870\n",
       "  48  -5.190 -4.631115\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 39.23886036872864,\n",
       "  'mean_mse': 1.5165648758411407,\n",
       "  'mean_l1': 0.8662446439266205,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [512, 512],\n",
       "  'predictor_hidden_feats': 128,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=512, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=512, bias=True)\n",
       "          (bn_layer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=512, out=512, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (bn_layer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=1024, out_features=128, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -2.700 -2.855096\n",
       "  1   -7.280 -5.219716\n",
       "  2   -2.920 -1.938317\n",
       "  3   -1.850 -2.920689\n",
       "  4   -1.990 -2.366584\n",
       "  ..     ...       ...\n",
       "  44  -3.880 -3.666046\n",
       "  45  -2.676 -3.110886\n",
       "  46  -2.780 -4.061741\n",
       "  47  -2.320 -2.404273\n",
       "  48  -8.000 -8.484308\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 43.46300745010376,\n",
       "  'mean_mse': 1.65733402967453,\n",
       "  'mean_l1': 0.836969405412674,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [512, 512],\n",
       "  'predictor_hidden_feats': 256,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=512, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=512, bias=True)\n",
       "          (bn_layer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=512, out=512, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (bn_layer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -2.338 -2.335159\n",
       "  1   -4.160 -4.759613\n",
       "  2   -4.883 -3.074960\n",
       "  3   -6.680 -6.640576\n",
       "  4   -4.120 -4.494097\n",
       "  ..     ...       ...\n",
       "  44  -3.590 -3.898415\n",
       "  45  -3.290 -2.689603\n",
       "  46  -4.950 -3.921059\n",
       "  47  -1.850 -2.654863\n",
       "  48  -4.799 -4.534502\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 35.577319383621216,\n",
       "  'mean_mse': 1.3348070681095123,\n",
       "  'mean_l1': 0.8358984291553497,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [512, 512],\n",
       "  'predictor_hidden_feats': 512,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=512, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=512, bias=True)\n",
       "          (bn_layer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=512, out=512, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (bn_layer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -8.000 -7.317903\n",
       "  1   -1.990 -2.640665\n",
       "  2   -6.780 -6.426686\n",
       "  3   -4.310 -4.012332\n",
       "  4   -0.400 -0.434621\n",
       "  ..     ...       ...\n",
       "  44  -3.930 -2.293896\n",
       "  45  -2.338 -3.651011\n",
       "  46  -5.270 -3.466949\n",
       "  47  -2.700 -3.284687\n",
       "  48  -2.320 -2.996588\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 34.680320501327515,\n",
       "  'mean_mse': 1.6075645685195923,\n",
       "  'mean_l1': 0.9737042784690857,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [512, 512],\n",
       "  'predictor_hidden_feats': 1024,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=512, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=512, bias=True)\n",
       "          (bn_layer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=512, out=512, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (bn_layer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=1024, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0    0.522 -0.542237\n",
       "  1   -5.270 -5.511364\n",
       "  2   -4.805 -3.697815\n",
       "  3   -3.800 -3.935678\n",
       "  4   -3.658 -3.388335\n",
       "  ..     ...       ...\n",
       "  44  -3.050 -2.871524\n",
       "  45  -2.350 -2.934321\n",
       "  46  -8.040 -7.010878\n",
       "  47  -3.672 -3.515472\n",
       "  48  -3.610 -2.604361\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 51.694236755371094,\n",
       "  'mean_mse': 1.9596003592014313,\n",
       "  'mean_l1': 0.8621276021003723,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [512, 512, 512],\n",
       "  'predictor_hidden_feats': 128,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=512, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=512, bias=True)\n",
       "          (bn_layer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-2): 2 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=512, out=512, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (bn_layer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=1024, out_features=128, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -3.290 -3.156161\n",
       "  1   -5.220 -5.092998\n",
       "  2   -3.094 -3.159121\n",
       "  3   -5.400 -5.650304\n",
       "  4   -2.676 -2.978156\n",
       "  ..     ...       ...\n",
       "  44  -6.237 -5.889016\n",
       "  45  -2.700 -2.530277\n",
       "  46  -1.899 -2.132233\n",
       "  47  -5.000 -4.638157\n",
       "  48  -4.634 -4.126410\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 85.60661578178406,\n",
       "  'mean_mse': 1.4245330691337585,\n",
       "  'mean_l1': 0.7512987554073334,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [512, 512, 512],\n",
       "  'predictor_hidden_feats': 256,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=512, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=512, bias=True)\n",
       "          (bn_layer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-2): 2 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=512, out=512, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (bn_layer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -6.780 -1.990355\n",
       "  1   -3.180 -3.223784\n",
       "  2   -1.040 -1.825807\n",
       "  3   -3.060 -2.106782\n",
       "  4   -5.190 -4.837547\n",
       "  ..     ...       ...\n",
       "  44  -5.190 -4.882609\n",
       "  45  -4.370 -3.884986\n",
       "  46  -4.805 -4.718503\n",
       "  47  -2.350 -3.110787\n",
       "  48  -2.060 -2.010581\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 56.36249828338623,\n",
       "  'mean_mse': 1.39113050699234,\n",
       "  'mean_l1': 0.8066555261611938,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [512, 512, 512],\n",
       "  'predictor_hidden_feats': 512,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=512, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=512, bias=True)\n",
       "          (bn_layer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-2): 2 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=512, out=512, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (bn_layer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -3.583 -0.650116\n",
       "  1   -3.094 -3.160447\n",
       "  2   -4.805 -4.040130\n",
       "  3   -2.060 -2.071943\n",
       "  4   -5.410 -5.706028\n",
       "  ..     ...       ...\n",
       "  44  -2.932 -1.835797\n",
       "  45  -2.461 -2.461827\n",
       "  46  -4.173 -4.617492\n",
       "  47  -6.860 -6.069307\n",
       "  48  -4.450 -3.108248\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 34.85790491104126,\n",
       "  'mean_mse': 1.9884393811225891,\n",
       "  'mean_l1': 0.9462423920631409,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [512, 512, 512],\n",
       "  'predictor_hidden_feats': 1024,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=512, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=512, bias=True)\n",
       "          (bn_layer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-2): 2 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=512, out=512, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (bn_layer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=1024, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -2.920 -3.324191\n",
       "  1   -3.220 -2.917831\n",
       "  2   -2.932 -1.324366\n",
       "  3   -2.320 -1.736355\n",
       "  4   -2.266 -0.300803\n",
       "  ..     ...       ...\n",
       "  44  -0.742 -3.175610\n",
       "  45  -1.800 -2.568467\n",
       "  46  -2.780 -4.111214\n",
       "  47  -3.290 -3.040609\n",
       "  48  -3.451 -3.427146\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 25.60805630683899,\n",
       "  'mean_mse': 1.511313259601593,\n",
       "  'mean_l1': 0.9614688158035278,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [64, 64],\n",
       "  'predictor_hidden_feats': 512,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=64, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=64, bias=True)\n",
       "          (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=64, out=64, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -3.290 -2.621056\n",
       "  1   -2.100 -1.097796\n",
       "  2   -3.120 -3.962737\n",
       "  3   -1.899 -1.598539\n",
       "  4   -4.173 -4.870955\n",
       "  ..     ...       ...\n",
       "  44  -5.190 -3.982471\n",
       "  45  -4.402 -4.170131\n",
       "  46  -3.180 -2.291596\n",
       "  47  -8.600 -6.902260\n",
       "  48  -5.270 -3.871854\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 41.89138865470886,\n",
       "  'mean_mse': 8.46771764755249,\n",
       "  'mean_l1': 1.1393769979476929,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [512, 512, 512, 512],\n",
       "  'predictor_hidden_feats': 128,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=512, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=512, bias=True)\n",
       "          (bn_layer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-3): 3 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=512, out=512, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (bn_layer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=1024, out_features=128, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -3.590 -3.719831\n",
       "  1   -3.094 -3.308612\n",
       "  2   -2.932 -1.705921\n",
       "  3   -3.021 -5.103544\n",
       "  4   -8.600 -6.669661\n",
       "  ..     ...       ...\n",
       "  44  -4.370 -3.973556\n",
       "  45  -1.040 -2.453128\n",
       "  46  -2.700 -2.032150\n",
       "  47  -7.280 -5.475136\n",
       "  48  -2.266 -1.465481\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 50.38999032974243,\n",
       "  'mean_mse': 3.3577005863189697,\n",
       "  'mean_l1': 0.9681451320648193,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [512, 512, 512, 512],\n",
       "  'predictor_hidden_feats': 256,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=512, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=512, bias=True)\n",
       "          (bn_layer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-3): 3 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=512, out=512, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (bn_layer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -3.460 -4.861591\n",
       "  1   -8.600 -6.703329\n",
       "  2   -1.716 -2.982683\n",
       "  3   -7.280 -4.615411\n",
       "  4   -2.780 -4.177316\n",
       "  ..     ...       ...\n",
       "  44  -1.960 -2.081944\n",
       "  45  -3.930 -2.494255\n",
       "  46  -1.040 -1.429340\n",
       "  47  -2.920 -2.919492\n",
       "  48  -2.070 -1.120757\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 66.70919060707092,\n",
       "  'mean_mse': 3.029941439628601,\n",
       "  'mean_l1': 0.9580661058425903,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [512, 512, 512, 512],\n",
       "  'predictor_hidden_feats': 512,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=512, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=512, bias=True)\n",
       "          (bn_layer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-3): 3 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=512, out=512, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (bn_layer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -6.860 -5.748713\n",
       "  1   -4.376 -4.302111\n",
       "  2   -3.460 -4.032753\n",
       "  3   -4.402 -3.877770\n",
       "  4   -8.000 -7.607932\n",
       "  ..     ...       ...\n",
       "  44  -4.634 -4.054945\n",
       "  45  -3.610 -3.379505\n",
       "  46  -0.400 -0.701213\n",
       "  47  -4.310 -2.404506\n",
       "  48  -3.451 -3.536663\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 57.57561230659485,\n",
       "  'mean_mse': 2.835737466812134,\n",
       "  'mean_l1': 0.954353541135788,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [512, 512, 512, 512],\n",
       "  'predictor_hidden_feats': 1024,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=512, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=512, bias=True)\n",
       "          (bn_layer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-3): 3 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=512, out=512, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (bn_layer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=1024, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -5.190 -4.218230\n",
       "  1   -3.180 -2.468305\n",
       "  2   -8.040 -6.832560\n",
       "  3   -4.173 -4.894733\n",
       "  4   -0.600 -1.049178\n",
       "  ..     ...       ...\n",
       "  44  -3.460 -4.792624\n",
       "  45  -3.538 -4.371584\n",
       "  46  -1.990 -1.949375\n",
       "  47  -3.460 -2.401754\n",
       "  48  -6.237 -7.232329\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 55.60633850097656,\n",
       "  'mean_mse': 0.9875992834568024,\n",
       "  'mean_l1': 0.7310875654220581,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [64, 64],\n",
       "  'predictor_hidden_feats': 1024,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=64, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=64, bias=True)\n",
       "          (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=64, out=64, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=128, out_features=1024, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=1024, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -2.320 -1.477617\n",
       "  1   -3.460 -1.758984\n",
       "  2   -3.610 -3.151979\n",
       "  3   -4.445 -5.319401\n",
       "  4   -3.043 -3.225101\n",
       "  ..     ...       ...\n",
       "  44  -8.600 -7.070382\n",
       "  45  -8.000 -7.640753\n",
       "  46  -5.270 -6.394108\n",
       "  47  -1.716 -3.663622\n",
       "  48  -2.780 -4.387049\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 38.16122102737427,\n",
       "  'mean_mse': 1.4608441591262817,\n",
       "  'mean_l1': 0.9751805067062378,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [64, 64, 64],\n",
       "  'predictor_hidden_feats': 128,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=64, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=64, bias=True)\n",
       "          (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-2): 2 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=64, out=64, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -2.700 -3.033841\n",
       "  1   -6.237 -4.769829\n",
       "  2   -3.583 -2.209256\n",
       "  3   -4.047 -5.879200\n",
       "  4   -8.040 -5.596009\n",
       "  ..     ...       ...\n",
       "  44  -2.160 -1.450571\n",
       "  45   0.522 -0.121413\n",
       "  46  -1.960 -2.932544\n",
       "  47   0.300 -0.897069\n",
       "  48  -2.349 -2.531119\n",
       "  \n",
       "  [113 rows x 2 columns],\n",
       "  'el_time': 31.033472776412964,\n",
       "  'mean_mse': 3.756445050239563,\n",
       "  'mean_l1': 1.2170975506305695,\n",
       "  'apply_scaffold_split': True,\n",
       "  'hidden_feats': [64, 64, 64],\n",
       "  'predictor_hidden_feats': 256,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=64, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=64, bias=True)\n",
       "          (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-2): 2 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=64, out=64, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=128, out_features=256, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file1_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = file1_load[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pred_df':     y_real    y_pred\n",
       " 0   -3.672 -2.948319\n",
       " 1   -3.050 -2.109394\n",
       " 2   -3.620 -2.985116\n",
       " 3   -3.658 -3.416189\n",
       " 4   -2.120 -1.447404\n",
       " ..     ...       ...\n",
       " 44  -4.743 -4.917334\n",
       " 45  -8.600 -7.652426\n",
       " 46  -3.880 -4.057945\n",
       " 47  -3.401 -2.210320\n",
       " 48  -3.460 -3.153419\n",
       " \n",
       " [113 rows x 2 columns],\n",
       " 'el_time': 54.7974739074707,\n",
       " 'mean_mse': 1.0798689126968384,\n",
       " 'mean_l1': 0.8108006715774536,\n",
       " 'apply_scaffold_split': True,\n",
       " 'hidden_feats': [64],\n",
       " 'predictor_hidden_feats': 128,\n",
       " 'apply_random_aggregations': False,\n",
       " 'learning_rate': 0.001,\n",
       " 'model_type': 'DGL',\n",
       " 'model': GCNPredictor(\n",
       "   (gnn): GCN(\n",
       "     (gnn_layers): ModuleList(\n",
       "       (0): GCNLayer(\n",
       "         (graph_conv): GraphConv(in=74, out=64, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "         (dropout): Dropout(p=0.0, inplace=False)\n",
       "         (res_connection): Linear(in_features=74, out_features=64, bias=True)\n",
       "         (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (readout): WeightedSumAndMax(\n",
       "     (weight_and_sum): WeightAndSum(\n",
       "       (atom_weighting): Sequential(\n",
       "         (0): Linear(in_features=64, out_features=1, bias=True)\n",
       "         (1): Sigmoid()\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (predict): MLPPredictor(\n",
       "     (predict): Sequential(\n",
       "       (0): Dropout(p=0.0, inplace=False)\n",
       "       (1): Linear(in_features=128, out_features=128, bias=True)\n",
       "       (2): ReLU()\n",
       "       (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (4): Linear(in_features=128, out_features=1, bias=True)\n",
       "     )\n",
       "   )\n",
       " )}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_real</th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3.672</td>\n",
       "      <td>-2.948319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3.050</td>\n",
       "      <td>-2.109394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-3.620</td>\n",
       "      <td>-2.985116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3.658</td>\n",
       "      <td>-3.416189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.120</td>\n",
       "      <td>-1.447404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>-4.743</td>\n",
       "      <td>-4.917334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>-8.600</td>\n",
       "      <td>-7.652426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>-3.880</td>\n",
       "      <td>-4.057945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>-3.401</td>\n",
       "      <td>-2.210320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>-3.460</td>\n",
       "      <td>-3.153419</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>113 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    y_real    y_pred\n",
       "0   -3.672 -2.948319\n",
       "1   -3.050 -2.109394\n",
       "2   -3.620 -2.985116\n",
       "3   -3.658 -3.416189\n",
       "4   -2.120 -1.447404\n",
       "..     ...       ...\n",
       "44  -4.743 -4.917334\n",
       "45  -8.600 -7.652426\n",
       "46  -3.880 -4.057945\n",
       "47  -3.401 -2.210320\n",
       "48  -3.460 -3.153419\n",
       "\n",
       "[113 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff[\"pred_df\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pred_df':     y_real    y_pred\n",
       " 0   -3.672 -2.948319\n",
       " 1   -3.050 -2.109394\n",
       " 2   -3.620 -2.985116\n",
       " 3   -3.658 -3.416189\n",
       " 4   -2.120 -1.447404\n",
       " ..     ...       ...\n",
       " 44  -4.743 -4.917334\n",
       " 45  -8.600 -7.652426\n",
       " 46  -3.880 -4.057945\n",
       " 47  -3.401 -2.210320\n",
       " 48  -3.460 -3.153419\n",
       " \n",
       " [113 rows x 2 columns],\n",
       " 'el_time': 54.7974739074707,\n",
       " 'mean_mse': 1.0798689126968384,\n",
       " 'mean_l1': 0.8108006715774536,\n",
       " 'apply_scaffold_split': True,\n",
       " 'hidden_feats': [64],\n",
       " 'predictor_hidden_feats': 128,\n",
       " 'apply_random_aggregations': False,\n",
       " 'learning_rate': 0.001,\n",
       " 'model_type': 'DGL',\n",
       " 'model': GCNPredictor(\n",
       "   (gnn): GCN(\n",
       "     (gnn_layers): ModuleList(\n",
       "       (0): GCNLayer(\n",
       "         (graph_conv): GraphConv(in=74, out=64, normalization=none, activation=<function relu at 0x0000025328863B80>)\n",
       "         (dropout): Dropout(p=0.0, inplace=False)\n",
       "         (res_connection): Linear(in_features=74, out_features=64, bias=True)\n",
       "         (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (readout): WeightedSumAndMax(\n",
       "     (weight_and_sum): WeightAndSum(\n",
       "       (atom_weighting): Sequential(\n",
       "         (0): Linear(in_features=64, out_features=1, bias=True)\n",
       "         (1): Sigmoid()\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (predict): MLPPredictor(\n",
       "     (predict): Sequential(\n",
       "       (0): Dropout(p=0.0, inplace=False)\n",
       "       (1): Linear(in_features=128, out_features=128, bias=True)\n",
       "       (2): ReLU()\n",
       "       (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (4): Linear(in_features=128, out_features=1, bias=True)\n",
       "     )\n",
       "   )\n",
       " )}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_to_extract = ['hidden_feats', 'predictor_hidden_feats']\n",
    "\n",
    "plot1 = []\n",
    "for run in file1_load:\n",
    "    lossvalue_mse = run[\"mean_mse\"]\n",
    "    lossvalue_l1 = run[\"mean_l1\"]\n",
    "    \n",
    "    extracted_dict = {key: run[key] for key in keys_to_extract if key in run}\n",
    "    extracted_dict[\"lossvalue_mse\"] = lossvalue_mse\n",
    "    extracted_dict[\"lossvalue_l1\"] = lossvalue_l1\n",
    "    df = pd.DataFrame([extracted_dict])\n",
    "    #plot1.append([lossvalue, df])\n",
    "    plot1.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[  hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0         [64]                     128       1.079869      0.810801,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0         [64]                     256       1.072731      0.780608,\n",
       "    hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [64, 64, 64]                     512        2.41775      1.059892,\n",
       "    hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [64, 64, 64]                    1024       4.786823      1.386452,\n",
       "        hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [64, 64, 64, 64]                     128       4.752409      1.065886,\n",
       "        hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [64, 64, 64, 64]                     256       6.754857       1.41673,\n",
       "        hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [64, 64, 64, 64]                     512       4.943508      1.206407,\n",
       "        hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [64, 64, 64, 64]                    1024       4.741706       1.29687,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0        [128]                     128        0.94237      0.766334,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0        [128]                     256       0.973967      0.791788,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0        [128]                     512         1.0059       0.78243,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0        [128]                    1024       1.188769      0.824239,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0         [64]                     512       0.971687      0.747013,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0   [128, 128]                     128       1.110174      0.812645,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0   [128, 128]                     256       0.940755      0.755797,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0   [128, 128]                     512       1.294355       0.79734,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0   [128, 128]                    1024       2.123503      0.902056,\n",
       "       hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [128, 128, 128]                     128       1.327627        0.8684,\n",
       "       hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [128, 128, 128]                     256       5.108416       1.22188,\n",
       "       hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [128, 128, 128]                     512       1.975284       0.96301,\n",
       "       hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [128, 128, 128]                    1024       4.011961      1.083603,\n",
       "            hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [128, 128, 128, 128]                     128       7.329994      1.201138,\n",
       "            hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [128, 128, 128, 128]                     256       2.184838      0.921111,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0         [64]                    1024        1.17404      0.836096,\n",
       "            hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [128, 128, 128, 128]                     512       4.346828       1.14202,\n",
       "            hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [128, 128, 128, 128]                    1024       4.839312      1.127221,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0        [256]                     128        1.00909      0.739197,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0        [256]                     256       0.967699      0.740711,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0        [256]                     512       1.250001      0.826729,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0        [256]                    1024       1.528696      0.815497,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0   [256, 256]                     128       0.888432      0.754004,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0   [256, 256]                     256        1.46013      0.814393,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0   [256, 256]                     512       1.422254      0.821105,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0   [256, 256]                    1024       1.242949      0.826995,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0     [64, 64]                     128       1.635544      0.899366,\n",
       "       hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [256, 256, 256]                     128       1.872141      0.887123,\n",
       "       hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [256, 256, 256]                     256       1.831527       0.90802,\n",
       "       hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [256, 256, 256]                     512       2.273733      0.949557,\n",
       "       hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [256, 256, 256]                    1024       1.142747      0.833134,\n",
       "            hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [256, 256, 256, 256]                     128       3.215892      1.106165,\n",
       "            hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [256, 256, 256, 256]                     256       7.585701      1.239384,\n",
       "            hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [256, 256, 256, 256]                     512       1.724635       0.90724,\n",
       "            hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [256, 256, 256, 256]                    1024       6.065442      1.185787,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0        [512]                     128       1.397099      0.821173,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0        [512]                     256       0.819683      0.694102,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0     [64, 64]                     256       1.215984        0.8911,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0        [512]                     512        1.03408      0.772547,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0        [512]                    1024       1.007337       0.74462,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0   [512, 512]                     128       1.516565      0.866245,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0   [512, 512]                     256       1.657334      0.836969,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0   [512, 512]                     512       1.334807      0.835898,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0   [512, 512]                    1024       1.607565      0.973704,\n",
       "       hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [512, 512, 512]                     128         1.9596      0.862128,\n",
       "       hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [512, 512, 512]                     256       1.424533      0.751299,\n",
       "       hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [512, 512, 512]                     512       1.391131      0.806656,\n",
       "       hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [512, 512, 512]                    1024       1.988439      0.946242,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0     [64, 64]                     512       1.511313      0.961469,\n",
       "            hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [512, 512, 512, 512]                     128       8.467718      1.139377,\n",
       "            hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [512, 512, 512, 512]                     256       3.357701      0.968145,\n",
       "            hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [512, 512, 512, 512]                     512       3.029941      0.958066,\n",
       "            hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [512, 512, 512, 512]                    1024       2.835737      0.954354,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0     [64, 64]                    1024       0.987599      0.731088,\n",
       "    hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [64, 64, 64]                     128       1.460844      0.975181,\n",
       "    hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [64, 64, 64]                     256       3.756445      1.217098]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hidden_feats</th>\n",
       "      <th>predictor_hidden_feats</th>\n",
       "      <th>lossvalue_mse</th>\n",
       "      <th>lossvalue_l1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64, 64, 64]</td>\n",
       "      <td>128</td>\n",
       "      <td>4.752409</td>\n",
       "      <td>1.065886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       "0  [64, 64, 64, 64]                     128       4.752409      1.065886"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot1[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullplot1 = pd.concat(plot1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hidden_feats</th>\n",
       "      <th>predictor_hidden_feats</th>\n",
       "      <th>lossvalue_mse</th>\n",
       "      <th>lossvalue_l1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64]</td>\n",
       "      <td>128</td>\n",
       "      <td>1.079869</td>\n",
       "      <td>0.810801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64]</td>\n",
       "      <td>256</td>\n",
       "      <td>1.072731</td>\n",
       "      <td>0.780608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64, 64]</td>\n",
       "      <td>512</td>\n",
       "      <td>2.417750</td>\n",
       "      <td>1.059892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64, 64]</td>\n",
       "      <td>1024</td>\n",
       "      <td>4.786823</td>\n",
       "      <td>1.386452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64, 64, 64]</td>\n",
       "      <td>128</td>\n",
       "      <td>4.752409</td>\n",
       "      <td>1.065886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512, 512, 512, 512]</td>\n",
       "      <td>512</td>\n",
       "      <td>3.029941</td>\n",
       "      <td>0.958066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512, 512, 512, 512]</td>\n",
       "      <td>1024</td>\n",
       "      <td>2.835737</td>\n",
       "      <td>0.954354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64]</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.987599</td>\n",
       "      <td>0.731088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64, 64]</td>\n",
       "      <td>128</td>\n",
       "      <td>1.460844</td>\n",
       "      <td>0.975181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64, 64]</td>\n",
       "      <td>256</td>\n",
       "      <td>3.756445</td>\n",
       "      <td>1.217098</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       "0                   [64]                     128       1.079869      0.810801\n",
       "0                   [64]                     256       1.072731      0.780608\n",
       "0           [64, 64, 64]                     512       2.417750      1.059892\n",
       "0           [64, 64, 64]                    1024       4.786823      1.386452\n",
       "0       [64, 64, 64, 64]                     128       4.752409      1.065886\n",
       "..                   ...                     ...            ...           ...\n",
       "0   [512, 512, 512, 512]                     512       3.029941      0.958066\n",
       "0   [512, 512, 512, 512]                    1024       2.835737      0.954354\n",
       "0               [64, 64]                    1024       0.987599      0.731088\n",
       "0           [64, 64, 64]                     128       1.460844      0.975181\n",
       "0           [64, 64, 64]                     256       3.756445      1.217098\n",
       "\n",
       "[64 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullplot1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hidden_feats</th>\n",
       "      <th>predictor_hidden_feats</th>\n",
       "      <th>lossvalue_l1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64]</td>\n",
       "      <td>128</td>\n",
       "      <td>0.810801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64]</td>\n",
       "      <td>256</td>\n",
       "      <td>0.780608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64, 64]</td>\n",
       "      <td>512</td>\n",
       "      <td>1.059892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64, 64]</td>\n",
       "      <td>1024</td>\n",
       "      <td>1.386452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64, 64, 64]</td>\n",
       "      <td>128</td>\n",
       "      <td>1.065886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512, 512, 512, 512]</td>\n",
       "      <td>512</td>\n",
       "      <td>0.958066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512, 512, 512, 512]</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.954354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64]</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.731088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64, 64]</td>\n",
       "      <td>128</td>\n",
       "      <td>0.975181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64, 64]</td>\n",
       "      <td>256</td>\n",
       "      <td>1.217098</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            hidden_feats  predictor_hidden_feats  lossvalue_l1\n",
       "0                   [64]                     128      0.810801\n",
       "0                   [64]                     256      0.780608\n",
       "0           [64, 64, 64]                     512      1.059892\n",
       "0           [64, 64, 64]                    1024      1.386452\n",
       "0       [64, 64, 64, 64]                     128      1.065886\n",
       "..                   ...                     ...           ...\n",
       "0   [512, 512, 512, 512]                     512      0.958066\n",
       "0   [512, 512, 512, 512]                    1024      0.954354\n",
       "0               [64, 64]                    1024      0.731088\n",
       "0           [64, 64, 64]                     128      0.975181\n",
       "0           [64, 64, 64]                     256      1.217098\n",
       "\n",
       "[64 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullplot1.drop(columns=\"lossvalue_mse\", axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1.079869\n",
       "0    1.072731\n",
       "0    2.417750\n",
       "0    4.786823\n",
       "0    4.752409\n",
       "       ...   \n",
       "0    3.029941\n",
       "0    2.835737\n",
       "0    0.987599\n",
       "0    1.460844\n",
       "0    3.756445\n",
       "Name: lossvalue_mse, Length: 64, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullplot1[\"lossvalue_mse\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullplot1['Combined Label'] = fullplot1.drop(columns=[\"lossvalue_mse\", \"lossvalue_l1\"], axis=1).astype(str).agg(' - '.join, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hidden_feats</th>\n",
       "      <th>predictor_hidden_feats</th>\n",
       "      <th>lossvalue_mse</th>\n",
       "      <th>lossvalue_l1</th>\n",
       "      <th>Combined Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512]</td>\n",
       "      <td>256</td>\n",
       "      <td>0.819683</td>\n",
       "      <td>0.694102</td>\n",
       "      <td>[512] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>128</td>\n",
       "      <td>0.888432</td>\n",
       "      <td>0.754004</td>\n",
       "      <td>[256, 256] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>256</td>\n",
       "      <td>0.940755</td>\n",
       "      <td>0.755797</td>\n",
       "      <td>[128, 128] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128]</td>\n",
       "      <td>128</td>\n",
       "      <td>0.942370</td>\n",
       "      <td>0.766334</td>\n",
       "      <td>[128] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256]</td>\n",
       "      <td>256</td>\n",
       "      <td>0.967699</td>\n",
       "      <td>0.740711</td>\n",
       "      <td>[256] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64]</td>\n",
       "      <td>512</td>\n",
       "      <td>0.971687</td>\n",
       "      <td>0.747013</td>\n",
       "      <td>[64] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128]</td>\n",
       "      <td>256</td>\n",
       "      <td>0.973967</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>[128] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64]</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.987599</td>\n",
       "      <td>0.731088</td>\n",
       "      <td>[64, 64] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128]</td>\n",
       "      <td>512</td>\n",
       "      <td>1.005900</td>\n",
       "      <td>0.782430</td>\n",
       "      <td>[128] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512]</td>\n",
       "      <td>1024</td>\n",
       "      <td>1.007337</td>\n",
       "      <td>0.744620</td>\n",
       "      <td>[512] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256]</td>\n",
       "      <td>128</td>\n",
       "      <td>1.009090</td>\n",
       "      <td>0.739197</td>\n",
       "      <td>[256] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512]</td>\n",
       "      <td>512</td>\n",
       "      <td>1.034080</td>\n",
       "      <td>0.772547</td>\n",
       "      <td>[512] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64]</td>\n",
       "      <td>256</td>\n",
       "      <td>1.072731</td>\n",
       "      <td>0.780608</td>\n",
       "      <td>[64] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64]</td>\n",
       "      <td>128</td>\n",
       "      <td>1.079869</td>\n",
       "      <td>0.810801</td>\n",
       "      <td>[64] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>128</td>\n",
       "      <td>1.110174</td>\n",
       "      <td>0.812645</td>\n",
       "      <td>[128, 128] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256, 256, 256]</td>\n",
       "      <td>1024</td>\n",
       "      <td>1.142747</td>\n",
       "      <td>0.833134</td>\n",
       "      <td>[256, 256, 256] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64]</td>\n",
       "      <td>1024</td>\n",
       "      <td>1.174040</td>\n",
       "      <td>0.836096</td>\n",
       "      <td>[64] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128]</td>\n",
       "      <td>1024</td>\n",
       "      <td>1.188769</td>\n",
       "      <td>0.824239</td>\n",
       "      <td>[128] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64]</td>\n",
       "      <td>256</td>\n",
       "      <td>1.215984</td>\n",
       "      <td>0.891100</td>\n",
       "      <td>[64, 64] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>1024</td>\n",
       "      <td>1.242949</td>\n",
       "      <td>0.826995</td>\n",
       "      <td>[256, 256] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256]</td>\n",
       "      <td>512</td>\n",
       "      <td>1.250001</td>\n",
       "      <td>0.826729</td>\n",
       "      <td>[256] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>512</td>\n",
       "      <td>1.294355</td>\n",
       "      <td>0.797340</td>\n",
       "      <td>[128, 128] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128, 128, 128]</td>\n",
       "      <td>128</td>\n",
       "      <td>1.327627</td>\n",
       "      <td>0.868400</td>\n",
       "      <td>[128, 128, 128] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512, 512]</td>\n",
       "      <td>512</td>\n",
       "      <td>1.334807</td>\n",
       "      <td>0.835898</td>\n",
       "      <td>[512, 512] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512, 512, 512]</td>\n",
       "      <td>512</td>\n",
       "      <td>1.391131</td>\n",
       "      <td>0.806656</td>\n",
       "      <td>[512, 512, 512] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512]</td>\n",
       "      <td>128</td>\n",
       "      <td>1.397099</td>\n",
       "      <td>0.821173</td>\n",
       "      <td>[512] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>512</td>\n",
       "      <td>1.422254</td>\n",
       "      <td>0.821105</td>\n",
       "      <td>[256, 256] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512, 512, 512]</td>\n",
       "      <td>256</td>\n",
       "      <td>1.424533</td>\n",
       "      <td>0.751299</td>\n",
       "      <td>[512, 512, 512] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>256</td>\n",
       "      <td>1.460130</td>\n",
       "      <td>0.814393</td>\n",
       "      <td>[256, 256] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64, 64]</td>\n",
       "      <td>128</td>\n",
       "      <td>1.460844</td>\n",
       "      <td>0.975181</td>\n",
       "      <td>[64, 64, 64] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64]</td>\n",
       "      <td>512</td>\n",
       "      <td>1.511313</td>\n",
       "      <td>0.961469</td>\n",
       "      <td>[64, 64] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512, 512]</td>\n",
       "      <td>128</td>\n",
       "      <td>1.516565</td>\n",
       "      <td>0.866245</td>\n",
       "      <td>[512, 512] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256]</td>\n",
       "      <td>1024</td>\n",
       "      <td>1.528696</td>\n",
       "      <td>0.815497</td>\n",
       "      <td>[256] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512, 512]</td>\n",
       "      <td>1024</td>\n",
       "      <td>1.607565</td>\n",
       "      <td>0.973704</td>\n",
       "      <td>[512, 512] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64]</td>\n",
       "      <td>128</td>\n",
       "      <td>1.635544</td>\n",
       "      <td>0.899366</td>\n",
       "      <td>[64, 64] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512, 512]</td>\n",
       "      <td>256</td>\n",
       "      <td>1.657334</td>\n",
       "      <td>0.836969</td>\n",
       "      <td>[512, 512] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256, 256, 256, 256]</td>\n",
       "      <td>512</td>\n",
       "      <td>1.724635</td>\n",
       "      <td>0.907240</td>\n",
       "      <td>[256, 256, 256, 256] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256, 256, 256]</td>\n",
       "      <td>256</td>\n",
       "      <td>1.831527</td>\n",
       "      <td>0.908020</td>\n",
       "      <td>[256, 256, 256] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256, 256, 256]</td>\n",
       "      <td>128</td>\n",
       "      <td>1.872141</td>\n",
       "      <td>0.887123</td>\n",
       "      <td>[256, 256, 256] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512, 512, 512]</td>\n",
       "      <td>128</td>\n",
       "      <td>1.959600</td>\n",
       "      <td>0.862128</td>\n",
       "      <td>[512, 512, 512] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128, 128, 128]</td>\n",
       "      <td>512</td>\n",
       "      <td>1.975284</td>\n",
       "      <td>0.963010</td>\n",
       "      <td>[128, 128, 128] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512, 512, 512]</td>\n",
       "      <td>1024</td>\n",
       "      <td>1.988439</td>\n",
       "      <td>0.946242</td>\n",
       "      <td>[512, 512, 512] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>1024</td>\n",
       "      <td>2.123503</td>\n",
       "      <td>0.902056</td>\n",
       "      <td>[128, 128] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128, 128, 128, 128]</td>\n",
       "      <td>256</td>\n",
       "      <td>2.184838</td>\n",
       "      <td>0.921111</td>\n",
       "      <td>[128, 128, 128, 128] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256, 256, 256]</td>\n",
       "      <td>512</td>\n",
       "      <td>2.273733</td>\n",
       "      <td>0.949557</td>\n",
       "      <td>[256, 256, 256] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64, 64]</td>\n",
       "      <td>512</td>\n",
       "      <td>2.417750</td>\n",
       "      <td>1.059892</td>\n",
       "      <td>[64, 64, 64] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512, 512, 512, 512]</td>\n",
       "      <td>1024</td>\n",
       "      <td>2.835737</td>\n",
       "      <td>0.954354</td>\n",
       "      <td>[512, 512, 512, 512] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512, 512, 512, 512]</td>\n",
       "      <td>512</td>\n",
       "      <td>3.029941</td>\n",
       "      <td>0.958066</td>\n",
       "      <td>[512, 512, 512, 512] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256, 256, 256, 256]</td>\n",
       "      <td>128</td>\n",
       "      <td>3.215892</td>\n",
       "      <td>1.106165</td>\n",
       "      <td>[256, 256, 256, 256] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512, 512, 512, 512]</td>\n",
       "      <td>256</td>\n",
       "      <td>3.357701</td>\n",
       "      <td>0.968145</td>\n",
       "      <td>[512, 512, 512, 512] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64, 64]</td>\n",
       "      <td>256</td>\n",
       "      <td>3.756445</td>\n",
       "      <td>1.217098</td>\n",
       "      <td>[64, 64, 64] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128, 128, 128]</td>\n",
       "      <td>1024</td>\n",
       "      <td>4.011961</td>\n",
       "      <td>1.083603</td>\n",
       "      <td>[128, 128, 128] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128, 128, 128, 128]</td>\n",
       "      <td>512</td>\n",
       "      <td>4.346828</td>\n",
       "      <td>1.142020</td>\n",
       "      <td>[128, 128, 128, 128] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64, 64, 64]</td>\n",
       "      <td>1024</td>\n",
       "      <td>4.741706</td>\n",
       "      <td>1.296870</td>\n",
       "      <td>[64, 64, 64, 64] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64, 64, 64]</td>\n",
       "      <td>128</td>\n",
       "      <td>4.752409</td>\n",
       "      <td>1.065886</td>\n",
       "      <td>[64, 64, 64, 64] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64, 64]</td>\n",
       "      <td>1024</td>\n",
       "      <td>4.786823</td>\n",
       "      <td>1.386452</td>\n",
       "      <td>[64, 64, 64] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128, 128, 128, 128]</td>\n",
       "      <td>1024</td>\n",
       "      <td>4.839312</td>\n",
       "      <td>1.127221</td>\n",
       "      <td>[128, 128, 128, 128] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64, 64, 64]</td>\n",
       "      <td>512</td>\n",
       "      <td>4.943508</td>\n",
       "      <td>1.206407</td>\n",
       "      <td>[64, 64, 64, 64] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128, 128, 128]</td>\n",
       "      <td>256</td>\n",
       "      <td>5.108416</td>\n",
       "      <td>1.221880</td>\n",
       "      <td>[128, 128, 128] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256, 256, 256, 256]</td>\n",
       "      <td>1024</td>\n",
       "      <td>6.065442</td>\n",
       "      <td>1.185787</td>\n",
       "      <td>[256, 256, 256, 256] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64, 64, 64]</td>\n",
       "      <td>256</td>\n",
       "      <td>6.754857</td>\n",
       "      <td>1.416730</td>\n",
       "      <td>[64, 64, 64, 64] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128, 128, 128, 128]</td>\n",
       "      <td>128</td>\n",
       "      <td>7.329994</td>\n",
       "      <td>1.201138</td>\n",
       "      <td>[128, 128, 128, 128] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256, 256, 256, 256]</td>\n",
       "      <td>256</td>\n",
       "      <td>7.585701</td>\n",
       "      <td>1.239384</td>\n",
       "      <td>[256, 256, 256, 256] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512, 512, 512, 512]</td>\n",
       "      <td>128</td>\n",
       "      <td>8.467718</td>\n",
       "      <td>1.139377</td>\n",
       "      <td>[512, 512, 512, 512] - 128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1  \\\n",
       "0                 [512]                     256       0.819683      0.694102   \n",
       "0            [256, 256]                     128       0.888432      0.754004   \n",
       "0            [128, 128]                     256       0.940755      0.755797   \n",
       "0                 [128]                     128       0.942370      0.766334   \n",
       "0                 [256]                     256       0.967699      0.740711   \n",
       "0                  [64]                     512       0.971687      0.747013   \n",
       "0                 [128]                     256       0.973967      0.791788   \n",
       "0              [64, 64]                    1024       0.987599      0.731088   \n",
       "0                 [128]                     512       1.005900      0.782430   \n",
       "0                 [512]                    1024       1.007337      0.744620   \n",
       "0                 [256]                     128       1.009090      0.739197   \n",
       "0                 [512]                     512       1.034080      0.772547   \n",
       "0                  [64]                     256       1.072731      0.780608   \n",
       "0                  [64]                     128       1.079869      0.810801   \n",
       "0            [128, 128]                     128       1.110174      0.812645   \n",
       "0       [256, 256, 256]                    1024       1.142747      0.833134   \n",
       "0                  [64]                    1024       1.174040      0.836096   \n",
       "0                 [128]                    1024       1.188769      0.824239   \n",
       "0              [64, 64]                     256       1.215984      0.891100   \n",
       "0            [256, 256]                    1024       1.242949      0.826995   \n",
       "0                 [256]                     512       1.250001      0.826729   \n",
       "0            [128, 128]                     512       1.294355      0.797340   \n",
       "0       [128, 128, 128]                     128       1.327627      0.868400   \n",
       "0            [512, 512]                     512       1.334807      0.835898   \n",
       "0       [512, 512, 512]                     512       1.391131      0.806656   \n",
       "0                 [512]                     128       1.397099      0.821173   \n",
       "0            [256, 256]                     512       1.422254      0.821105   \n",
       "0       [512, 512, 512]                     256       1.424533      0.751299   \n",
       "0            [256, 256]                     256       1.460130      0.814393   \n",
       "0          [64, 64, 64]                     128       1.460844      0.975181   \n",
       "0              [64, 64]                     512       1.511313      0.961469   \n",
       "0            [512, 512]                     128       1.516565      0.866245   \n",
       "0                 [256]                    1024       1.528696      0.815497   \n",
       "0            [512, 512]                    1024       1.607565      0.973704   \n",
       "0              [64, 64]                     128       1.635544      0.899366   \n",
       "0            [512, 512]                     256       1.657334      0.836969   \n",
       "0  [256, 256, 256, 256]                     512       1.724635      0.907240   \n",
       "0       [256, 256, 256]                     256       1.831527      0.908020   \n",
       "0       [256, 256, 256]                     128       1.872141      0.887123   \n",
       "0       [512, 512, 512]                     128       1.959600      0.862128   \n",
       "0       [128, 128, 128]                     512       1.975284      0.963010   \n",
       "0       [512, 512, 512]                    1024       1.988439      0.946242   \n",
       "0            [128, 128]                    1024       2.123503      0.902056   \n",
       "0  [128, 128, 128, 128]                     256       2.184838      0.921111   \n",
       "0       [256, 256, 256]                     512       2.273733      0.949557   \n",
       "0          [64, 64, 64]                     512       2.417750      1.059892   \n",
       "0  [512, 512, 512, 512]                    1024       2.835737      0.954354   \n",
       "0  [512, 512, 512, 512]                     512       3.029941      0.958066   \n",
       "0  [256, 256, 256, 256]                     128       3.215892      1.106165   \n",
       "0  [512, 512, 512, 512]                     256       3.357701      0.968145   \n",
       "0          [64, 64, 64]                     256       3.756445      1.217098   \n",
       "0       [128, 128, 128]                    1024       4.011961      1.083603   \n",
       "0  [128, 128, 128, 128]                     512       4.346828      1.142020   \n",
       "0      [64, 64, 64, 64]                    1024       4.741706      1.296870   \n",
       "0      [64, 64, 64, 64]                     128       4.752409      1.065886   \n",
       "0          [64, 64, 64]                    1024       4.786823      1.386452   \n",
       "0  [128, 128, 128, 128]                    1024       4.839312      1.127221   \n",
       "0      [64, 64, 64, 64]                     512       4.943508      1.206407   \n",
       "0       [128, 128, 128]                     256       5.108416      1.221880   \n",
       "0  [256, 256, 256, 256]                    1024       6.065442      1.185787   \n",
       "0      [64, 64, 64, 64]                     256       6.754857      1.416730   \n",
       "0  [128, 128, 128, 128]                     128       7.329994      1.201138   \n",
       "0  [256, 256, 256, 256]                     256       7.585701      1.239384   \n",
       "0  [512, 512, 512, 512]                     128       8.467718      1.139377   \n",
       "\n",
       "                Combined Label  \n",
       "0                  [512] - 256  \n",
       "0             [256, 256] - 128  \n",
       "0             [128, 128] - 256  \n",
       "0                  [128] - 128  \n",
       "0                  [256] - 256  \n",
       "0                   [64] - 512  \n",
       "0                  [128] - 256  \n",
       "0              [64, 64] - 1024  \n",
       "0                  [128] - 512  \n",
       "0                 [512] - 1024  \n",
       "0                  [256] - 128  \n",
       "0                  [512] - 512  \n",
       "0                   [64] - 256  \n",
       "0                   [64] - 128  \n",
       "0             [128, 128] - 128  \n",
       "0       [256, 256, 256] - 1024  \n",
       "0                  [64] - 1024  \n",
       "0                 [128] - 1024  \n",
       "0               [64, 64] - 256  \n",
       "0            [256, 256] - 1024  \n",
       "0                  [256] - 512  \n",
       "0             [128, 128] - 512  \n",
       "0        [128, 128, 128] - 128  \n",
       "0             [512, 512] - 512  \n",
       "0        [512, 512, 512] - 512  \n",
       "0                  [512] - 128  \n",
       "0             [256, 256] - 512  \n",
       "0        [512, 512, 512] - 256  \n",
       "0             [256, 256] - 256  \n",
       "0           [64, 64, 64] - 128  \n",
       "0               [64, 64] - 512  \n",
       "0             [512, 512] - 128  \n",
       "0                 [256] - 1024  \n",
       "0            [512, 512] - 1024  \n",
       "0               [64, 64] - 128  \n",
       "0             [512, 512] - 256  \n",
       "0   [256, 256, 256, 256] - 512  \n",
       "0        [256, 256, 256] - 256  \n",
       "0        [256, 256, 256] - 128  \n",
       "0        [512, 512, 512] - 128  \n",
       "0        [128, 128, 128] - 512  \n",
       "0       [512, 512, 512] - 1024  \n",
       "0            [128, 128] - 1024  \n",
       "0   [128, 128, 128, 128] - 256  \n",
       "0        [256, 256, 256] - 512  \n",
       "0           [64, 64, 64] - 512  \n",
       "0  [512, 512, 512, 512] - 1024  \n",
       "0   [512, 512, 512, 512] - 512  \n",
       "0   [256, 256, 256, 256] - 128  \n",
       "0   [512, 512, 512, 512] - 256  \n",
       "0           [64, 64, 64] - 256  \n",
       "0       [128, 128, 128] - 1024  \n",
       "0   [128, 128, 128, 128] - 512  \n",
       "0      [64, 64, 64, 64] - 1024  \n",
       "0       [64, 64, 64, 64] - 128  \n",
       "0          [64, 64, 64] - 1024  \n",
       "0  [128, 128, 128, 128] - 1024  \n",
       "0       [64, 64, 64, 64] - 512  \n",
       "0        [128, 128, 128] - 256  \n",
       "0  [256, 256, 256, 256] - 1024  \n",
       "0       [64, 64, 64, 64] - 256  \n",
       "0   [128, 128, 128, 128] - 128  \n",
       "0   [256, 256, 256, 256] - 256  \n",
       "0   [512, 512, 512, 512] - 128  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullplot1.sort_values(by=['lossvalue_mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hidden_feats</th>\n",
       "      <th>predictor_hidden_feats</th>\n",
       "      <th>lossvalue_mse</th>\n",
       "      <th>lossvalue_l1</th>\n",
       "      <th>Combined Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512]</td>\n",
       "      <td>256</td>\n",
       "      <td>0.819683</td>\n",
       "      <td>0.694102</td>\n",
       "      <td>[512] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64]</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.987599</td>\n",
       "      <td>0.731088</td>\n",
       "      <td>[64, 64] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256]</td>\n",
       "      <td>128</td>\n",
       "      <td>1.009090</td>\n",
       "      <td>0.739197</td>\n",
       "      <td>[256] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256]</td>\n",
       "      <td>256</td>\n",
       "      <td>0.967699</td>\n",
       "      <td>0.740711</td>\n",
       "      <td>[256] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512]</td>\n",
       "      <td>1024</td>\n",
       "      <td>1.007337</td>\n",
       "      <td>0.744620</td>\n",
       "      <td>[512] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64]</td>\n",
       "      <td>512</td>\n",
       "      <td>0.971687</td>\n",
       "      <td>0.747013</td>\n",
       "      <td>[64] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512, 512, 512]</td>\n",
       "      <td>256</td>\n",
       "      <td>1.424533</td>\n",
       "      <td>0.751299</td>\n",
       "      <td>[512, 512, 512] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>128</td>\n",
       "      <td>0.888432</td>\n",
       "      <td>0.754004</td>\n",
       "      <td>[256, 256] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>256</td>\n",
       "      <td>0.940755</td>\n",
       "      <td>0.755797</td>\n",
       "      <td>[128, 128] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128]</td>\n",
       "      <td>128</td>\n",
       "      <td>0.942370</td>\n",
       "      <td>0.766334</td>\n",
       "      <td>[128] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512]</td>\n",
       "      <td>512</td>\n",
       "      <td>1.034080</td>\n",
       "      <td>0.772547</td>\n",
       "      <td>[512] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64]</td>\n",
       "      <td>256</td>\n",
       "      <td>1.072731</td>\n",
       "      <td>0.780608</td>\n",
       "      <td>[64] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128]</td>\n",
       "      <td>512</td>\n",
       "      <td>1.005900</td>\n",
       "      <td>0.782430</td>\n",
       "      <td>[128] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128]</td>\n",
       "      <td>256</td>\n",
       "      <td>0.973967</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>[128] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>512</td>\n",
       "      <td>1.294355</td>\n",
       "      <td>0.797340</td>\n",
       "      <td>[128, 128] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512, 512, 512]</td>\n",
       "      <td>512</td>\n",
       "      <td>1.391131</td>\n",
       "      <td>0.806656</td>\n",
       "      <td>[512, 512, 512] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64]</td>\n",
       "      <td>128</td>\n",
       "      <td>1.079869</td>\n",
       "      <td>0.810801</td>\n",
       "      <td>[64] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>128</td>\n",
       "      <td>1.110174</td>\n",
       "      <td>0.812645</td>\n",
       "      <td>[128, 128] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>256</td>\n",
       "      <td>1.460130</td>\n",
       "      <td>0.814393</td>\n",
       "      <td>[256, 256] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256]</td>\n",
       "      <td>1024</td>\n",
       "      <td>1.528696</td>\n",
       "      <td>0.815497</td>\n",
       "      <td>[256] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>512</td>\n",
       "      <td>1.422254</td>\n",
       "      <td>0.821105</td>\n",
       "      <td>[256, 256] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512]</td>\n",
       "      <td>128</td>\n",
       "      <td>1.397099</td>\n",
       "      <td>0.821173</td>\n",
       "      <td>[512] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128]</td>\n",
       "      <td>1024</td>\n",
       "      <td>1.188769</td>\n",
       "      <td>0.824239</td>\n",
       "      <td>[128] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256]</td>\n",
       "      <td>512</td>\n",
       "      <td>1.250001</td>\n",
       "      <td>0.826729</td>\n",
       "      <td>[256] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>1024</td>\n",
       "      <td>1.242949</td>\n",
       "      <td>0.826995</td>\n",
       "      <td>[256, 256] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256, 256, 256]</td>\n",
       "      <td>1024</td>\n",
       "      <td>1.142747</td>\n",
       "      <td>0.833134</td>\n",
       "      <td>[256, 256, 256] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512, 512]</td>\n",
       "      <td>512</td>\n",
       "      <td>1.334807</td>\n",
       "      <td>0.835898</td>\n",
       "      <td>[512, 512] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64]</td>\n",
       "      <td>1024</td>\n",
       "      <td>1.174040</td>\n",
       "      <td>0.836096</td>\n",
       "      <td>[64] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512, 512]</td>\n",
       "      <td>256</td>\n",
       "      <td>1.657334</td>\n",
       "      <td>0.836969</td>\n",
       "      <td>[512, 512] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512, 512, 512]</td>\n",
       "      <td>128</td>\n",
       "      <td>1.959600</td>\n",
       "      <td>0.862128</td>\n",
       "      <td>[512, 512, 512] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512, 512]</td>\n",
       "      <td>128</td>\n",
       "      <td>1.516565</td>\n",
       "      <td>0.866245</td>\n",
       "      <td>[512, 512] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128, 128, 128]</td>\n",
       "      <td>128</td>\n",
       "      <td>1.327627</td>\n",
       "      <td>0.868400</td>\n",
       "      <td>[128, 128, 128] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256, 256, 256]</td>\n",
       "      <td>128</td>\n",
       "      <td>1.872141</td>\n",
       "      <td>0.887123</td>\n",
       "      <td>[256, 256, 256] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64]</td>\n",
       "      <td>256</td>\n",
       "      <td>1.215984</td>\n",
       "      <td>0.891100</td>\n",
       "      <td>[64, 64] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64]</td>\n",
       "      <td>128</td>\n",
       "      <td>1.635544</td>\n",
       "      <td>0.899366</td>\n",
       "      <td>[64, 64] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>1024</td>\n",
       "      <td>2.123503</td>\n",
       "      <td>0.902056</td>\n",
       "      <td>[128, 128] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256, 256, 256, 256]</td>\n",
       "      <td>512</td>\n",
       "      <td>1.724635</td>\n",
       "      <td>0.907240</td>\n",
       "      <td>[256, 256, 256, 256] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256, 256, 256]</td>\n",
       "      <td>256</td>\n",
       "      <td>1.831527</td>\n",
       "      <td>0.908020</td>\n",
       "      <td>[256, 256, 256] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128, 128, 128, 128]</td>\n",
       "      <td>256</td>\n",
       "      <td>2.184838</td>\n",
       "      <td>0.921111</td>\n",
       "      <td>[128, 128, 128, 128] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512, 512, 512]</td>\n",
       "      <td>1024</td>\n",
       "      <td>1.988439</td>\n",
       "      <td>0.946242</td>\n",
       "      <td>[512, 512, 512] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256, 256, 256]</td>\n",
       "      <td>512</td>\n",
       "      <td>2.273733</td>\n",
       "      <td>0.949557</td>\n",
       "      <td>[256, 256, 256] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512, 512, 512, 512]</td>\n",
       "      <td>1024</td>\n",
       "      <td>2.835737</td>\n",
       "      <td>0.954354</td>\n",
       "      <td>[512, 512, 512, 512] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512, 512, 512, 512]</td>\n",
       "      <td>512</td>\n",
       "      <td>3.029941</td>\n",
       "      <td>0.958066</td>\n",
       "      <td>[512, 512, 512, 512] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64]</td>\n",
       "      <td>512</td>\n",
       "      <td>1.511313</td>\n",
       "      <td>0.961469</td>\n",
       "      <td>[64, 64] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128, 128, 128]</td>\n",
       "      <td>512</td>\n",
       "      <td>1.975284</td>\n",
       "      <td>0.963010</td>\n",
       "      <td>[128, 128, 128] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512, 512, 512, 512]</td>\n",
       "      <td>256</td>\n",
       "      <td>3.357701</td>\n",
       "      <td>0.968145</td>\n",
       "      <td>[512, 512, 512, 512] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512, 512]</td>\n",
       "      <td>1024</td>\n",
       "      <td>1.607565</td>\n",
       "      <td>0.973704</td>\n",
       "      <td>[512, 512] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64, 64]</td>\n",
       "      <td>128</td>\n",
       "      <td>1.460844</td>\n",
       "      <td>0.975181</td>\n",
       "      <td>[64, 64, 64] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64, 64]</td>\n",
       "      <td>512</td>\n",
       "      <td>2.417750</td>\n",
       "      <td>1.059892</td>\n",
       "      <td>[64, 64, 64] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64, 64, 64]</td>\n",
       "      <td>128</td>\n",
       "      <td>4.752409</td>\n",
       "      <td>1.065886</td>\n",
       "      <td>[64, 64, 64, 64] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128, 128, 128]</td>\n",
       "      <td>1024</td>\n",
       "      <td>4.011961</td>\n",
       "      <td>1.083603</td>\n",
       "      <td>[128, 128, 128] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256, 256, 256, 256]</td>\n",
       "      <td>128</td>\n",
       "      <td>3.215892</td>\n",
       "      <td>1.106165</td>\n",
       "      <td>[256, 256, 256, 256] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128, 128, 128, 128]</td>\n",
       "      <td>1024</td>\n",
       "      <td>4.839312</td>\n",
       "      <td>1.127221</td>\n",
       "      <td>[128, 128, 128, 128] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512, 512, 512, 512]</td>\n",
       "      <td>128</td>\n",
       "      <td>8.467718</td>\n",
       "      <td>1.139377</td>\n",
       "      <td>[512, 512, 512, 512] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128, 128, 128, 128]</td>\n",
       "      <td>512</td>\n",
       "      <td>4.346828</td>\n",
       "      <td>1.142020</td>\n",
       "      <td>[128, 128, 128, 128] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256, 256, 256, 256]</td>\n",
       "      <td>1024</td>\n",
       "      <td>6.065442</td>\n",
       "      <td>1.185787</td>\n",
       "      <td>[256, 256, 256, 256] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128, 128, 128, 128]</td>\n",
       "      <td>128</td>\n",
       "      <td>7.329994</td>\n",
       "      <td>1.201138</td>\n",
       "      <td>[128, 128, 128, 128] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64, 64, 64]</td>\n",
       "      <td>512</td>\n",
       "      <td>4.943508</td>\n",
       "      <td>1.206407</td>\n",
       "      <td>[64, 64, 64, 64] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64, 64]</td>\n",
       "      <td>256</td>\n",
       "      <td>3.756445</td>\n",
       "      <td>1.217098</td>\n",
       "      <td>[64, 64, 64] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128, 128, 128]</td>\n",
       "      <td>256</td>\n",
       "      <td>5.108416</td>\n",
       "      <td>1.221880</td>\n",
       "      <td>[128, 128, 128] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256, 256, 256, 256]</td>\n",
       "      <td>256</td>\n",
       "      <td>7.585701</td>\n",
       "      <td>1.239384</td>\n",
       "      <td>[256, 256, 256, 256] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64, 64, 64]</td>\n",
       "      <td>1024</td>\n",
       "      <td>4.741706</td>\n",
       "      <td>1.296870</td>\n",
       "      <td>[64, 64, 64, 64] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64, 64]</td>\n",
       "      <td>1024</td>\n",
       "      <td>4.786823</td>\n",
       "      <td>1.386452</td>\n",
       "      <td>[64, 64, 64] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64, 64, 64]</td>\n",
       "      <td>256</td>\n",
       "      <td>6.754857</td>\n",
       "      <td>1.416730</td>\n",
       "      <td>[64, 64, 64, 64] - 256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1  \\\n",
       "0                 [512]                     256       0.819683      0.694102   \n",
       "0              [64, 64]                    1024       0.987599      0.731088   \n",
       "0                 [256]                     128       1.009090      0.739197   \n",
       "0                 [256]                     256       0.967699      0.740711   \n",
       "0                 [512]                    1024       1.007337      0.744620   \n",
       "0                  [64]                     512       0.971687      0.747013   \n",
       "0       [512, 512, 512]                     256       1.424533      0.751299   \n",
       "0            [256, 256]                     128       0.888432      0.754004   \n",
       "0            [128, 128]                     256       0.940755      0.755797   \n",
       "0                 [128]                     128       0.942370      0.766334   \n",
       "0                 [512]                     512       1.034080      0.772547   \n",
       "0                  [64]                     256       1.072731      0.780608   \n",
       "0                 [128]                     512       1.005900      0.782430   \n",
       "0                 [128]                     256       0.973967      0.791788   \n",
       "0            [128, 128]                     512       1.294355      0.797340   \n",
       "0       [512, 512, 512]                     512       1.391131      0.806656   \n",
       "0                  [64]                     128       1.079869      0.810801   \n",
       "0            [128, 128]                     128       1.110174      0.812645   \n",
       "0            [256, 256]                     256       1.460130      0.814393   \n",
       "0                 [256]                    1024       1.528696      0.815497   \n",
       "0            [256, 256]                     512       1.422254      0.821105   \n",
       "0                 [512]                     128       1.397099      0.821173   \n",
       "0                 [128]                    1024       1.188769      0.824239   \n",
       "0                 [256]                     512       1.250001      0.826729   \n",
       "0            [256, 256]                    1024       1.242949      0.826995   \n",
       "0       [256, 256, 256]                    1024       1.142747      0.833134   \n",
       "0            [512, 512]                     512       1.334807      0.835898   \n",
       "0                  [64]                    1024       1.174040      0.836096   \n",
       "0            [512, 512]                     256       1.657334      0.836969   \n",
       "0       [512, 512, 512]                     128       1.959600      0.862128   \n",
       "0            [512, 512]                     128       1.516565      0.866245   \n",
       "0       [128, 128, 128]                     128       1.327627      0.868400   \n",
       "0       [256, 256, 256]                     128       1.872141      0.887123   \n",
       "0              [64, 64]                     256       1.215984      0.891100   \n",
       "0              [64, 64]                     128       1.635544      0.899366   \n",
       "0            [128, 128]                    1024       2.123503      0.902056   \n",
       "0  [256, 256, 256, 256]                     512       1.724635      0.907240   \n",
       "0       [256, 256, 256]                     256       1.831527      0.908020   \n",
       "0  [128, 128, 128, 128]                     256       2.184838      0.921111   \n",
       "0       [512, 512, 512]                    1024       1.988439      0.946242   \n",
       "0       [256, 256, 256]                     512       2.273733      0.949557   \n",
       "0  [512, 512, 512, 512]                    1024       2.835737      0.954354   \n",
       "0  [512, 512, 512, 512]                     512       3.029941      0.958066   \n",
       "0              [64, 64]                     512       1.511313      0.961469   \n",
       "0       [128, 128, 128]                     512       1.975284      0.963010   \n",
       "0  [512, 512, 512, 512]                     256       3.357701      0.968145   \n",
       "0            [512, 512]                    1024       1.607565      0.973704   \n",
       "0          [64, 64, 64]                     128       1.460844      0.975181   \n",
       "0          [64, 64, 64]                     512       2.417750      1.059892   \n",
       "0      [64, 64, 64, 64]                     128       4.752409      1.065886   \n",
       "0       [128, 128, 128]                    1024       4.011961      1.083603   \n",
       "0  [256, 256, 256, 256]                     128       3.215892      1.106165   \n",
       "0  [128, 128, 128, 128]                    1024       4.839312      1.127221   \n",
       "0  [512, 512, 512, 512]                     128       8.467718      1.139377   \n",
       "0  [128, 128, 128, 128]                     512       4.346828      1.142020   \n",
       "0  [256, 256, 256, 256]                    1024       6.065442      1.185787   \n",
       "0  [128, 128, 128, 128]                     128       7.329994      1.201138   \n",
       "0      [64, 64, 64, 64]                     512       4.943508      1.206407   \n",
       "0          [64, 64, 64]                     256       3.756445      1.217098   \n",
       "0       [128, 128, 128]                     256       5.108416      1.221880   \n",
       "0  [256, 256, 256, 256]                     256       7.585701      1.239384   \n",
       "0      [64, 64, 64, 64]                    1024       4.741706      1.296870   \n",
       "0          [64, 64, 64]                    1024       4.786823      1.386452   \n",
       "0      [64, 64, 64, 64]                     256       6.754857      1.416730   \n",
       "\n",
       "                Combined Label  \n",
       "0                  [512] - 256  \n",
       "0              [64, 64] - 1024  \n",
       "0                  [256] - 128  \n",
       "0                  [256] - 256  \n",
       "0                 [512] - 1024  \n",
       "0                   [64] - 512  \n",
       "0        [512, 512, 512] - 256  \n",
       "0             [256, 256] - 128  \n",
       "0             [128, 128] - 256  \n",
       "0                  [128] - 128  \n",
       "0                  [512] - 512  \n",
       "0                   [64] - 256  \n",
       "0                  [128] - 512  \n",
       "0                  [128] - 256  \n",
       "0             [128, 128] - 512  \n",
       "0        [512, 512, 512] - 512  \n",
       "0                   [64] - 128  \n",
       "0             [128, 128] - 128  \n",
       "0             [256, 256] - 256  \n",
       "0                 [256] - 1024  \n",
       "0             [256, 256] - 512  \n",
       "0                  [512] - 128  \n",
       "0                 [128] - 1024  \n",
       "0                  [256] - 512  \n",
       "0            [256, 256] - 1024  \n",
       "0       [256, 256, 256] - 1024  \n",
       "0             [512, 512] - 512  \n",
       "0                  [64] - 1024  \n",
       "0             [512, 512] - 256  \n",
       "0        [512, 512, 512] - 128  \n",
       "0             [512, 512] - 128  \n",
       "0        [128, 128, 128] - 128  \n",
       "0        [256, 256, 256] - 128  \n",
       "0               [64, 64] - 256  \n",
       "0               [64, 64] - 128  \n",
       "0            [128, 128] - 1024  \n",
       "0   [256, 256, 256, 256] - 512  \n",
       "0        [256, 256, 256] - 256  \n",
       "0   [128, 128, 128, 128] - 256  \n",
       "0       [512, 512, 512] - 1024  \n",
       "0        [256, 256, 256] - 512  \n",
       "0  [512, 512, 512, 512] - 1024  \n",
       "0   [512, 512, 512, 512] - 512  \n",
       "0               [64, 64] - 512  \n",
       "0        [128, 128, 128] - 512  \n",
       "0   [512, 512, 512, 512] - 256  \n",
       "0            [512, 512] - 1024  \n",
       "0           [64, 64, 64] - 128  \n",
       "0           [64, 64, 64] - 512  \n",
       "0       [64, 64, 64, 64] - 128  \n",
       "0       [128, 128, 128] - 1024  \n",
       "0   [256, 256, 256, 256] - 128  \n",
       "0  [128, 128, 128, 128] - 1024  \n",
       "0   [512, 512, 512, 512] - 128  \n",
       "0   [128, 128, 128, 128] - 512  \n",
       "0  [256, 256, 256, 256] - 1024  \n",
       "0   [128, 128, 128, 128] - 128  \n",
       "0       [64, 64, 64, 64] - 512  \n",
       "0           [64, 64, 64] - 256  \n",
       "0        [128, 128, 128] - 256  \n",
       "0   [256, 256, 256, 256] - 256  \n",
       "0      [64, 64, 64, 64] - 1024  \n",
       "0          [64, 64, 64] - 1024  \n",
       "0       [64, 64, 64, 64] - 256  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullplot1.sort_values(by=['lossvalue_l1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='Combined Label'>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fullplot1.set_index('Combined Label', inplace=True)\n",
    "\n",
    "fullplot1['lossvalue_mse'].plot(kind='bar')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newest_dgl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
