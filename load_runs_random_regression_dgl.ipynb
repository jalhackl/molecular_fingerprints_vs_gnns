{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def list_files(directory):\n",
    "    files = []\n",
    "    for entry in os.listdir(directory):\n",
    "        if os.path.isfile(os.path.join(directory, entry)):\n",
    "            files.append(entry)\n",
    "    return files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = \"results_dgl_regression_val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['model_gnn_conv', 'model_gnn_conv_1', 'model_gnn_conv_10', 'model_gnn_conv_11', 'model_gnn_conv_12', 'model_gnn_conv_13', 'model_gnn_conv_14', 'model_gnn_conv_15', 'model_gnn_conv_16', 'model_gnn_conv_17', 'model_gnn_conv_18', 'model_gnn_conv_19', 'model_gnn_conv_2', 'model_gnn_conv_20', 'model_gnn_conv_21', 'model_gnn_conv_22', 'model_gnn_conv_23', 'model_gnn_conv_24', 'model_gnn_conv_25', 'model_gnn_conv_26', 'model_gnn_conv_27', 'model_gnn_conv_28', 'model_gnn_conv_29', 'model_gnn_conv_3', 'model_gnn_conv_30', 'model_gnn_conv_31', 'model_gnn_conv_32', 'model_gnn_conv_33', 'model_gnn_conv_34', 'model_gnn_conv_35', 'model_gnn_conv_36', 'model_gnn_conv_37', 'model_gnn_conv_38', 'model_gnn_conv_39', 'model_gnn_conv_4', 'model_gnn_conv_40', 'model_gnn_conv_41', 'model_gnn_conv_42', 'model_gnn_conv_43', 'model_gnn_conv_44', 'model_gnn_conv_45', 'model_gnn_conv_46', 'model_gnn_conv_47', 'model_gnn_conv_48', 'model_gnn_conv_49', 'model_gnn_conv_5', 'model_gnn_conv_50', 'model_gnn_conv_51', 'model_gnn_conv_52', 'model_gnn_conv_53', 'model_gnn_conv_54', 'model_gnn_conv_55', 'model_gnn_conv_56', 'model_gnn_conv_57', 'model_gnn_conv_58', 'model_gnn_conv_59', 'model_gnn_conv_6', 'model_gnn_conv_60', 'model_gnn_conv_61', 'model_gnn_conv_62', 'model_gnn_conv_63', 'model_gnn_conv_7', 'model_gnn_conv_8', 'model_gnn_conv_9']\n"
     ]
    }
   ],
   "source": [
    "files = list_files(path1)\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1_load = []\n",
    "import dill as pickle\n",
    "\n",
    "for file in files:\n",
    "    file_path = os.path.join(path1, file)\n",
    "    with open(file_path, 'rb') as filel:\n",
    "        loaded_data = pickle.load(filel)\n",
    "    file1_load.append(loaded_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'pred_df':     y_real    y_pred\n",
       "  0   -5.915 -4.559448\n",
       "  1   -3.850 -3.701262\n",
       "  2   -7.850 -7.446312\n",
       "  3   -2.218 -3.183792\n",
       "  4   -0.180 -0.286621\n",
       "  ..     ...       ...\n",
       "  43  -2.730 -3.284103\n",
       "  44  -2.550 -2.500223\n",
       "  45  -1.390 -2.545388\n",
       "  46  -3.928 -3.512547\n",
       "  47  -5.115 -6.244727\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 33.08916878700256,\n",
       "  'mean_mse': 0.4911479949951172,\n",
       "  'mean_l1': 0.5329278409481049,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [64],\n",
       "  'predictor_hidden_feats': 128,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=64, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=64, bias=True)\n",
       "          (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -2.460 -1.920683\n",
       "  1   -2.218 -3.033240\n",
       "  2   -4.799 -4.305702\n",
       "  3   -4.173 -5.793506\n",
       "  4   -2.280 -2.256960\n",
       "  ..     ...       ...\n",
       "  43  -1.947 -0.971487\n",
       "  44  -7.420 -8.295823\n",
       "  45  -6.291 -6.391837\n",
       "  46  -2.380 -2.126666\n",
       "  47  -2.770 -2.361686\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 27.03418278694153,\n",
       "  'mean_mse': 0.6036356091499329,\n",
       "  'mean_l1': 0.5715228617191315,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [64],\n",
       "  'predictor_hidden_feats': 256,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=64, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=64, bias=True)\n",
       "          (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=128, out_features=256, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0    -2.28 -4.779880\n",
       "  1    -7.85 -7.877604\n",
       "  2    -2.42 -2.660986\n",
       "  3     0.61 -0.735967\n",
       "  4    -0.72 -0.117709\n",
       "  ..     ...       ...\n",
       "  43   -3.51 -3.717095\n",
       "  44   -4.92 -4.096706\n",
       "  45   -5.40 -4.743131\n",
       "  46   -3.14 -2.688085\n",
       "  47   -1.52 -1.103873\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 82.11869525909424,\n",
       "  'mean_mse': 0.5424412488937378,\n",
       "  'mean_l1': 0.5375226736068726,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [64, 64, 64],\n",
       "  'predictor_hidden_feats': 512,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=64, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=64, bias=True)\n",
       "          (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-2): 2 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=64, out=64, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -5.400 -3.513179\n",
       "  1   -2.080 -1.922785\n",
       "  2    0.610 -0.782923\n",
       "  3   -3.927 -2.930907\n",
       "  4   -2.370 -1.574453\n",
       "  ..     ...       ...\n",
       "  43  -4.470 -3.870495\n",
       "  44   0.580 -0.489396\n",
       "  45  -3.510 -3.448997\n",
       "  46  -1.110 -1.406769\n",
       "  47  -1.390 -1.224116\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 35.62978672981262,\n",
       "  'mean_mse': 0.9802747666835785,\n",
       "  'mean_l1': 0.7511229813098907,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [64, 64, 64],\n",
       "  'predictor_hidden_feats': 1024,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=64, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=64, bias=True)\n",
       "          (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-2): 2 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=64, out=64, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=128, out_features=1024, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=1024, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -1.228 -1.487447\n",
       "  1   -1.458 -0.541361\n",
       "  2   -5.915 -3.458658\n",
       "  3   -3.300 -3.021861\n",
       "  4   -2.460 -1.682145\n",
       "  ..     ...       ...\n",
       "  43  -1.640 -1.240591\n",
       "  44  -6.250 -5.202848\n",
       "  45  -2.820 -4.650784\n",
       "  46  -4.630 -4.279152\n",
       "  47  -0.360 -0.719051\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 47.51831555366516,\n",
       "  'mean_mse': 0.8466293215751648,\n",
       "  'mean_l1': 0.7442261576652527,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [64, 64, 64, 64],\n",
       "  'predictor_hidden_feats': 128,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=64, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=64, bias=True)\n",
       "          (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-3): 3 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=64, out=64, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -7.420 -7.568581\n",
       "  1   -4.799 -5.253725\n",
       "  2   -3.928 -3.748386\n",
       "  3   -0.720 -0.696899\n",
       "  4   -4.470 -3.921747\n",
       "  ..     ...       ...\n",
       "  43  -3.927 -2.963404\n",
       "  44  -0.030 -1.173676\n",
       "  45  -3.850 -2.839435\n",
       "  46  -3.010 -3.486372\n",
       "  47   0.522 -0.907113\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 42.3408145904541,\n",
       "  'mean_mse': 0.8377315104007721,\n",
       "  'mean_l1': 0.7468691468238831,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [64, 64, 64, 64],\n",
       "  'predictor_hidden_feats': 256,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=64, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=64, bias=True)\n",
       "          (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-3): 3 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=64, out=64, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=128, out_features=256, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -0.440 -0.901585\n",
       "  1   -6.780 -8.345374\n",
       "  2   -2.000 -1.742604\n",
       "  3   -4.345 -4.422322\n",
       "  4    0.620 -0.423201\n",
       "  ..     ...       ...\n",
       "  43  -1.300 -0.917520\n",
       "  44  -1.820 -1.363785\n",
       "  45  -6.291 -6.093051\n",
       "  46   0.009 -1.509184\n",
       "  47  -0.360 -0.275850\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 75.22035121917725,\n",
       "  'mean_mse': 0.7641059756278992,\n",
       "  'mean_l1': 0.6790552139282227,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [64, 64, 64, 64],\n",
       "  'predictor_hidden_feats': 512,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=64, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=64, bias=True)\n",
       "          (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-3): 3 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=64, out=64, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0    0.620 -0.398299\n",
       "  1   -9.160 -9.869523\n",
       "  2   -1.989 -4.350567\n",
       "  3   -3.680 -1.983739\n",
       "  4    0.610 -0.886537\n",
       "  ..     ...       ...\n",
       "  43  -1.520 -0.741321\n",
       "  44  -1.520 -1.037207\n",
       "  45  -5.240 -5.461331\n",
       "  46  -2.370 -1.478123\n",
       "  47  -2.218 -3.178489\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 58.58035206794739,\n",
       "  'mean_mse': 0.7074449360370636,\n",
       "  'mean_l1': 0.6342839896678925,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [64, 64, 64, 64],\n",
       "  'predictor_hidden_feats': 1024,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=64, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=64, bias=True)\n",
       "          (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-3): 3 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=64, out=64, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=128, out_features=1024, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=1024, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -1.228 -1.417299\n",
       "  1    0.540 -0.133141\n",
       "  2   -1.300 -1.219182\n",
       "  3   -3.140 -3.182689\n",
       "  4   -1.614 -2.117244\n",
       "  ..     ...       ...\n",
       "  43  -1.990 -2.727630\n",
       "  44  -2.210 -2.414367\n",
       "  45  -0.807 -0.700976\n",
       "  46  -7.420 -8.442083\n",
       "  47  -4.310 -4.373218\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 21.296897888183594,\n",
       "  'mean_mse': 0.5945679247379303,\n",
       "  'mean_l1': 0.5492647290229797,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [128],\n",
       "  'predictor_hidden_feats': 128,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=128, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=128, bias=True)\n",
       "          (bn_layer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -3.300 -3.315135\n",
       "  1   -2.280 -2.553098\n",
       "  2   -0.807 -0.548951\n",
       "  3   -4.280 -4.155097\n",
       "  4   -1.990 -2.397751\n",
       "  ..     ...       ...\n",
       "  43  -1.520 -1.133201\n",
       "  44  -9.160 -8.862709\n",
       "  45  -0.600 -0.970070\n",
       "  46  -3.796 -3.664131\n",
       "  47  -7.420 -8.050907\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 36.948543310165405,\n",
       "  'mean_mse': 0.5027127712965012,\n",
       "  'mean_l1': 0.519672617316246,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [128],\n",
       "  'predictor_hidden_feats': 256,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=128, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=128, bias=True)\n",
       "          (bn_layer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -3.796 -4.166352\n",
       "  1   -1.640 -1.930344\n",
       "  2   -3.928 -3.550265\n",
       "  3   -2.840 -5.055584\n",
       "  4    0.260  0.823276\n",
       "  ..     ...       ...\n",
       "  43  -2.210 -0.850807\n",
       "  44  -3.630 -3.737589\n",
       "  45  -1.870 -1.729888\n",
       "  46   0.580  0.219452\n",
       "  47  -2.420 -2.243718\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 33.066514015197754,\n",
       "  'mean_mse': 0.4444136321544647,\n",
       "  'mean_l1': 0.48732972145080566,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [128],\n",
       "  'predictor_hidden_feats': 512,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=128, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=128, bias=True)\n",
       "          (bn_layer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -2.840 -4.931706\n",
       "  1   -0.040 -0.161579\n",
       "  2   -4.470 -3.865511\n",
       "  3   -2.770 -2.777969\n",
       "  4    0.540 -0.420513\n",
       "  ..     ...       ...\n",
       "  43  -3.010 -2.695385\n",
       "  44  -3.927 -3.364128\n",
       "  45  -1.110 -1.321150\n",
       "  46  -3.100 -3.077043\n",
       "  47  -3.460 -4.782909\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 23.330917358398438,\n",
       "  'mean_mse': 0.49545054137706757,\n",
       "  'mean_l1': 0.5258369743824005,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [128],\n",
       "  'predictor_hidden_feats': 1024,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=128, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=128, bias=True)\n",
       "          (bn_layer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=1024, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -4.920 -4.187924\n",
       "  1   -2.820 -3.768059\n",
       "  2   -4.173 -6.219845\n",
       "  3   -3.927 -3.489550\n",
       "  4   -3.220 -3.288044\n",
       "  ..     ...       ...\n",
       "  43  -2.460 -1.671619\n",
       "  44  -5.840 -4.806260\n",
       "  45  -2.210 -1.404716\n",
       "  46  -2.080 -2.531121\n",
       "  47  -1.520 -1.823789\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 20.559200286865234,\n",
       "  'mean_mse': 0.6892943680286407,\n",
       "  'mean_l1': 0.6322596967220306,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [64],\n",
       "  'predictor_hidden_feats': 512,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=64, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=64, bias=True)\n",
       "          (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -0.807 -0.649924\n",
       "  1   -5.507 -5.298653\n",
       "  2   -3.680 -3.304997\n",
       "  3   -0.030 -0.480041\n",
       "  4   -2.420 -3.004842\n",
       "  ..     ...       ...\n",
       "  43  -1.730 -1.870828\n",
       "  44  -1.060 -1.183661\n",
       "  45  -1.228 -1.544861\n",
       "  46  -4.310 -4.032377\n",
       "  47  -2.000 -1.952080\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 77.62437915802002,\n",
       "  'mean_mse': 0.4468193054199219,\n",
       "  'mean_l1': 0.48336321115493774,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [128, 128],\n",
       "  'predictor_hidden_feats': 128,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=128, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=128, bias=True)\n",
       "          (bn_layer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=128, out=128, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (bn_layer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -7.200 -5.904960\n",
       "  1   -2.770 -2.775911\n",
       "  2   -2.210 -1.077490\n",
       "  3   -2.370 -2.111543\n",
       "  4   -4.370 -4.048647\n",
       "  ..     ...       ...\n",
       "  43  -4.470 -3.941896\n",
       "  44  -1.870 -1.831071\n",
       "  45  -1.370 -1.382389\n",
       "  46  -3.796 -4.091167\n",
       "  47  -5.115 -6.768526\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 60.989418029785156,\n",
       "  'mean_mse': 0.44742584228515625,\n",
       "  'mean_l1': 0.4899100512266159,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [128, 128],\n",
       "  'predictor_hidden_feats': 256,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=128, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=128, bias=True)\n",
       "          (bn_layer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=128, out=128, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (bn_layer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -1.458 -1.293598\n",
       "  1   -1.228 -1.267154\n",
       "  2    0.009 -0.598771\n",
       "  3   -2.210 -2.536745\n",
       "  4   -1.220 -1.358390\n",
       "  ..     ...       ...\n",
       "  43  -1.640 -2.141324\n",
       "  44  -0.600 -1.668530\n",
       "  45  -3.850 -3.514014\n",
       "  46   0.610 -0.430619\n",
       "  47  -0.220 -0.479996\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 39.5663378238678,\n",
       "  'mean_mse': 0.4271136373281479,\n",
       "  'mean_l1': 0.5015258193016052,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [128, 128],\n",
       "  'predictor_hidden_feats': 512,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=128, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=128, bias=True)\n",
       "          (bn_layer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=128, out=128, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (bn_layer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -9.160 -9.493030\n",
       "  1   -1.820 -0.806551\n",
       "  2   -2.070 -1.825088\n",
       "  3   -3.510 -3.569772\n",
       "  4   -7.200 -6.419431\n",
       "  ..     ...       ...\n",
       "  43  -3.300 -3.098197\n",
       "  44  -2.850 -3.234814\n",
       "  45  -3.140 -2.716487\n",
       "  46  -3.460 -4.198349\n",
       "  47  -3.083 -2.567499\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 74.12894773483276,\n",
       "  'mean_mse': 0.432888388633728,\n",
       "  'mean_l1': 0.4829365909099579,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [128, 128],\n",
       "  'predictor_hidden_feats': 1024,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=128, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=128, bias=True)\n",
       "          (bn_layer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=128, out=128, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (bn_layer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=1024, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -6.780 -6.601304\n",
       "  1   -1.947 -0.961954\n",
       "  2    0.580 -0.091296\n",
       "  3   -2.000 -1.913763\n",
       "  4   -1.990 -2.197965\n",
       "  ..     ...       ...\n",
       "  43  -3.927 -3.156810\n",
       "  44   0.620  0.071831\n",
       "  45   0.260 -0.230883\n",
       "  46  -3.460 -4.647739\n",
       "  47  -1.110 -0.953297\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 57.63464426994324,\n",
       "  'mean_mse': 0.4489916265010834,\n",
       "  'mean_l1': 0.5389180779457092,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [128, 128, 128],\n",
       "  'predictor_hidden_feats': 128,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=128, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=128, bias=True)\n",
       "          (bn_layer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-2): 2 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=128, out=128, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (bn_layer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -8.400 -6.991892\n",
       "  1   -1.360 -1.034411\n",
       "  2   -1.228 -1.738009\n",
       "  3   -3.927 -2.852717\n",
       "  4   -3.850 -3.985654\n",
       "  ..     ...       ...\n",
       "  43  -5.400 -4.101635\n",
       "  44  -3.630 -3.375173\n",
       "  45  -5.915 -5.678890\n",
       "  46  -7.850 -7.513658\n",
       "  47  -2.550 -2.111416\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 100.58767986297607,\n",
       "  'mean_mse': 0.4507651925086975,\n",
       "  'mean_l1': 0.5028000473976135,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [128, 128, 128],\n",
       "  'predictor_hidden_feats': 256,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=128, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=128, bias=True)\n",
       "          (bn_layer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-2): 2 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=128, out=128, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (bn_layer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -4.380 -4.427256\n",
       "  1   -1.520 -2.298798\n",
       "  2   -0.220 -0.389120\n",
       "  3   -3.100 -2.811436\n",
       "  4   -2.420 -2.731253\n",
       "  ..     ...       ...\n",
       "  43  -3.630 -3.386207\n",
       "  44  -0.720 -0.382954\n",
       "  45  -1.360 -1.223831\n",
       "  46  -6.780 -5.064166\n",
       "  47  -5.915 -6.643846\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 100.40558123588562,\n",
       "  'mean_mse': 0.5904108583927155,\n",
       "  'mean_l1': 0.541715145111084,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [128, 128, 128],\n",
       "  'predictor_hidden_feats': 512,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=128, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=128, bias=True)\n",
       "          (bn_layer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-2): 2 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=128, out=128, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (bn_layer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -2.000 -2.117391\n",
       "  1   -3.140 -2.833317\n",
       "  2   -4.799 -3.284095\n",
       "  3   -1.730 -1.608367\n",
       "  4   -2.730 -3.812206\n",
       "  ..     ...       ...\n",
       "  43  -2.560 -1.716631\n",
       "  44  -3.083 -2.848899\n",
       "  45  -2.070 -1.402268\n",
       "  46   1.100 -0.533274\n",
       "  47  -2.676 -2.829926\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 24.017510890960693,\n",
       "  'mean_mse': 0.8731695115566254,\n",
       "  'mean_l1': 0.7635863125324249,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [128, 128, 128],\n",
       "  'predictor_hidden_feats': 1024,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=128, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=128, bias=True)\n",
       "          (bn_layer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-2): 2 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=128, out=128, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (bn_layer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=1024, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -2.210 -2.568089\n",
       "  1   -4.345 -3.904092\n",
       "  2   -2.730 -3.993223\n",
       "  3   -3.927 -3.230818\n",
       "  4   -4.380 -4.610976\n",
       "  ..     ...       ...\n",
       "  43   0.610 -0.287853\n",
       "  44  -1.730 -1.689623\n",
       "  45  -1.228 -1.391845\n",
       "  46  -2.676 -2.592931\n",
       "  47  -1.990 -2.377621\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 101.19983792304993,\n",
       "  'mean_mse': 0.5446774661540985,\n",
       "  'mean_l1': 0.54473477602005,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [128, 128, 128, 128],\n",
       "  'predictor_hidden_feats': 128,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=128, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=128, bias=True)\n",
       "          (bn_layer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-3): 3 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=128, out=128, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (bn_layer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0    0.522  0.384447\n",
       "  1   -3.460 -4.984162\n",
       "  2   -1.340 -1.239172\n",
       "  3   -5.507 -5.585218\n",
       "  4   -1.110 -0.641754\n",
       "  ..     ...       ...\n",
       "  43  -0.600 -0.747275\n",
       "  44  -2.770 -2.472120\n",
       "  45   0.580  0.057410\n",
       "  46  -3.010 -3.313749\n",
       "  47  -6.250 -5.808660\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 205.37302827835083,\n",
       "  'mean_mse': 0.5363730192184448,\n",
       "  'mean_l1': 0.5060844421386719,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [128, 128, 128, 128],\n",
       "  'predictor_hidden_feats': 256,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=128, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=128, bias=True)\n",
       "          (bn_layer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-3): 3 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=128, out=128, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (bn_layer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -2.420 -2.407246\n",
       "  1   -5.507 -5.055968\n",
       "  2   -5.233 -5.854072\n",
       "  3   -1.520 -1.328892\n",
       "  4   -1.228 -1.229094\n",
       "  ..     ...       ...\n",
       "  43  -0.040 -0.099140\n",
       "  44  -1.340 -1.286946\n",
       "  45  -3.796 -3.799334\n",
       "  46   0.620  0.170027\n",
       "  47   0.510 -0.351289\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 38.256020069122314,\n",
       "  'mean_mse': 0.4945468008518219,\n",
       "  'mean_l1': 0.5154705941677094,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [64],\n",
       "  'predictor_hidden_feats': 1024,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=64, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=64, bias=True)\n",
       "          (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=128, out_features=1024, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=1024, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -1.730 -1.626559\n",
       "  1   -4.370 -4.198953\n",
       "  2    0.540  0.123076\n",
       "  3   -1.110 -0.918193\n",
       "  4   -1.458 -1.440175\n",
       "  ..     ...       ...\n",
       "  43  -5.915 -5.322179\n",
       "  44  -1.520 -1.882843\n",
       "  45  -0.220 -0.242880\n",
       "  46  -0.040 -0.232194\n",
       "  47   0.009 -0.945080\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 97.17761063575745,\n",
       "  'mean_mse': 0.5843817293643951,\n",
       "  'mean_l1': 0.5893613696098328,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [128, 128, 128, 128],\n",
       "  'predictor_hidden_feats': 512,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=128, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=128, bias=True)\n",
       "          (bn_layer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-3): 3 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=128, out=128, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (bn_layer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -3.450 -5.019199\n",
       "  1   -3.400 -2.957676\n",
       "  2   -5.240 -4.910619\n",
       "  3   -1.458 -0.847196\n",
       "  4   -2.210 -1.271028\n",
       "  ..     ...       ...\n",
       "  43   0.009 -0.452991\n",
       "  44  -2.676 -2.258249\n",
       "  45  -4.920 -4.132463\n",
       "  46  -0.440 -0.504203\n",
       "  47  -4.280 -3.660615\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 80.05066418647766,\n",
       "  'mean_mse': 0.7554923892021179,\n",
       "  'mean_l1': 0.6818098425865173,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [128, 128, 128, 128],\n",
       "  'predictor_hidden_feats': 1024,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=128, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=128, bias=True)\n",
       "          (bn_layer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-3): 3 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=128, out=128, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (bn_layer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=1024, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -6.780 -6.578453\n",
       "  1   -3.220 -3.269836\n",
       "  2   -1.458 -1.371541\n",
       "  3   -1.640 -2.061657\n",
       "  4   -1.730 -2.106361\n",
       "  ..     ...       ...\n",
       "  43   0.522 -0.576013\n",
       "  44  -3.300 -3.413841\n",
       "  45  -1.614 -1.946003\n",
       "  46  -3.460 -4.747932\n",
       "  47  -2.560 -2.566132\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 39.716142654418945,\n",
       "  'mean_mse': 0.6043450683355331,\n",
       "  'mean_l1': 0.5388340353965759,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [256],\n",
       "  'predictor_hidden_feats': 128,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=256, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=256, bias=True)\n",
       "          (bn_layer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -1.870 -1.449348\n",
       "  1   -6.560 -5.475453\n",
       "  2   -1.947 -1.094826\n",
       "  3   -3.928 -3.763534\n",
       "  4   -4.110 -3.890446\n",
       "  ..     ...       ...\n",
       "  43  -3.083 -2.466246\n",
       "  44  -1.300 -1.474250\n",
       "  45   0.790 -0.392325\n",
       "  46  -7.850 -8.061951\n",
       "  47  -1.614 -2.031489\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 26.013391733169556,\n",
       "  'mean_mse': 0.5724552720785141,\n",
       "  'mean_l1': 0.5802259147167206,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [256],\n",
       "  'predictor_hidden_feats': 256,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=256, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=256, bias=True)\n",
       "          (bn_layer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -3.083 -2.559444\n",
       "  1   -0.600 -1.577800\n",
       "  2   -4.110 -2.963269\n",
       "  3   -0.400 -0.483752\n",
       "  4   -4.280 -4.605061\n",
       "  ..     ...       ...\n",
       "  43  -7.420 -8.062977\n",
       "  44  -3.400 -3.200234\n",
       "  45   1.100  0.986296\n",
       "  46  -3.010 -3.118894\n",
       "  47  -2.850 -2.721924\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 54.89883804321289,\n",
       "  'mean_mse': 0.47301070392131805,\n",
       "  'mean_l1': 0.4854460060596466,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [256],\n",
       "  'predictor_hidden_feats': 512,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=256, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=256, bias=True)\n",
       "          (bn_layer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -1.458 -1.109419\n",
       "  1   -3.850 -4.144278\n",
       "  2   -3.083 -2.645339\n",
       "  3   -2.460 -1.693658\n",
       "  4    1.100  1.123857\n",
       "  ..     ...       ...\n",
       "  43  -2.070 -1.567056\n",
       "  44  -4.920 -4.303786\n",
       "  45  -2.820 -3.173413\n",
       "  46  -5.030 -4.219281\n",
       "  47  -5.915 -5.707914\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 37.11041069030762,\n",
       "  'mean_mse': 0.603954017162323,\n",
       "  'mean_l1': 0.537689745426178,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [256],\n",
       "  'predictor_hidden_feats': 1024,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=256, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=256, bias=True)\n",
       "          (bn_layer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=1024, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -1.520 -2.200805\n",
       "  1   -3.083 -2.844298\n",
       "  2   -1.060 -1.637456\n",
       "  3   -4.380 -3.956644\n",
       "  4    0.610 -0.783844\n",
       "  ..     ...       ...\n",
       "  43  -3.927 -2.886211\n",
       "  44  -2.630 -3.002216\n",
       "  45  -2.080 -2.373748\n",
       "  46   0.620  0.259071\n",
       "  47  -0.180 -0.324454\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 61.424834966659546,\n",
       "  'mean_mse': 0.5517675876617432,\n",
       "  'mean_l1': 0.5165728628635406,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [256, 256],\n",
       "  'predictor_hidden_feats': 128,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=256, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=256, bias=True)\n",
       "          (bn_layer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=256, out=256, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (bn_layer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -1.520 -1.158344\n",
       "  1   -5.840 -5.798542\n",
       "  2   -4.173 -5.271594\n",
       "  3    0.620  0.257560\n",
       "  4   -6.780 -5.992531\n",
       "  ..     ...       ...\n",
       "  43  -1.370 -1.441774\n",
       "  44  -8.400 -7.419004\n",
       "  45  -3.680 -3.466519\n",
       "  46   0.430  0.409076\n",
       "  47  -3.927 -3.312910\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 61.33456015586853,\n",
       "  'mean_mse': 0.40847210586071014,\n",
       "  'mean_l1': 0.4504161924123764,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [256, 256],\n",
       "  'predictor_hidden_feats': 256,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=256, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=256, bias=True)\n",
       "          (bn_layer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=256, out=256, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (bn_layer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0    0.620 -0.382620\n",
       "  1   -1.360 -1.503747\n",
       "  2   -2.730 -3.861401\n",
       "  3    0.540 -0.469913\n",
       "  4   -0.807 -0.559463\n",
       "  ..     ...       ...\n",
       "  43  -2.550 -2.359991\n",
       "  44  -1.870 -1.933605\n",
       "  45  -7.420 -7.840533\n",
       "  46   0.580 -0.139334\n",
       "  47  -0.220 -0.825537\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 60.63623070716858,\n",
       "  'mean_mse': 0.443557932972908,\n",
       "  'mean_l1': 0.4979982376098633,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [256, 256],\n",
       "  'predictor_hidden_feats': 512,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=256, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=256, bias=True)\n",
       "          (bn_layer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=256, out=256, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (bn_layer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -0.440 -0.798341\n",
       "  1   -2.630 -2.460328\n",
       "  2   -2.840 -4.757239\n",
       "  3   -2.820 -3.066102\n",
       "  4   -2.210 -2.703663\n",
       "  ..     ...       ...\n",
       "  43  -7.420 -8.112725\n",
       "  44  -1.989 -3.855048\n",
       "  45  -2.550 -2.396138\n",
       "  46  -3.928 -4.126210\n",
       "  47   0.430  0.178454\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 57.71332669258118,\n",
       "  'mean_mse': 0.48101015388965607,\n",
       "  'mean_l1': 0.5049898326396942,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [256, 256],\n",
       "  'predictor_hidden_feats': 1024,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=256, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=256, bias=True)\n",
       "          (bn_layer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=256, out=256, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (bn_layer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=1024, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -1.390 -1.930328\n",
       "  1   -5.840 -5.866981\n",
       "  2   -1.300 -1.356912\n",
       "  3   -0.360 -0.349151\n",
       "  4   -2.370 -2.050210\n",
       "  ..     ...       ...\n",
       "  43  -4.345 -4.583311\n",
       "  44  -0.720  0.120392\n",
       "  45  -0.807 -0.349966\n",
       "  46  -3.450 -4.006646\n",
       "  47   0.790 -0.710762\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 64.27463507652283,\n",
       "  'mean_mse': 0.5316790044307709,\n",
       "  'mean_l1': 0.5109429955482483,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [64, 64],\n",
       "  'predictor_hidden_feats': 128,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=64, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=64, bias=True)\n",
       "          (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=64, out=64, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -4.799 -4.151670\n",
       "  1   -1.458 -0.915790\n",
       "  2   -5.240 -4.115936\n",
       "  3   -5.233 -5.453090\n",
       "  4   -2.730 -3.709167\n",
       "  ..     ...       ...\n",
       "  43  -4.173 -7.375923\n",
       "  44  -0.390 -0.411945\n",
       "  45  -5.915 -4.758587\n",
       "  46  -8.400 -7.603702\n",
       "  47  -2.000 -1.643024\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 59.27565860748291,\n",
       "  'mean_mse': 0.785103052854538,\n",
       "  'mean_l1': 0.6323600113391876,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [256, 256, 256],\n",
       "  'predictor_hidden_feats': 128,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=256, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=256, bias=True)\n",
       "          (bn_layer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-2): 2 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=256, out=256, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (bn_layer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -3.010 -2.797759\n",
       "  1   -4.310 -4.436487\n",
       "  2   -0.400 -0.666801\n",
       "  3   -2.000 -2.256442\n",
       "  4   -3.928 -4.527981\n",
       "  ..     ...       ...\n",
       "  43  -2.370 -1.997843\n",
       "  44  -2.560 -2.266892\n",
       "  45  -3.300 -3.087285\n",
       "  46   0.522 -0.376585\n",
       "  47  -1.300 -1.123175\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 117.43732523918152,\n",
       "  'mean_mse': 0.43789854645729065,\n",
       "  'mean_l1': 0.4979993999004364,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [256, 256, 256],\n",
       "  'predictor_hidden_feats': 256,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=256, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=256, bias=True)\n",
       "          (bn_layer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-2): 2 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=256, out=256, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (bn_layer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -1.060 -1.282186\n",
       "  1   -2.080 -2.646562\n",
       "  2    0.620  0.092158\n",
       "  3   -2.210 -1.166609\n",
       "  4   -3.140 -3.309480\n",
       "  ..     ...       ...\n",
       "  43  -5.840 -6.204146\n",
       "  44  -3.850 -3.603664\n",
       "  45  -3.780 -4.025443\n",
       "  46  -1.110 -1.116598\n",
       "  47  -2.218 -3.365373\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 76.33806729316711,\n",
       "  'mean_mse': 0.5608175247907639,\n",
       "  'mean_l1': 0.5321245491504669,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [256, 256, 256],\n",
       "  'predictor_hidden_feats': 512,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=256, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=256, bias=True)\n",
       "          (bn_layer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-2): 2 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=256, out=256, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (bn_layer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -2.070 -1.880867\n",
       "  1   -4.110 -3.634671\n",
       "  2   -6.780 -7.116181\n",
       "  3   -2.000 -2.020548\n",
       "  4   -5.400 -5.273995\n",
       "  ..     ...       ...\n",
       "  43  -9.160 -8.763844\n",
       "  44  -2.380 -1.967260\n",
       "  45  -3.630 -3.468777\n",
       "  46  -4.799 -4.223830\n",
       "  47  -6.291 -5.924565\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 114.20378279685974,\n",
       "  'mean_mse': 0.4401886910200119,\n",
       "  'mean_l1': 0.47796793282032013,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [256, 256, 256],\n",
       "  'predictor_hidden_feats': 1024,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=256, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=256, bias=True)\n",
       "          (bn_layer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-2): 2 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=256, out=256, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (bn_layer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=1024, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -1.228 -1.724545\n",
       "  1   -9.160 -8.324146\n",
       "  2   -2.420 -2.271815\n",
       "  3   -4.799 -4.375565\n",
       "  4   -1.870 -1.703527\n",
       "  ..     ...       ...\n",
       "  43   0.510 -0.504581\n",
       "  44  -2.840 -4.677625\n",
       "  45  -0.400 -0.225900\n",
       "  46  -7.200 -6.437867\n",
       "  47  -4.345 -3.569880\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 83.84959530830383,\n",
       "  'mean_mse': 0.5649882405996323,\n",
       "  'mean_l1': 0.5510416924953461,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [256, 256, 256, 256],\n",
       "  'predictor_hidden_feats': 128,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=256, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=256, bias=True)\n",
       "          (bn_layer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-3): 3 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=256, out=256, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (bn_layer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -4.630 -4.674022\n",
       "  1   -2.820 -2.981560\n",
       "  2   -7.420 -7.325960\n",
       "  3   -1.989 -4.021908\n",
       "  4   -6.560 -5.336815\n",
       "  ..     ...       ...\n",
       "  43  -1.340 -1.143739\n",
       "  44   0.620  0.013971\n",
       "  45  -5.400 -3.789118\n",
       "  46  -0.807 -0.268610\n",
       "  47  -2.460 -1.833688\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 139.15758109092712,\n",
       "  'mean_mse': 0.5727324932813644,\n",
       "  'mean_l1': 0.5106356143951416,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [256, 256, 256, 256],\n",
       "  'predictor_hidden_feats': 256,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=256, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=256, bias=True)\n",
       "          (bn_layer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-3): 3 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=256, out=256, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (bn_layer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -3.083 -2.503661\n",
       "  1   -6.560 -5.243165\n",
       "  2   -6.291 -5.825855\n",
       "  3    0.610 -0.848535\n",
       "  4   -0.390 -0.960526\n",
       "  ..     ...       ...\n",
       "  43  -1.520 -1.427657\n",
       "  44  -2.380 -2.270755\n",
       "  45  -1.300 -1.256539\n",
       "  46  -4.470 -4.306814\n",
       "  47  -1.390 -1.895638\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 98.67875337600708,\n",
       "  'mean_mse': 0.5991019159555435,\n",
       "  'mean_l1': 0.5834992527961731,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [256, 256, 256, 256],\n",
       "  'predictor_hidden_feats': 512,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=256, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=256, bias=True)\n",
       "          (bn_layer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-3): 3 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=256, out=256, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (bn_layer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -2.770 -2.477203\n",
       "  1   -4.370 -4.904602\n",
       "  2   -5.507 -5.458411\n",
       "  3   -3.780 -3.730534\n",
       "  4   -6.291 -6.240525\n",
       "  ..     ...       ...\n",
       "  43  -0.720 -0.316805\n",
       "  44  -3.083 -3.178775\n",
       "  45  -1.110 -0.733568\n",
       "  46  -2.676 -1.923284\n",
       "  47  -1.730 -1.339528\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 50.451552629470825,\n",
       "  'mean_mse': 0.6461379826068878,\n",
       "  'mean_l1': 0.5905770659446716,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [256, 256, 256, 256],\n",
       "  'predictor_hidden_feats': 1024,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=256, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=256, bias=True)\n",
       "          (bn_layer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-3): 3 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=256, out=256, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (bn_layer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=1024, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0    -4.28 -4.143384\n",
       "  1    -4.47 -3.998731\n",
       "  2    -6.78 -6.500405\n",
       "  3    -2.63 -2.665809\n",
       "  4    -3.46 -5.197036\n",
       "  ..     ...       ...\n",
       "  43   -2.37 -1.547049\n",
       "  44   -1.06 -1.162942\n",
       "  45   -1.22 -0.724780\n",
       "  46    0.51  0.038146\n",
       "  47   -4.37 -3.698435\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 35.689982891082764,\n",
       "  'mean_mse': 0.630661815404892,\n",
       "  'mean_l1': 0.5658562183380127,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [512],\n",
       "  'predictor_hidden_feats': 128,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=512, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=512, bias=True)\n",
       "          (bn_layer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=1024, out_features=128, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -1.520 -1.518396\n",
       "  1   -3.780 -4.209366\n",
       "  2   -5.400 -4.188918\n",
       "  3   -6.291 -6.147020\n",
       "  4   -5.030 -4.326177\n",
       "  ..     ...       ...\n",
       "  43  -1.990 -2.583450\n",
       "  44  -3.083 -2.186130\n",
       "  45   0.510 -0.303452\n",
       "  46  -2.850 -2.527527\n",
       "  47  -1.370 -1.564588\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 27.35990262031555,\n",
       "  'mean_mse': 0.5094400644302368,\n",
       "  'mean_l1': 0.5279238820075989,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [512],\n",
       "  'predictor_hidden_feats': 256,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=512, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=512, bias=True)\n",
       "          (bn_layer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -4.110 -3.889526\n",
       "  1   -1.990 -2.272365\n",
       "  2   -1.458 -0.888776\n",
       "  3   -0.400 -0.258384\n",
       "  4   -2.370 -1.541598\n",
       "  ..     ...       ...\n",
       "  43  -3.780 -4.283499\n",
       "  44  -2.820 -3.486914\n",
       "  45  -4.799 -4.726192\n",
       "  46  -3.010 -2.862907\n",
       "  47  -6.250 -5.837038\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 44.45275640487671,\n",
       "  'mean_mse': 0.6143221259117126,\n",
       "  'mean_l1': 0.5569195747375488,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [64, 64],\n",
       "  'predictor_hidden_feats': 256,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=64, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=64, bias=True)\n",
       "          (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=64, out=64, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=128, out_features=256, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -1.870 -1.587686\n",
       "  1   -2.070 -1.411880\n",
       "  2    0.790  0.032507\n",
       "  3   -4.345 -4.013604\n",
       "  4   -1.640 -1.815434\n",
       "  ..     ...       ...\n",
       "  43  -3.796 -4.253510\n",
       "  44  -2.210 -2.443797\n",
       "  45  -4.799 -4.153564\n",
       "  46  -1.614 -1.919930\n",
       "  47  -1.290 -1.910521\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 35.15282893180847,\n",
       "  'mean_mse': 0.4564373642206192,\n",
       "  'mean_l1': 0.5035115331411362,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [512],\n",
       "  'predictor_hidden_feats': 512,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=512, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=512, bias=True)\n",
       "          (bn_layer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -1.300 -1.589018\n",
       "  1    0.790 -0.310397\n",
       "  2   -2.210 -2.287708\n",
       "  3    0.540 -0.153525\n",
       "  4   -3.083 -2.135434\n",
       "  ..     ...       ...\n",
       "  43  -3.400 -3.051852\n",
       "  44  -1.340 -1.303666\n",
       "  45  -5.400 -4.638736\n",
       "  46  -1.110 -0.460274\n",
       "  47  -0.720 -0.787383\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 36.95298671722412,\n",
       "  'mean_mse': 0.5949508845806122,\n",
       "  'mean_l1': 0.5362673699855804,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [512],\n",
       "  'predictor_hidden_feats': 1024,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=512, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=512, bias=True)\n",
       "          (bn_layer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=1024, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -1.370 -1.547326\n",
       "  1    0.790  0.268609\n",
       "  2   -3.083 -2.608924\n",
       "  3   -5.030 -4.145324\n",
       "  4   -2.000 -1.710378\n",
       "  ..     ...       ...\n",
       "  43  -4.280 -4.158005\n",
       "  44  -2.210 -1.000083\n",
       "  45  -4.380 -3.449926\n",
       "  46   0.522 -0.666173\n",
       "  47  -3.510 -3.408728\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 122.17333340644836,\n",
       "  'mean_mse': 0.36293618381023407,\n",
       "  'mean_l1': 0.431387335062027,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [512, 512],\n",
       "  'predictor_hidden_feats': 128,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=512, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=512, bias=True)\n",
       "          (bn_layer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=512, out=512, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (bn_layer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=1024, out_features=128, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -3.220 -3.216669\n",
       "  1   -0.040 -0.634990\n",
       "  2   -7.420 -7.429129\n",
       "  3   -9.160 -9.375417\n",
       "  4   -1.520 -1.332328\n",
       "  ..     ...       ...\n",
       "  43  -1.300 -1.525904\n",
       "  44  -4.380 -4.054625\n",
       "  45  -0.440 -0.787721\n",
       "  46  -1.220 -1.567866\n",
       "  47  -0.807 -1.164322\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 33.214969635009766,\n",
       "  'mean_mse': 0.4449477642774582,\n",
       "  'mean_l1': 0.5092543661594391,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [512, 512],\n",
       "  'predictor_hidden_feats': 256,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=512, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=512, bias=True)\n",
       "          (bn_layer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=512, out=512, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (bn_layer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -0.440 -0.948478\n",
       "  1   -6.560 -5.263064\n",
       "  2   -1.870 -2.024345\n",
       "  3   -5.233 -5.469103\n",
       "  4   -1.220 -1.531761\n",
       "  ..     ...       ...\n",
       "  43  -5.840 -5.675939\n",
       "  44  -7.850 -7.770588\n",
       "  45  -3.100 -2.726957\n",
       "  46  -3.900 -3.298654\n",
       "  47  -4.380 -4.041658\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 68.84708762168884,\n",
       "  'mean_mse': 0.545830100774765,\n",
       "  'mean_l1': 0.5424742102622986,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [512, 512],\n",
       "  'predictor_hidden_feats': 512,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=512, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=512, bias=True)\n",
       "          (bn_layer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=512, out=512, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (bn_layer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -1.989 -4.241355\n",
       "  1   -2.550 -2.178815\n",
       "  2   -2.380 -2.382899\n",
       "  3   -4.280 -3.907692\n",
       "  4    0.790 -1.288406\n",
       "  ..     ...       ...\n",
       "  43   0.620  0.231116\n",
       "  44  -2.218 -3.116113\n",
       "  45  -2.730 -3.846462\n",
       "  46  -2.770 -2.846864\n",
       "  47  -3.680 -3.935470\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 41.59466361999512,\n",
       "  'mean_mse': 0.6170553863048553,\n",
       "  'mean_l1': 0.5809765458106995,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [512, 512],\n",
       "  'predictor_hidden_feats': 1024,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=512, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=512, bias=True)\n",
       "          (bn_layer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=512, out=512, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (bn_layer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=1024, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -3.680 -2.768564\n",
       "  1   -1.300 -1.190247\n",
       "  2   -1.989 -4.264301\n",
       "  3   -3.220 -1.715892\n",
       "  4   -6.780 -7.362407\n",
       "  ..     ...       ...\n",
       "  43  -2.820 -2.140929\n",
       "  44  -3.927 -2.520453\n",
       "  45  -7.200 -6.224628\n",
       "  46  -3.510 -3.491383\n",
       "  47  -1.340 -1.227226\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 137.223548412323,\n",
       "  'mean_mse': 0.4750411808490753,\n",
       "  'mean_l1': 0.5011547356843948,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [512, 512, 512],\n",
       "  'predictor_hidden_feats': 128,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=512, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=512, bias=True)\n",
       "          (bn_layer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-2): 2 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=512, out=512, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (bn_layer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=1024, out_features=128, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -2.820 -2.203779\n",
       "  1   -7.420 -7.850112\n",
       "  2   -2.780 -2.587647\n",
       "  3   -4.310 -4.463851\n",
       "  4   -5.507 -5.284608\n",
       "  ..     ...       ...\n",
       "  43   1.100  0.685498\n",
       "  44  -7.850 -7.537099\n",
       "  45  -3.010 -3.051520\n",
       "  46  -2.770 -2.926082\n",
       "  47  -6.780 -6.148563\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 46.05719995498657,\n",
       "  'mean_mse': 0.5153354108333588,\n",
       "  'mean_l1': 0.5054822564125061,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [512, 512, 512],\n",
       "  'predictor_hidden_feats': 256,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=512, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=512, bias=True)\n",
       "          (bn_layer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-2): 2 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=512, out=512, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (bn_layer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -2.080 -2.199890\n",
       "  1   -7.200 -6.589081\n",
       "  2    0.260  0.033684\n",
       "  3    0.790 -1.314567\n",
       "  4    1.100  1.187721\n",
       "  ..     ...       ...\n",
       "  43  -3.900 -2.555991\n",
       "  44  -1.614 -2.082319\n",
       "  45  -2.460 -2.038384\n",
       "  46  -0.807 -0.166061\n",
       "  47  -4.630 -5.307119\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 103.80205011367798,\n",
       "  'mean_mse': 0.5530145466327667,\n",
       "  'mean_l1': 0.5375088453292847,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [512, 512, 512],\n",
       "  'predictor_hidden_feats': 512,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=512, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=512, bias=True)\n",
       "          (bn_layer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-2): 2 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=512, out=512, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (bn_layer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0    0.610 -0.782229\n",
       "  1   -0.600 -1.038928\n",
       "  2   -1.947 -1.262194\n",
       "  3   -1.390 -1.710591\n",
       "  4   -4.630 -4.264237\n",
       "  ..     ...       ...\n",
       "  43  -1.360 -0.948388\n",
       "  44  -0.807 -0.459729\n",
       "  45  -2.730 -3.241100\n",
       "  46  -4.799 -4.380151\n",
       "  47  -4.110 -3.343424\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 82.55941414833069,\n",
       "  'mean_mse': 0.5032042860984802,\n",
       "  'mean_l1': 0.5138895511627197,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [512, 512, 512],\n",
       "  'predictor_hidden_feats': 1024,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=512, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=512, bias=True)\n",
       "          (bn_layer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-2): 2 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=512, out=512, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (bn_layer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=1024, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -4.920 -4.351989\n",
       "  1   -0.440 -0.947038\n",
       "  2    0.610 -1.156638\n",
       "  3   -4.310 -4.252786\n",
       "  4   -1.520 -1.916888\n",
       "  ..     ...       ...\n",
       "  43  -5.030 -3.974002\n",
       "  44   0.620 -0.144487\n",
       "  45  -3.100 -2.900758\n",
       "  46  -2.770 -2.379703\n",
       "  47  -3.928 -4.002181\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 31.279619932174683,\n",
       "  'mean_mse': 0.703956663608551,\n",
       "  'mean_l1': 0.617807149887085,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [64, 64],\n",
       "  'predictor_hidden_feats': 512,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=64, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=64, bias=True)\n",
       "          (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=64, out=64, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -2.000 -1.382275\n",
       "  1   -3.680 -2.534847\n",
       "  2   -3.010 -2.868567\n",
       "  3   -0.440 -0.791075\n",
       "  4   -2.420 -2.561851\n",
       "  ..     ...       ...\n",
       "  43  -2.080 -2.296332\n",
       "  44  -4.630 -4.451766\n",
       "  45   0.510 -0.555259\n",
       "  46  -6.291 -5.655367\n",
       "  47  -6.250 -5.997903\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 54.351181983947754,\n",
       "  'mean_mse': 0.5915735960006714,\n",
       "  'mean_l1': 0.5920868515968323,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [512, 512, 512, 512],\n",
       "  'predictor_hidden_feats': 128,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=512, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=512, bias=True)\n",
       "          (bn_layer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-3): 3 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=512, out=512, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (bn_layer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=1024, out_features=128, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -3.010 -2.950769\n",
       "  1   -3.300 -3.542869\n",
       "  2   -3.900 -2.888783\n",
       "  3   -1.614 -2.041187\n",
       "  4   -3.928 -3.944489\n",
       "  ..     ...       ...\n",
       "  43  -2.840 -4.556862\n",
       "  44   0.510 -0.397717\n",
       "  45   0.790 -0.358107\n",
       "  46  -2.280 -1.467180\n",
       "  47  -5.400 -4.476998\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 145.41762566566467,\n",
       "  'mean_mse': 0.5998875349760056,\n",
       "  'mean_l1': 0.5807869136333466,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [512, 512, 512, 512],\n",
       "  'predictor_hidden_feats': 256,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=512, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=512, bias=True)\n",
       "          (bn_layer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-3): 3 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=512, out=512, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (bn_layer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -2.550 -2.080017\n",
       "  1   -3.083 -2.607118\n",
       "  2   -0.600 -1.121540\n",
       "  3   -2.000 -1.867887\n",
       "  4   -1.228 -1.456432\n",
       "  ..     ...       ...\n",
       "  43  -1.458 -0.969598\n",
       "  44  -1.730 -1.807972\n",
       "  45   0.522  0.021430\n",
       "  46   0.790 -0.520939\n",
       "  47  -8.400 -6.708647\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 74.65561318397522,\n",
       "  'mean_mse': 0.6048263311386108,\n",
       "  'mean_l1': 0.5854826271533966,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [512, 512, 512, 512],\n",
       "  'predictor_hidden_feats': 512,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=512, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=512, bias=True)\n",
       "          (bn_layer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-3): 3 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=512, out=512, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (bn_layer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0    -4.38 -4.700814\n",
       "  1    -1.29 -2.165927\n",
       "  2    -5.40 -4.167893\n",
       "  3    -1.06 -1.281746\n",
       "  4    -1.37 -1.585650\n",
       "  ..     ...       ...\n",
       "  43   -2.77 -2.435874\n",
       "  44   -5.03 -4.173787\n",
       "  45   -4.47 -3.761697\n",
       "  46   -2.00 -1.816215\n",
       "  47    1.10 -0.214482\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 47.231855154037476,\n",
       "  'mean_mse': 0.7431122958660126,\n",
       "  'mean_l1': 0.675959974527359,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [512, 512, 512, 512],\n",
       "  'predictor_hidden_feats': 1024,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=512, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=512, bias=True)\n",
       "          (bn_layer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-3): 3 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=512, out=512, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (bn_layer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=1024, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -7.420 -7.776689\n",
       "  1   -0.807 -0.233865\n",
       "  2   -5.115 -9.164706\n",
       "  3   -0.720 -0.627938\n",
       "  4    0.620 -0.013657\n",
       "  ..     ...       ...\n",
       "  43  -2.820 -2.626022\n",
       "  44  -6.780 -5.214625\n",
       "  45  -1.820 -0.685245\n",
       "  46  -3.450 -4.618182\n",
       "  47  -1.990 -2.729711\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 31.102152824401855,\n",
       "  'mean_mse': 0.5997305512428284,\n",
       "  'mean_l1': 0.5601603388786316,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [64, 64],\n",
       "  'predictor_hidden_feats': 1024,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=64, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=64, bias=True)\n",
       "          (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=64, out=64, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=128, out_features=1024, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=1024, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -5.400 -4.075252\n",
       "  1   -3.630 -3.253923\n",
       "  2   -1.640 -1.611284\n",
       "  3   -3.796 -4.702380\n",
       "  4   -5.915 -3.936274\n",
       "  ..     ...       ...\n",
       "  43  -2.780 -2.664713\n",
       "  44  -1.300 -0.923486\n",
       "  45   0.522 -0.696256\n",
       "  46  -3.450 -3.817762\n",
       "  47  -3.510 -3.421814\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 72.12637996673584,\n",
       "  'mean_mse': 0.49483510851860046,\n",
       "  'mean_l1': 0.5424335300922394,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [64, 64, 64],\n",
       "  'predictor_hidden_feats': 128,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=64, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=64, bias=True)\n",
       "          (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-2): 2 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=64, out=64, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )},\n",
       " {'pred_df':     y_real    y_pred\n",
       "  0   -4.173 -4.587562\n",
       "  1   -1.300 -0.561790\n",
       "  2   -1.340 -1.013731\n",
       "  3   -5.240 -4.385204\n",
       "  4   -1.820 -1.230250\n",
       "  ..     ...       ...\n",
       "  43  -5.115 -6.310171\n",
       "  44  -2.730 -3.494230\n",
       "  45   0.620  0.419225\n",
       "  46  -3.220 -3.223589\n",
       "  47  -1.640 -1.093217\n",
       "  \n",
       "  [112 rows x 2 columns],\n",
       "  'el_time': 46.95937538146973,\n",
       "  'mean_mse': 0.519304484128952,\n",
       "  'mean_l1': 0.5701031386852264,\n",
       "  'apply_scaffold_split': False,\n",
       "  'hidden_feats': [64, 64, 64],\n",
       "  'predictor_hidden_feats': 256,\n",
       "  'apply_random_aggregations': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'model_type': 'DGL',\n",
       "  'model': GCNPredictor(\n",
       "    (gnn): GCN(\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (graph_conv): GraphConv(in=74, out=64, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=74, out_features=64, bias=True)\n",
       "          (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1-2): 2 x GCNLayer(\n",
       "          (graph_conv): GraphConv(in=64, out=64, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (res_connection): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): WeightedSumAndMax(\n",
       "      (weight_and_sum): WeightAndSum(\n",
       "        (atom_weighting): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=1, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predict): MLPPredictor(\n",
       "      (predict): Sequential(\n",
       "        (0): Dropout(p=0.0, inplace=False)\n",
       "        (1): Linear(in_features=128, out_features=256, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file1_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = file1_load[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pred_df':     y_real    y_pred\n",
       " 0   -5.915 -4.559448\n",
       " 1   -3.850 -3.701262\n",
       " 2   -7.850 -7.446312\n",
       " 3   -2.218 -3.183792\n",
       " 4   -0.180 -0.286621\n",
       " ..     ...       ...\n",
       " 43  -2.730 -3.284103\n",
       " 44  -2.550 -2.500223\n",
       " 45  -1.390 -2.545388\n",
       " 46  -3.928 -3.512547\n",
       " 47  -5.115 -6.244727\n",
       " \n",
       " [112 rows x 2 columns],\n",
       " 'el_time': 33.08916878700256,\n",
       " 'mean_mse': 0.4911479949951172,\n",
       " 'mean_l1': 0.5329278409481049,\n",
       " 'apply_scaffold_split': False,\n",
       " 'hidden_feats': [64],\n",
       " 'predictor_hidden_feats': 128,\n",
       " 'apply_random_aggregations': False,\n",
       " 'learning_rate': 0.001,\n",
       " 'model_type': 'DGL',\n",
       " 'model': GCNPredictor(\n",
       "   (gnn): GCN(\n",
       "     (gnn_layers): ModuleList(\n",
       "       (0): GCNLayer(\n",
       "         (graph_conv): GraphConv(in=74, out=64, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "         (dropout): Dropout(p=0.0, inplace=False)\n",
       "         (res_connection): Linear(in_features=74, out_features=64, bias=True)\n",
       "         (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (readout): WeightedSumAndMax(\n",
       "     (weight_and_sum): WeightAndSum(\n",
       "       (atom_weighting): Sequential(\n",
       "         (0): Linear(in_features=64, out_features=1, bias=True)\n",
       "         (1): Sigmoid()\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (predict): MLPPredictor(\n",
       "     (predict): Sequential(\n",
       "       (0): Dropout(p=0.0, inplace=False)\n",
       "       (1): Linear(in_features=128, out_features=128, bias=True)\n",
       "       (2): ReLU()\n",
       "       (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (4): Linear(in_features=128, out_features=1, bias=True)\n",
       "     )\n",
       "   )\n",
       " )}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_real</th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-5.915</td>\n",
       "      <td>-4.559448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3.850</td>\n",
       "      <td>-3.701262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-7.850</td>\n",
       "      <td>-7.446312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.218</td>\n",
       "      <td>-3.183792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.180</td>\n",
       "      <td>-0.286621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>-2.730</td>\n",
       "      <td>-3.284103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>-2.550</td>\n",
       "      <td>-2.500223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>-1.390</td>\n",
       "      <td>-2.545388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>-3.928</td>\n",
       "      <td>-3.512547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>-5.115</td>\n",
       "      <td>-6.244727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    y_real    y_pred\n",
       "0   -5.915 -4.559448\n",
       "1   -3.850 -3.701262\n",
       "2   -7.850 -7.446312\n",
       "3   -2.218 -3.183792\n",
       "4   -0.180 -0.286621\n",
       "..     ...       ...\n",
       "43  -2.730 -3.284103\n",
       "44  -2.550 -2.500223\n",
       "45  -1.390 -2.545388\n",
       "46  -3.928 -3.512547\n",
       "47  -5.115 -6.244727\n",
       "\n",
       "[112 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff[\"pred_df\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pred_df':     y_real    y_pred\n",
       " 0   -5.915 -4.559448\n",
       " 1   -3.850 -3.701262\n",
       " 2   -7.850 -7.446312\n",
       " 3   -2.218 -3.183792\n",
       " 4   -0.180 -0.286621\n",
       " ..     ...       ...\n",
       " 43  -2.730 -3.284103\n",
       " 44  -2.550 -2.500223\n",
       " 45  -1.390 -2.545388\n",
       " 46  -3.928 -3.512547\n",
       " 47  -5.115 -6.244727\n",
       " \n",
       " [112 rows x 2 columns],\n",
       " 'el_time': 33.08916878700256,\n",
       " 'mean_mse': 0.4911479949951172,\n",
       " 'mean_l1': 0.5329278409481049,\n",
       " 'apply_scaffold_split': False,\n",
       " 'hidden_feats': [64],\n",
       " 'predictor_hidden_feats': 128,\n",
       " 'apply_random_aggregations': False,\n",
       " 'learning_rate': 0.001,\n",
       " 'model_type': 'DGL',\n",
       " 'model': GCNPredictor(\n",
       "   (gnn): GCN(\n",
       "     (gnn_layers): ModuleList(\n",
       "       (0): GCNLayer(\n",
       "         (graph_conv): GraphConv(in=74, out=64, normalization=none, activation=<function relu at 0x000002B107403B80>)\n",
       "         (dropout): Dropout(p=0.0, inplace=False)\n",
       "         (res_connection): Linear(in_features=74, out_features=64, bias=True)\n",
       "         (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (readout): WeightedSumAndMax(\n",
       "     (weight_and_sum): WeightAndSum(\n",
       "       (atom_weighting): Sequential(\n",
       "         (0): Linear(in_features=64, out_features=1, bias=True)\n",
       "         (1): Sigmoid()\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (predict): MLPPredictor(\n",
       "     (predict): Sequential(\n",
       "       (0): Dropout(p=0.0, inplace=False)\n",
       "       (1): Linear(in_features=128, out_features=128, bias=True)\n",
       "       (2): ReLU()\n",
       "       (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (4): Linear(in_features=128, out_features=1, bias=True)\n",
       "     )\n",
       "   )\n",
       " )}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_to_extract = ['hidden_feats', 'predictor_hidden_feats']\n",
    "\n",
    "plot1 = []\n",
    "for run in file1_load:\n",
    "    lossvalue_mse = run[\"mean_mse\"]\n",
    "    lossvalue_l1 = run[\"mean_l1\"]\n",
    "    \n",
    "    extracted_dict = {key: run[key] for key in keys_to_extract if key in run}\n",
    "    extracted_dict[\"lossvalue_mse\"] = lossvalue_mse\n",
    "    extracted_dict[\"lossvalue_l1\"] = lossvalue_l1\n",
    "    df = pd.DataFrame([extracted_dict])\n",
    "    #plot1.append([lossvalue, df])\n",
    "    plot1.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[  hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0         [64]                     128       0.491148      0.532928,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0         [64]                     256       0.603636      0.571523,\n",
       "    hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [64, 64, 64]                     512       0.542441      0.537523,\n",
       "    hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [64, 64, 64]                    1024       0.980275      0.751123,\n",
       "        hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [64, 64, 64, 64]                     128       0.846629      0.744226,\n",
       "        hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [64, 64, 64, 64]                     256       0.837732      0.746869,\n",
       "        hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [64, 64, 64, 64]                     512       0.764106      0.679055,\n",
       "        hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [64, 64, 64, 64]                    1024       0.707445      0.634284,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0        [128]                     128       0.594568      0.549265,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0        [128]                     256       0.502713      0.519673,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0        [128]                     512       0.444414       0.48733,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0        [128]                    1024       0.495451      0.525837,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0         [64]                     512       0.689294       0.63226,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0   [128, 128]                     128       0.446819      0.483363,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0   [128, 128]                     256       0.447426       0.48991,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0   [128, 128]                     512       0.427114      0.501526,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0   [128, 128]                    1024       0.432888      0.482937,\n",
       "       hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [128, 128, 128]                     128       0.448992      0.538918,\n",
       "       hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [128, 128, 128]                     256       0.450765        0.5028,\n",
       "       hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [128, 128, 128]                     512       0.590411      0.541715,\n",
       "       hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [128, 128, 128]                    1024        0.87317      0.763586,\n",
       "            hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [128, 128, 128, 128]                     128       0.544677      0.544735,\n",
       "            hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [128, 128, 128, 128]                     256       0.536373      0.506084,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0         [64]                    1024       0.494547      0.515471,\n",
       "            hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [128, 128, 128, 128]                     512       0.584382      0.589361,\n",
       "            hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [128, 128, 128, 128]                    1024       0.755492       0.68181,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0        [256]                     128       0.604345      0.538834,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0        [256]                     256       0.572455      0.580226,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0        [256]                     512       0.473011      0.485446,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0        [256]                    1024       0.603954       0.53769,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0   [256, 256]                     128       0.551768      0.516573,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0   [256, 256]                     256       0.408472      0.450416,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0   [256, 256]                     512       0.443558      0.497998,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0   [256, 256]                    1024        0.48101       0.50499,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0     [64, 64]                     128       0.531679      0.510943,\n",
       "       hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [256, 256, 256]                     128       0.785103       0.63236,\n",
       "       hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [256, 256, 256]                     256       0.437899      0.497999,\n",
       "       hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [256, 256, 256]                     512       0.560818      0.532125,\n",
       "       hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [256, 256, 256]                    1024       0.440189      0.477968,\n",
       "            hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [256, 256, 256, 256]                     128       0.564988      0.551042,\n",
       "            hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [256, 256, 256, 256]                     256       0.572732      0.510636,\n",
       "            hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [256, 256, 256, 256]                     512       0.599102      0.583499,\n",
       "            hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [256, 256, 256, 256]                    1024       0.646138      0.590577,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0        [512]                     128       0.630662      0.565856,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0        [512]                     256        0.50944      0.527924,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0     [64, 64]                     256       0.614322       0.55692,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0        [512]                     512       0.456437      0.503512,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0        [512]                    1024       0.594951      0.536267,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0   [512, 512]                     128       0.362936      0.431387,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0   [512, 512]                     256       0.444948      0.509254,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0   [512, 512]                     512        0.54583      0.542474,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0   [512, 512]                    1024       0.617055      0.580977,\n",
       "       hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [512, 512, 512]                     128       0.475041      0.501155,\n",
       "       hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [512, 512, 512]                     256       0.515335      0.505482,\n",
       "       hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [512, 512, 512]                     512       0.553015      0.537509,\n",
       "       hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [512, 512, 512]                    1024       0.503204       0.51389,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0     [64, 64]                     512       0.703957      0.617807,\n",
       "            hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [512, 512, 512, 512]                     128       0.591574      0.592087,\n",
       "            hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [512, 512, 512, 512]                     256       0.599888      0.580787,\n",
       "            hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [512, 512, 512, 512]                     512       0.604826      0.585483,\n",
       "            hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [512, 512, 512, 512]                    1024       0.743112       0.67596,\n",
       "   hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0     [64, 64]                    1024       0.599731       0.56016,\n",
       "    hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [64, 64, 64]                     128       0.494835      0.542434,\n",
       "    hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       " 0  [64, 64, 64]                     256       0.519304      0.570103]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hidden_feats</th>\n",
       "      <th>predictor_hidden_feats</th>\n",
       "      <th>lossvalue_mse</th>\n",
       "      <th>lossvalue_l1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64, 64, 64]</td>\n",
       "      <td>128</td>\n",
       "      <td>0.846629</td>\n",
       "      <td>0.744226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       "0  [64, 64, 64, 64]                     128       0.846629      0.744226"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot1[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullplot1 = pd.concat(plot1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hidden_feats</th>\n",
       "      <th>predictor_hidden_feats</th>\n",
       "      <th>lossvalue_mse</th>\n",
       "      <th>lossvalue_l1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64]</td>\n",
       "      <td>128</td>\n",
       "      <td>0.491148</td>\n",
       "      <td>0.532928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64]</td>\n",
       "      <td>256</td>\n",
       "      <td>0.603636</td>\n",
       "      <td>0.571523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64, 64]</td>\n",
       "      <td>512</td>\n",
       "      <td>0.542441</td>\n",
       "      <td>0.537523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64, 64]</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.980275</td>\n",
       "      <td>0.751123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64, 64, 64]</td>\n",
       "      <td>128</td>\n",
       "      <td>0.846629</td>\n",
       "      <td>0.744226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512, 512, 512, 512]</td>\n",
       "      <td>512</td>\n",
       "      <td>0.604826</td>\n",
       "      <td>0.585483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512, 512, 512, 512]</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.743112</td>\n",
       "      <td>0.675960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64]</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.599731</td>\n",
       "      <td>0.560160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64, 64]</td>\n",
       "      <td>128</td>\n",
       "      <td>0.494835</td>\n",
       "      <td>0.542434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64, 64]</td>\n",
       "      <td>256</td>\n",
       "      <td>0.519304</td>\n",
       "      <td>0.570103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1\n",
       "0                   [64]                     128       0.491148      0.532928\n",
       "0                   [64]                     256       0.603636      0.571523\n",
       "0           [64, 64, 64]                     512       0.542441      0.537523\n",
       "0           [64, 64, 64]                    1024       0.980275      0.751123\n",
       "0       [64, 64, 64, 64]                     128       0.846629      0.744226\n",
       "..                   ...                     ...            ...           ...\n",
       "0   [512, 512, 512, 512]                     512       0.604826      0.585483\n",
       "0   [512, 512, 512, 512]                    1024       0.743112      0.675960\n",
       "0               [64, 64]                    1024       0.599731      0.560160\n",
       "0           [64, 64, 64]                     128       0.494835      0.542434\n",
       "0           [64, 64, 64]                     256       0.519304      0.570103\n",
       "\n",
       "[64 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullplot1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hidden_feats</th>\n",
       "      <th>predictor_hidden_feats</th>\n",
       "      <th>lossvalue_l1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64]</td>\n",
       "      <td>128</td>\n",
       "      <td>0.532928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64]</td>\n",
       "      <td>256</td>\n",
       "      <td>0.571523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64, 64]</td>\n",
       "      <td>512</td>\n",
       "      <td>0.537523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64, 64]</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.751123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64, 64, 64]</td>\n",
       "      <td>128</td>\n",
       "      <td>0.744226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512, 512, 512, 512]</td>\n",
       "      <td>512</td>\n",
       "      <td>0.585483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512, 512, 512, 512]</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.675960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64]</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.560160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64, 64]</td>\n",
       "      <td>128</td>\n",
       "      <td>0.542434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64, 64]</td>\n",
       "      <td>256</td>\n",
       "      <td>0.570103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            hidden_feats  predictor_hidden_feats  lossvalue_l1\n",
       "0                   [64]                     128      0.532928\n",
       "0                   [64]                     256      0.571523\n",
       "0           [64, 64, 64]                     512      0.537523\n",
       "0           [64, 64, 64]                    1024      0.751123\n",
       "0       [64, 64, 64, 64]                     128      0.744226\n",
       "..                   ...                     ...           ...\n",
       "0   [512, 512, 512, 512]                     512      0.585483\n",
       "0   [512, 512, 512, 512]                    1024      0.675960\n",
       "0               [64, 64]                    1024      0.560160\n",
       "0           [64, 64, 64]                     128      0.542434\n",
       "0           [64, 64, 64]                     256      0.570103\n",
       "\n",
       "[64 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullplot1.drop(columns=\"lossvalue_mse\", axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.491148\n",
       "0    0.603636\n",
       "0    0.542441\n",
       "0    0.980275\n",
       "0    0.846629\n",
       "       ...   \n",
       "0    0.604826\n",
       "0    0.743112\n",
       "0    0.599731\n",
       "0    0.494835\n",
       "0    0.519304\n",
       "Name: lossvalue_mse, Length: 64, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullplot1[\"lossvalue_mse\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullplot1['Combined Label'] = fullplot1.drop(columns=[\"lossvalue_mse\", \"lossvalue_l1\"], axis=1).astype(str).agg(' - '.join, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hidden_feats</th>\n",
       "      <th>predictor_hidden_feats</th>\n",
       "      <th>lossvalue_mse</th>\n",
       "      <th>lossvalue_l1</th>\n",
       "      <th>Combined Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512, 512]</td>\n",
       "      <td>128</td>\n",
       "      <td>0.362936</td>\n",
       "      <td>0.431387</td>\n",
       "      <td>[512, 512] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>256</td>\n",
       "      <td>0.408472</td>\n",
       "      <td>0.450416</td>\n",
       "      <td>[256, 256] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>512</td>\n",
       "      <td>0.427114</td>\n",
       "      <td>0.501526</td>\n",
       "      <td>[128, 128] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.432888</td>\n",
       "      <td>0.482937</td>\n",
       "      <td>[128, 128] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256, 256, 256]</td>\n",
       "      <td>256</td>\n",
       "      <td>0.437899</td>\n",
       "      <td>0.497999</td>\n",
       "      <td>[256, 256, 256] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256, 256, 256]</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.440189</td>\n",
       "      <td>0.477968</td>\n",
       "      <td>[256, 256, 256] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>512</td>\n",
       "      <td>0.443558</td>\n",
       "      <td>0.497998</td>\n",
       "      <td>[256, 256] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128]</td>\n",
       "      <td>512</td>\n",
       "      <td>0.444414</td>\n",
       "      <td>0.487330</td>\n",
       "      <td>[128] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512, 512]</td>\n",
       "      <td>256</td>\n",
       "      <td>0.444948</td>\n",
       "      <td>0.509254</td>\n",
       "      <td>[512, 512] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>128</td>\n",
       "      <td>0.446819</td>\n",
       "      <td>0.483363</td>\n",
       "      <td>[128, 128] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>256</td>\n",
       "      <td>0.447426</td>\n",
       "      <td>0.489910</td>\n",
       "      <td>[128, 128] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128, 128, 128]</td>\n",
       "      <td>128</td>\n",
       "      <td>0.448992</td>\n",
       "      <td>0.538918</td>\n",
       "      <td>[128, 128, 128] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128, 128, 128]</td>\n",
       "      <td>256</td>\n",
       "      <td>0.450765</td>\n",
       "      <td>0.502800</td>\n",
       "      <td>[128, 128, 128] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512]</td>\n",
       "      <td>512</td>\n",
       "      <td>0.456437</td>\n",
       "      <td>0.503512</td>\n",
       "      <td>[512] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256]</td>\n",
       "      <td>512</td>\n",
       "      <td>0.473011</td>\n",
       "      <td>0.485446</td>\n",
       "      <td>[256] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512, 512, 512]</td>\n",
       "      <td>128</td>\n",
       "      <td>0.475041</td>\n",
       "      <td>0.501155</td>\n",
       "      <td>[512, 512, 512] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.481010</td>\n",
       "      <td>0.504990</td>\n",
       "      <td>[256, 256] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64]</td>\n",
       "      <td>128</td>\n",
       "      <td>0.491148</td>\n",
       "      <td>0.532928</td>\n",
       "      <td>[64] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64]</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.494547</td>\n",
       "      <td>0.515471</td>\n",
       "      <td>[64] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64, 64]</td>\n",
       "      <td>128</td>\n",
       "      <td>0.494835</td>\n",
       "      <td>0.542434</td>\n",
       "      <td>[64, 64, 64] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128]</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.495451</td>\n",
       "      <td>0.525837</td>\n",
       "      <td>[128] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128]</td>\n",
       "      <td>256</td>\n",
       "      <td>0.502713</td>\n",
       "      <td>0.519673</td>\n",
       "      <td>[128] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512, 512, 512]</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.503204</td>\n",
       "      <td>0.513890</td>\n",
       "      <td>[512, 512, 512] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512]</td>\n",
       "      <td>256</td>\n",
       "      <td>0.509440</td>\n",
       "      <td>0.527924</td>\n",
       "      <td>[512] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512, 512, 512]</td>\n",
       "      <td>256</td>\n",
       "      <td>0.515335</td>\n",
       "      <td>0.505482</td>\n",
       "      <td>[512, 512, 512] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64, 64]</td>\n",
       "      <td>256</td>\n",
       "      <td>0.519304</td>\n",
       "      <td>0.570103</td>\n",
       "      <td>[64, 64, 64] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64]</td>\n",
       "      <td>128</td>\n",
       "      <td>0.531679</td>\n",
       "      <td>0.510943</td>\n",
       "      <td>[64, 64] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128, 128, 128, 128]</td>\n",
       "      <td>256</td>\n",
       "      <td>0.536373</td>\n",
       "      <td>0.506084</td>\n",
       "      <td>[128, 128, 128, 128] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64, 64]</td>\n",
       "      <td>512</td>\n",
       "      <td>0.542441</td>\n",
       "      <td>0.537523</td>\n",
       "      <td>[64, 64, 64] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128, 128, 128, 128]</td>\n",
       "      <td>128</td>\n",
       "      <td>0.544677</td>\n",
       "      <td>0.544735</td>\n",
       "      <td>[128, 128, 128, 128] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512, 512]</td>\n",
       "      <td>512</td>\n",
       "      <td>0.545830</td>\n",
       "      <td>0.542474</td>\n",
       "      <td>[512, 512] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>128</td>\n",
       "      <td>0.551768</td>\n",
       "      <td>0.516573</td>\n",
       "      <td>[256, 256] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512, 512, 512]</td>\n",
       "      <td>512</td>\n",
       "      <td>0.553015</td>\n",
       "      <td>0.537509</td>\n",
       "      <td>[512, 512, 512] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256, 256, 256]</td>\n",
       "      <td>512</td>\n",
       "      <td>0.560818</td>\n",
       "      <td>0.532125</td>\n",
       "      <td>[256, 256, 256] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256, 256, 256, 256]</td>\n",
       "      <td>128</td>\n",
       "      <td>0.564988</td>\n",
       "      <td>0.551042</td>\n",
       "      <td>[256, 256, 256, 256] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256]</td>\n",
       "      <td>256</td>\n",
       "      <td>0.572455</td>\n",
       "      <td>0.580226</td>\n",
       "      <td>[256] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256, 256, 256, 256]</td>\n",
       "      <td>256</td>\n",
       "      <td>0.572732</td>\n",
       "      <td>0.510636</td>\n",
       "      <td>[256, 256, 256, 256] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128, 128, 128, 128]</td>\n",
       "      <td>512</td>\n",
       "      <td>0.584382</td>\n",
       "      <td>0.589361</td>\n",
       "      <td>[128, 128, 128, 128] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128, 128, 128]</td>\n",
       "      <td>512</td>\n",
       "      <td>0.590411</td>\n",
       "      <td>0.541715</td>\n",
       "      <td>[128, 128, 128] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512, 512, 512, 512]</td>\n",
       "      <td>128</td>\n",
       "      <td>0.591574</td>\n",
       "      <td>0.592087</td>\n",
       "      <td>[512, 512, 512, 512] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128]</td>\n",
       "      <td>128</td>\n",
       "      <td>0.594568</td>\n",
       "      <td>0.549265</td>\n",
       "      <td>[128] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512]</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.594951</td>\n",
       "      <td>0.536267</td>\n",
       "      <td>[512] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256, 256, 256, 256]</td>\n",
       "      <td>512</td>\n",
       "      <td>0.599102</td>\n",
       "      <td>0.583499</td>\n",
       "      <td>[256, 256, 256, 256] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64]</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.599731</td>\n",
       "      <td>0.560160</td>\n",
       "      <td>[64, 64] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512, 512, 512, 512]</td>\n",
       "      <td>256</td>\n",
       "      <td>0.599888</td>\n",
       "      <td>0.580787</td>\n",
       "      <td>[512, 512, 512, 512] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64]</td>\n",
       "      <td>256</td>\n",
       "      <td>0.603636</td>\n",
       "      <td>0.571523</td>\n",
       "      <td>[64] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256]</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.603954</td>\n",
       "      <td>0.537690</td>\n",
       "      <td>[256] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256]</td>\n",
       "      <td>128</td>\n",
       "      <td>0.604345</td>\n",
       "      <td>0.538834</td>\n",
       "      <td>[256] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512, 512, 512, 512]</td>\n",
       "      <td>512</td>\n",
       "      <td>0.604826</td>\n",
       "      <td>0.585483</td>\n",
       "      <td>[512, 512, 512, 512] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64]</td>\n",
       "      <td>256</td>\n",
       "      <td>0.614322</td>\n",
       "      <td>0.556920</td>\n",
       "      <td>[64, 64] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512, 512]</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.617055</td>\n",
       "      <td>0.580977</td>\n",
       "      <td>[512, 512] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512]</td>\n",
       "      <td>128</td>\n",
       "      <td>0.630662</td>\n",
       "      <td>0.565856</td>\n",
       "      <td>[512] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256, 256, 256, 256]</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.646138</td>\n",
       "      <td>0.590577</td>\n",
       "      <td>[256, 256, 256, 256] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64]</td>\n",
       "      <td>512</td>\n",
       "      <td>0.689294</td>\n",
       "      <td>0.632260</td>\n",
       "      <td>[64] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64]</td>\n",
       "      <td>512</td>\n",
       "      <td>0.703957</td>\n",
       "      <td>0.617807</td>\n",
       "      <td>[64, 64] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64, 64, 64]</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.707445</td>\n",
       "      <td>0.634284</td>\n",
       "      <td>[64, 64, 64, 64] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512, 512, 512, 512]</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.743112</td>\n",
       "      <td>0.675960</td>\n",
       "      <td>[512, 512, 512, 512] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128, 128, 128, 128]</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.755492</td>\n",
       "      <td>0.681810</td>\n",
       "      <td>[128, 128, 128, 128] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64, 64, 64]</td>\n",
       "      <td>512</td>\n",
       "      <td>0.764106</td>\n",
       "      <td>0.679055</td>\n",
       "      <td>[64, 64, 64, 64] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256, 256, 256]</td>\n",
       "      <td>128</td>\n",
       "      <td>0.785103</td>\n",
       "      <td>0.632360</td>\n",
       "      <td>[256, 256, 256] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64, 64, 64]</td>\n",
       "      <td>256</td>\n",
       "      <td>0.837732</td>\n",
       "      <td>0.746869</td>\n",
       "      <td>[64, 64, 64, 64] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64, 64, 64]</td>\n",
       "      <td>128</td>\n",
       "      <td>0.846629</td>\n",
       "      <td>0.744226</td>\n",
       "      <td>[64, 64, 64, 64] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128, 128, 128]</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.873170</td>\n",
       "      <td>0.763586</td>\n",
       "      <td>[128, 128, 128] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64, 64]</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.980275</td>\n",
       "      <td>0.751123</td>\n",
       "      <td>[64, 64, 64] - 1024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1  \\\n",
       "0            [512, 512]                     128       0.362936      0.431387   \n",
       "0            [256, 256]                     256       0.408472      0.450416   \n",
       "0            [128, 128]                     512       0.427114      0.501526   \n",
       "0            [128, 128]                    1024       0.432888      0.482937   \n",
       "0       [256, 256, 256]                     256       0.437899      0.497999   \n",
       "0       [256, 256, 256]                    1024       0.440189      0.477968   \n",
       "0            [256, 256]                     512       0.443558      0.497998   \n",
       "0                 [128]                     512       0.444414      0.487330   \n",
       "0            [512, 512]                     256       0.444948      0.509254   \n",
       "0            [128, 128]                     128       0.446819      0.483363   \n",
       "0            [128, 128]                     256       0.447426      0.489910   \n",
       "0       [128, 128, 128]                     128       0.448992      0.538918   \n",
       "0       [128, 128, 128]                     256       0.450765      0.502800   \n",
       "0                 [512]                     512       0.456437      0.503512   \n",
       "0                 [256]                     512       0.473011      0.485446   \n",
       "0       [512, 512, 512]                     128       0.475041      0.501155   \n",
       "0            [256, 256]                    1024       0.481010      0.504990   \n",
       "0                  [64]                     128       0.491148      0.532928   \n",
       "0                  [64]                    1024       0.494547      0.515471   \n",
       "0          [64, 64, 64]                     128       0.494835      0.542434   \n",
       "0                 [128]                    1024       0.495451      0.525837   \n",
       "0                 [128]                     256       0.502713      0.519673   \n",
       "0       [512, 512, 512]                    1024       0.503204      0.513890   \n",
       "0                 [512]                     256       0.509440      0.527924   \n",
       "0       [512, 512, 512]                     256       0.515335      0.505482   \n",
       "0          [64, 64, 64]                     256       0.519304      0.570103   \n",
       "0              [64, 64]                     128       0.531679      0.510943   \n",
       "0  [128, 128, 128, 128]                     256       0.536373      0.506084   \n",
       "0          [64, 64, 64]                     512       0.542441      0.537523   \n",
       "0  [128, 128, 128, 128]                     128       0.544677      0.544735   \n",
       "0            [512, 512]                     512       0.545830      0.542474   \n",
       "0            [256, 256]                     128       0.551768      0.516573   \n",
       "0       [512, 512, 512]                     512       0.553015      0.537509   \n",
       "0       [256, 256, 256]                     512       0.560818      0.532125   \n",
       "0  [256, 256, 256, 256]                     128       0.564988      0.551042   \n",
       "0                 [256]                     256       0.572455      0.580226   \n",
       "0  [256, 256, 256, 256]                     256       0.572732      0.510636   \n",
       "0  [128, 128, 128, 128]                     512       0.584382      0.589361   \n",
       "0       [128, 128, 128]                     512       0.590411      0.541715   \n",
       "0  [512, 512, 512, 512]                     128       0.591574      0.592087   \n",
       "0                 [128]                     128       0.594568      0.549265   \n",
       "0                 [512]                    1024       0.594951      0.536267   \n",
       "0  [256, 256, 256, 256]                     512       0.599102      0.583499   \n",
       "0              [64, 64]                    1024       0.599731      0.560160   \n",
       "0  [512, 512, 512, 512]                     256       0.599888      0.580787   \n",
       "0                  [64]                     256       0.603636      0.571523   \n",
       "0                 [256]                    1024       0.603954      0.537690   \n",
       "0                 [256]                     128       0.604345      0.538834   \n",
       "0  [512, 512, 512, 512]                     512       0.604826      0.585483   \n",
       "0              [64, 64]                     256       0.614322      0.556920   \n",
       "0            [512, 512]                    1024       0.617055      0.580977   \n",
       "0                 [512]                     128       0.630662      0.565856   \n",
       "0  [256, 256, 256, 256]                    1024       0.646138      0.590577   \n",
       "0                  [64]                     512       0.689294      0.632260   \n",
       "0              [64, 64]                     512       0.703957      0.617807   \n",
       "0      [64, 64, 64, 64]                    1024       0.707445      0.634284   \n",
       "0  [512, 512, 512, 512]                    1024       0.743112      0.675960   \n",
       "0  [128, 128, 128, 128]                    1024       0.755492      0.681810   \n",
       "0      [64, 64, 64, 64]                     512       0.764106      0.679055   \n",
       "0       [256, 256, 256]                     128       0.785103      0.632360   \n",
       "0      [64, 64, 64, 64]                     256       0.837732      0.746869   \n",
       "0      [64, 64, 64, 64]                     128       0.846629      0.744226   \n",
       "0       [128, 128, 128]                    1024       0.873170      0.763586   \n",
       "0          [64, 64, 64]                    1024       0.980275      0.751123   \n",
       "\n",
       "                Combined Label  \n",
       "0             [512, 512] - 128  \n",
       "0             [256, 256] - 256  \n",
       "0             [128, 128] - 512  \n",
       "0            [128, 128] - 1024  \n",
       "0        [256, 256, 256] - 256  \n",
       "0       [256, 256, 256] - 1024  \n",
       "0             [256, 256] - 512  \n",
       "0                  [128] - 512  \n",
       "0             [512, 512] - 256  \n",
       "0             [128, 128] - 128  \n",
       "0             [128, 128] - 256  \n",
       "0        [128, 128, 128] - 128  \n",
       "0        [128, 128, 128] - 256  \n",
       "0                  [512] - 512  \n",
       "0                  [256] - 512  \n",
       "0        [512, 512, 512] - 128  \n",
       "0            [256, 256] - 1024  \n",
       "0                   [64] - 128  \n",
       "0                  [64] - 1024  \n",
       "0           [64, 64, 64] - 128  \n",
       "0                 [128] - 1024  \n",
       "0                  [128] - 256  \n",
       "0       [512, 512, 512] - 1024  \n",
       "0                  [512] - 256  \n",
       "0        [512, 512, 512] - 256  \n",
       "0           [64, 64, 64] - 256  \n",
       "0               [64, 64] - 128  \n",
       "0   [128, 128, 128, 128] - 256  \n",
       "0           [64, 64, 64] - 512  \n",
       "0   [128, 128, 128, 128] - 128  \n",
       "0             [512, 512] - 512  \n",
       "0             [256, 256] - 128  \n",
       "0        [512, 512, 512] - 512  \n",
       "0        [256, 256, 256] - 512  \n",
       "0   [256, 256, 256, 256] - 128  \n",
       "0                  [256] - 256  \n",
       "0   [256, 256, 256, 256] - 256  \n",
       "0   [128, 128, 128, 128] - 512  \n",
       "0        [128, 128, 128] - 512  \n",
       "0   [512, 512, 512, 512] - 128  \n",
       "0                  [128] - 128  \n",
       "0                 [512] - 1024  \n",
       "0   [256, 256, 256, 256] - 512  \n",
       "0              [64, 64] - 1024  \n",
       "0   [512, 512, 512, 512] - 256  \n",
       "0                   [64] - 256  \n",
       "0                 [256] - 1024  \n",
       "0                  [256] - 128  \n",
       "0   [512, 512, 512, 512] - 512  \n",
       "0               [64, 64] - 256  \n",
       "0            [512, 512] - 1024  \n",
       "0                  [512] - 128  \n",
       "0  [256, 256, 256, 256] - 1024  \n",
       "0                   [64] - 512  \n",
       "0               [64, 64] - 512  \n",
       "0      [64, 64, 64, 64] - 1024  \n",
       "0  [512, 512, 512, 512] - 1024  \n",
       "0  [128, 128, 128, 128] - 1024  \n",
       "0       [64, 64, 64, 64] - 512  \n",
       "0        [256, 256, 256] - 128  \n",
       "0       [64, 64, 64, 64] - 256  \n",
       "0       [64, 64, 64, 64] - 128  \n",
       "0       [128, 128, 128] - 1024  \n",
       "0          [64, 64, 64] - 1024  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullplot1.sort_values(by=['lossvalue_mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hidden_feats</th>\n",
       "      <th>predictor_hidden_feats</th>\n",
       "      <th>lossvalue_mse</th>\n",
       "      <th>lossvalue_l1</th>\n",
       "      <th>Combined Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512, 512]</td>\n",
       "      <td>128</td>\n",
       "      <td>0.362936</td>\n",
       "      <td>0.431387</td>\n",
       "      <td>[512, 512] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>256</td>\n",
       "      <td>0.408472</td>\n",
       "      <td>0.450416</td>\n",
       "      <td>[256, 256] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256, 256, 256]</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.440189</td>\n",
       "      <td>0.477968</td>\n",
       "      <td>[256, 256, 256] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.432888</td>\n",
       "      <td>0.482937</td>\n",
       "      <td>[128, 128] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>128</td>\n",
       "      <td>0.446819</td>\n",
       "      <td>0.483363</td>\n",
       "      <td>[128, 128] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256]</td>\n",
       "      <td>512</td>\n",
       "      <td>0.473011</td>\n",
       "      <td>0.485446</td>\n",
       "      <td>[256] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128]</td>\n",
       "      <td>512</td>\n",
       "      <td>0.444414</td>\n",
       "      <td>0.487330</td>\n",
       "      <td>[128] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>256</td>\n",
       "      <td>0.447426</td>\n",
       "      <td>0.489910</td>\n",
       "      <td>[128, 128] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>512</td>\n",
       "      <td>0.443558</td>\n",
       "      <td>0.497998</td>\n",
       "      <td>[256, 256] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256, 256, 256]</td>\n",
       "      <td>256</td>\n",
       "      <td>0.437899</td>\n",
       "      <td>0.497999</td>\n",
       "      <td>[256, 256, 256] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512, 512, 512]</td>\n",
       "      <td>128</td>\n",
       "      <td>0.475041</td>\n",
       "      <td>0.501155</td>\n",
       "      <td>[512, 512, 512] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>512</td>\n",
       "      <td>0.427114</td>\n",
       "      <td>0.501526</td>\n",
       "      <td>[128, 128] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128, 128, 128]</td>\n",
       "      <td>256</td>\n",
       "      <td>0.450765</td>\n",
       "      <td>0.502800</td>\n",
       "      <td>[128, 128, 128] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512]</td>\n",
       "      <td>512</td>\n",
       "      <td>0.456437</td>\n",
       "      <td>0.503512</td>\n",
       "      <td>[512] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.481010</td>\n",
       "      <td>0.504990</td>\n",
       "      <td>[256, 256] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512, 512, 512]</td>\n",
       "      <td>256</td>\n",
       "      <td>0.515335</td>\n",
       "      <td>0.505482</td>\n",
       "      <td>[512, 512, 512] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128, 128, 128, 128]</td>\n",
       "      <td>256</td>\n",
       "      <td>0.536373</td>\n",
       "      <td>0.506084</td>\n",
       "      <td>[128, 128, 128, 128] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512, 512]</td>\n",
       "      <td>256</td>\n",
       "      <td>0.444948</td>\n",
       "      <td>0.509254</td>\n",
       "      <td>[512, 512] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256, 256, 256, 256]</td>\n",
       "      <td>256</td>\n",
       "      <td>0.572732</td>\n",
       "      <td>0.510636</td>\n",
       "      <td>[256, 256, 256, 256] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64]</td>\n",
       "      <td>128</td>\n",
       "      <td>0.531679</td>\n",
       "      <td>0.510943</td>\n",
       "      <td>[64, 64] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512, 512, 512]</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.503204</td>\n",
       "      <td>0.513890</td>\n",
       "      <td>[512, 512, 512] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64]</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.494547</td>\n",
       "      <td>0.515471</td>\n",
       "      <td>[64] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>128</td>\n",
       "      <td>0.551768</td>\n",
       "      <td>0.516573</td>\n",
       "      <td>[256, 256] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128]</td>\n",
       "      <td>256</td>\n",
       "      <td>0.502713</td>\n",
       "      <td>0.519673</td>\n",
       "      <td>[128] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128]</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.495451</td>\n",
       "      <td>0.525837</td>\n",
       "      <td>[128] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512]</td>\n",
       "      <td>256</td>\n",
       "      <td>0.509440</td>\n",
       "      <td>0.527924</td>\n",
       "      <td>[512] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256, 256, 256]</td>\n",
       "      <td>512</td>\n",
       "      <td>0.560818</td>\n",
       "      <td>0.532125</td>\n",
       "      <td>[256, 256, 256] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64]</td>\n",
       "      <td>128</td>\n",
       "      <td>0.491148</td>\n",
       "      <td>0.532928</td>\n",
       "      <td>[64] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512]</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.594951</td>\n",
       "      <td>0.536267</td>\n",
       "      <td>[512] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512, 512, 512]</td>\n",
       "      <td>512</td>\n",
       "      <td>0.553015</td>\n",
       "      <td>0.537509</td>\n",
       "      <td>[512, 512, 512] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64, 64]</td>\n",
       "      <td>512</td>\n",
       "      <td>0.542441</td>\n",
       "      <td>0.537523</td>\n",
       "      <td>[64, 64, 64] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256]</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.603954</td>\n",
       "      <td>0.537690</td>\n",
       "      <td>[256] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256]</td>\n",
       "      <td>128</td>\n",
       "      <td>0.604345</td>\n",
       "      <td>0.538834</td>\n",
       "      <td>[256] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128, 128, 128]</td>\n",
       "      <td>128</td>\n",
       "      <td>0.448992</td>\n",
       "      <td>0.538918</td>\n",
       "      <td>[128, 128, 128] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128, 128, 128]</td>\n",
       "      <td>512</td>\n",
       "      <td>0.590411</td>\n",
       "      <td>0.541715</td>\n",
       "      <td>[128, 128, 128] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64, 64]</td>\n",
       "      <td>128</td>\n",
       "      <td>0.494835</td>\n",
       "      <td>0.542434</td>\n",
       "      <td>[64, 64, 64] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512, 512]</td>\n",
       "      <td>512</td>\n",
       "      <td>0.545830</td>\n",
       "      <td>0.542474</td>\n",
       "      <td>[512, 512] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128, 128, 128, 128]</td>\n",
       "      <td>128</td>\n",
       "      <td>0.544677</td>\n",
       "      <td>0.544735</td>\n",
       "      <td>[128, 128, 128, 128] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128]</td>\n",
       "      <td>128</td>\n",
       "      <td>0.594568</td>\n",
       "      <td>0.549265</td>\n",
       "      <td>[128] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256, 256, 256, 256]</td>\n",
       "      <td>128</td>\n",
       "      <td>0.564988</td>\n",
       "      <td>0.551042</td>\n",
       "      <td>[256, 256, 256, 256] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64]</td>\n",
       "      <td>256</td>\n",
       "      <td>0.614322</td>\n",
       "      <td>0.556920</td>\n",
       "      <td>[64, 64] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64]</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.599731</td>\n",
       "      <td>0.560160</td>\n",
       "      <td>[64, 64] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512]</td>\n",
       "      <td>128</td>\n",
       "      <td>0.630662</td>\n",
       "      <td>0.565856</td>\n",
       "      <td>[512] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64, 64]</td>\n",
       "      <td>256</td>\n",
       "      <td>0.519304</td>\n",
       "      <td>0.570103</td>\n",
       "      <td>[64, 64, 64] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64]</td>\n",
       "      <td>256</td>\n",
       "      <td>0.603636</td>\n",
       "      <td>0.571523</td>\n",
       "      <td>[64] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256]</td>\n",
       "      <td>256</td>\n",
       "      <td>0.572455</td>\n",
       "      <td>0.580226</td>\n",
       "      <td>[256] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512, 512, 512, 512]</td>\n",
       "      <td>256</td>\n",
       "      <td>0.599888</td>\n",
       "      <td>0.580787</td>\n",
       "      <td>[512, 512, 512, 512] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512, 512]</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.617055</td>\n",
       "      <td>0.580977</td>\n",
       "      <td>[512, 512] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256, 256, 256, 256]</td>\n",
       "      <td>512</td>\n",
       "      <td>0.599102</td>\n",
       "      <td>0.583499</td>\n",
       "      <td>[256, 256, 256, 256] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512, 512, 512, 512]</td>\n",
       "      <td>512</td>\n",
       "      <td>0.604826</td>\n",
       "      <td>0.585483</td>\n",
       "      <td>[512, 512, 512, 512] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128, 128, 128, 128]</td>\n",
       "      <td>512</td>\n",
       "      <td>0.584382</td>\n",
       "      <td>0.589361</td>\n",
       "      <td>[128, 128, 128, 128] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256, 256, 256, 256]</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.646138</td>\n",
       "      <td>0.590577</td>\n",
       "      <td>[256, 256, 256, 256] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512, 512, 512, 512]</td>\n",
       "      <td>128</td>\n",
       "      <td>0.591574</td>\n",
       "      <td>0.592087</td>\n",
       "      <td>[512, 512, 512, 512] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64]</td>\n",
       "      <td>512</td>\n",
       "      <td>0.703957</td>\n",
       "      <td>0.617807</td>\n",
       "      <td>[64, 64] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64]</td>\n",
       "      <td>512</td>\n",
       "      <td>0.689294</td>\n",
       "      <td>0.632260</td>\n",
       "      <td>[64] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[256, 256, 256]</td>\n",
       "      <td>128</td>\n",
       "      <td>0.785103</td>\n",
       "      <td>0.632360</td>\n",
       "      <td>[256, 256, 256] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64, 64, 64]</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.707445</td>\n",
       "      <td>0.634284</td>\n",
       "      <td>[64, 64, 64, 64] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[512, 512, 512, 512]</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.743112</td>\n",
       "      <td>0.675960</td>\n",
       "      <td>[512, 512, 512, 512] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64, 64, 64]</td>\n",
       "      <td>512</td>\n",
       "      <td>0.764106</td>\n",
       "      <td>0.679055</td>\n",
       "      <td>[64, 64, 64, 64] - 512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128, 128, 128, 128]</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.755492</td>\n",
       "      <td>0.681810</td>\n",
       "      <td>[128, 128, 128, 128] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64, 64, 64]</td>\n",
       "      <td>128</td>\n",
       "      <td>0.846629</td>\n",
       "      <td>0.744226</td>\n",
       "      <td>[64, 64, 64, 64] - 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64, 64, 64]</td>\n",
       "      <td>256</td>\n",
       "      <td>0.837732</td>\n",
       "      <td>0.746869</td>\n",
       "      <td>[64, 64, 64, 64] - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64, 64]</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.980275</td>\n",
       "      <td>0.751123</td>\n",
       "      <td>[64, 64, 64] - 1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128, 128, 128]</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.873170</td>\n",
       "      <td>0.763586</td>\n",
       "      <td>[128, 128, 128] - 1024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           hidden_feats  predictor_hidden_feats  lossvalue_mse  lossvalue_l1  \\\n",
       "0            [512, 512]                     128       0.362936      0.431387   \n",
       "0            [256, 256]                     256       0.408472      0.450416   \n",
       "0       [256, 256, 256]                    1024       0.440189      0.477968   \n",
       "0            [128, 128]                    1024       0.432888      0.482937   \n",
       "0            [128, 128]                     128       0.446819      0.483363   \n",
       "0                 [256]                     512       0.473011      0.485446   \n",
       "0                 [128]                     512       0.444414      0.487330   \n",
       "0            [128, 128]                     256       0.447426      0.489910   \n",
       "0            [256, 256]                     512       0.443558      0.497998   \n",
       "0       [256, 256, 256]                     256       0.437899      0.497999   \n",
       "0       [512, 512, 512]                     128       0.475041      0.501155   \n",
       "0            [128, 128]                     512       0.427114      0.501526   \n",
       "0       [128, 128, 128]                     256       0.450765      0.502800   \n",
       "0                 [512]                     512       0.456437      0.503512   \n",
       "0            [256, 256]                    1024       0.481010      0.504990   \n",
       "0       [512, 512, 512]                     256       0.515335      0.505482   \n",
       "0  [128, 128, 128, 128]                     256       0.536373      0.506084   \n",
       "0            [512, 512]                     256       0.444948      0.509254   \n",
       "0  [256, 256, 256, 256]                     256       0.572732      0.510636   \n",
       "0              [64, 64]                     128       0.531679      0.510943   \n",
       "0       [512, 512, 512]                    1024       0.503204      0.513890   \n",
       "0                  [64]                    1024       0.494547      0.515471   \n",
       "0            [256, 256]                     128       0.551768      0.516573   \n",
       "0                 [128]                     256       0.502713      0.519673   \n",
       "0                 [128]                    1024       0.495451      0.525837   \n",
       "0                 [512]                     256       0.509440      0.527924   \n",
       "0       [256, 256, 256]                     512       0.560818      0.532125   \n",
       "0                  [64]                     128       0.491148      0.532928   \n",
       "0                 [512]                    1024       0.594951      0.536267   \n",
       "0       [512, 512, 512]                     512       0.553015      0.537509   \n",
       "0          [64, 64, 64]                     512       0.542441      0.537523   \n",
       "0                 [256]                    1024       0.603954      0.537690   \n",
       "0                 [256]                     128       0.604345      0.538834   \n",
       "0       [128, 128, 128]                     128       0.448992      0.538918   \n",
       "0       [128, 128, 128]                     512       0.590411      0.541715   \n",
       "0          [64, 64, 64]                     128       0.494835      0.542434   \n",
       "0            [512, 512]                     512       0.545830      0.542474   \n",
       "0  [128, 128, 128, 128]                     128       0.544677      0.544735   \n",
       "0                 [128]                     128       0.594568      0.549265   \n",
       "0  [256, 256, 256, 256]                     128       0.564988      0.551042   \n",
       "0              [64, 64]                     256       0.614322      0.556920   \n",
       "0              [64, 64]                    1024       0.599731      0.560160   \n",
       "0                 [512]                     128       0.630662      0.565856   \n",
       "0          [64, 64, 64]                     256       0.519304      0.570103   \n",
       "0                  [64]                     256       0.603636      0.571523   \n",
       "0                 [256]                     256       0.572455      0.580226   \n",
       "0  [512, 512, 512, 512]                     256       0.599888      0.580787   \n",
       "0            [512, 512]                    1024       0.617055      0.580977   \n",
       "0  [256, 256, 256, 256]                     512       0.599102      0.583499   \n",
       "0  [512, 512, 512, 512]                     512       0.604826      0.585483   \n",
       "0  [128, 128, 128, 128]                     512       0.584382      0.589361   \n",
       "0  [256, 256, 256, 256]                    1024       0.646138      0.590577   \n",
       "0  [512, 512, 512, 512]                     128       0.591574      0.592087   \n",
       "0              [64, 64]                     512       0.703957      0.617807   \n",
       "0                  [64]                     512       0.689294      0.632260   \n",
       "0       [256, 256, 256]                     128       0.785103      0.632360   \n",
       "0      [64, 64, 64, 64]                    1024       0.707445      0.634284   \n",
       "0  [512, 512, 512, 512]                    1024       0.743112      0.675960   \n",
       "0      [64, 64, 64, 64]                     512       0.764106      0.679055   \n",
       "0  [128, 128, 128, 128]                    1024       0.755492      0.681810   \n",
       "0      [64, 64, 64, 64]                     128       0.846629      0.744226   \n",
       "0      [64, 64, 64, 64]                     256       0.837732      0.746869   \n",
       "0          [64, 64, 64]                    1024       0.980275      0.751123   \n",
       "0       [128, 128, 128]                    1024       0.873170      0.763586   \n",
       "\n",
       "                Combined Label  \n",
       "0             [512, 512] - 128  \n",
       "0             [256, 256] - 256  \n",
       "0       [256, 256, 256] - 1024  \n",
       "0            [128, 128] - 1024  \n",
       "0             [128, 128] - 128  \n",
       "0                  [256] - 512  \n",
       "0                  [128] - 512  \n",
       "0             [128, 128] - 256  \n",
       "0             [256, 256] - 512  \n",
       "0        [256, 256, 256] - 256  \n",
       "0        [512, 512, 512] - 128  \n",
       "0             [128, 128] - 512  \n",
       "0        [128, 128, 128] - 256  \n",
       "0                  [512] - 512  \n",
       "0            [256, 256] - 1024  \n",
       "0        [512, 512, 512] - 256  \n",
       "0   [128, 128, 128, 128] - 256  \n",
       "0             [512, 512] - 256  \n",
       "0   [256, 256, 256, 256] - 256  \n",
       "0               [64, 64] - 128  \n",
       "0       [512, 512, 512] - 1024  \n",
       "0                  [64] - 1024  \n",
       "0             [256, 256] - 128  \n",
       "0                  [128] - 256  \n",
       "0                 [128] - 1024  \n",
       "0                  [512] - 256  \n",
       "0        [256, 256, 256] - 512  \n",
       "0                   [64] - 128  \n",
       "0                 [512] - 1024  \n",
       "0        [512, 512, 512] - 512  \n",
       "0           [64, 64, 64] - 512  \n",
       "0                 [256] - 1024  \n",
       "0                  [256] - 128  \n",
       "0        [128, 128, 128] - 128  \n",
       "0        [128, 128, 128] - 512  \n",
       "0           [64, 64, 64] - 128  \n",
       "0             [512, 512] - 512  \n",
       "0   [128, 128, 128, 128] - 128  \n",
       "0                  [128] - 128  \n",
       "0   [256, 256, 256, 256] - 128  \n",
       "0               [64, 64] - 256  \n",
       "0              [64, 64] - 1024  \n",
       "0                  [512] - 128  \n",
       "0           [64, 64, 64] - 256  \n",
       "0                   [64] - 256  \n",
       "0                  [256] - 256  \n",
       "0   [512, 512, 512, 512] - 256  \n",
       "0            [512, 512] - 1024  \n",
       "0   [256, 256, 256, 256] - 512  \n",
       "0   [512, 512, 512, 512] - 512  \n",
       "0   [128, 128, 128, 128] - 512  \n",
       "0  [256, 256, 256, 256] - 1024  \n",
       "0   [512, 512, 512, 512] - 128  \n",
       "0               [64, 64] - 512  \n",
       "0                   [64] - 512  \n",
       "0        [256, 256, 256] - 128  \n",
       "0      [64, 64, 64, 64] - 1024  \n",
       "0  [512, 512, 512, 512] - 1024  \n",
       "0       [64, 64, 64, 64] - 512  \n",
       "0  [128, 128, 128, 128] - 1024  \n",
       "0       [64, 64, 64, 64] - 128  \n",
       "0       [64, 64, 64, 64] - 256  \n",
       "0          [64, 64, 64] - 1024  \n",
       "0       [128, 128, 128] - 1024  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullplot1.sort_values(by=['lossvalue_l1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='Combined Label'>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fullplot1.set_index('Combined Label', inplace=True)\n",
    "\n",
    "fullplot1['lossvalue_mse'].plot(kind='bar')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newest_dgl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
